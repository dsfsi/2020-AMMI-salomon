{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation with Transformer \n",
    "\n",
    "In this notebook we will be implementing a (slightly modified version) of the Transformer model from the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. All images in this notebook will be taken from the Transformer paper. For more information about the Transformer, [see](https://www.mihaileric.com/posts/transformers-attention-in-disguise/) [these](https://jalammar.github.io/illustrated-transformer/) [three](http://nlp.seas.harvard.edu/2018/04/03/attention.html) articles.\n",
    "\n",
    "![](assets/transformer1.png)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Similar to the Convolutional Sequence-to-Sequence model, the Transformer does not use any recurrence. It also does not use any convolutional layers. Instead the model is entirely made up of linear layers, attention mechanisms and normalization. \n",
    "\n",
    "As of January 2020, Transformers are the dominant architecture in NLP and are used to achieve state-of-the-art results for many tasks and it appears as if they will be for the near future. \n",
    "\n",
    "The most popular Transformer variant is [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) and pre-trained versions of BERT are commonly used to replace the embedding layers - if not more - in NLP models. \n",
    "\n",
    "A common library used when dealing with pre-trained transformers is the [Transformers](https://huggingface.co/transformers/) library, see [here](https://huggingface.co/transformers/pretrained_models.html) for a list of all pre-trained models available.\n",
    "\n",
    "The differences between the implementation in this notebook and the paper are:\n",
    "- we use a learned positional encoding instead of a static one\n",
    "- we use the standard Adam optimizer with a static learning rate instead of one with warm-up and cool-down steps\n",
    "- we do not use label smoothing\n",
    "\n",
    "We make all of these changes as they closely follow BERT's set-up and the majority of Transformer variants use a similar set-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "As always, let's import all the required modules and set the random seeds for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then create our tokenizers as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ln = spacy.load('en')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ln(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return text.split() # [tok.text for tok in spacy_ln.tokenizer(text)] #text.split()\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return text.split() #[tok.text for tok in spacy_en.tokenizer(text)] #text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'I', 'am', 'Salomon']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_ln('Hello I am Salomon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fields are the same as the previous notebook. The model expects data to be fed in with the batch dimension first, so we use `batch_first = True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_ln, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the Multi30k dataset and build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "# train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "#                                                     fields = (SRC, TRG))\n",
    "\n",
    "MAX_LEN = 90\n",
    "\n",
    "\n",
    "# load our dataset\n",
    "train_data, valid_data = datasets.TranslationDataset.splits(train=\"train\", validation=\"valid\", test=None,\n",
    "    path = \"./data\", exts = ('.en', '.ln'), fields=(SRC, TRG),\n",
    "    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "    len(vars(x)['trg']) <= MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the device and the data iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)\n",
    "\n",
    "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "#     (train_data, valid_data, test_data), \n",
    "#      batch_size = BATCH_SIZE,\n",
    "#      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,   246,   119,  ...,     1,     1,     1],\n",
       "        [    2,   310,     6,  ...,     1,     1,     1],\n",
       "        [    2,     6, 10835,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    2,   548,    38,  ...,     1,     1,     1],\n",
       "        [    2,    19,   652,  ...,     1,     1,     1],\n",
       "        [    2,   152,     5,  ...,     1,     1,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iterator)).src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Next, we'll build the model. Like previous notebooks it is made up of an *encoder* and a *decoder*, with the encoder *encoding* the input/source sentence (in German) into *context vector* and the decoder then *decoding* this context vector to output our output/target sentence (in English). \n",
    "\n",
    "### Encoder\n",
    "\n",
    "Similar to the ConvSeq2Seq model, the Transformer's encoder does not attempt to compress the entire source sentence, $X = (x_1, ... ,x_n)$, into a single context vector, $z$. Instead it produces a sequence of context vectors, $Z = (z_1, ... , z_n)$. So, if our input sequence was 5 tokens long we would have $Z = (z_1, z_2, z_3, z_4, z_5)$. Why do we call this a sequence of context vectors and not a sequence of hidden states? A hidden state at time $t$ in an RNN has only seen tokens $x_t$ and all the tokens before it. However, each context vector here has seen all tokens at all positions within the input sequence.\n",
    "\n",
    "![](assets/transformer-encoder.png)\n",
    "\n",
    "First, the tokens are passed through a standard embedding layer. Next, as the model has no recurrent it has no idea about the order of the tokens within the sequence. We solve this by using a second embedding layer called a *positional embedding layer*. This is a standard embedding layer where the input is not the token itself but the position of the token within the sequence, starting with the first token, the `<sos>` (start of sequence) token, in position 0. The position embedding has a \"vocabulary\" size of 100, which means our model can accept sentences up to 100 tokens long. This can be increased if we want to handle longer sentences.\n",
    "\n",
    "The original Transformer implementation from the Attention is All You Need paper does not learn positional embeddings. Instead it uses a fixed static embedding. Modern Transformer architectures, like BERT, use positional embeddings instead, hence we have decided to use them in these tutorials. Check out [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding) section to read more about the positional embeddings used in the original Transformer model.\n",
    "\n",
    "Next, the token and positional embeddings are elementwise summed together to get a vector which contains information about the token and also its position with in the sequence. However, before they are summed, the token embeddings are multiplied by a scaling factor which is $\\sqrt{d_{model}}$, where $d_{model}$ is the hidden dimension size, `hid_dim`. This supposedly reduces variance in the embeddings and the model is difficult to train reliably without this scaling factor. Dropout is then applied to the combined embeddings.\n",
    "\n",
    "The combined embeddings are then passed through $N$ *encoder layers* to get $Z$, which is then output and can be used by the decoder.\n",
    "\n",
    "The source mask, `src_mask`, is simply the same shape as the source sentence but has a value of 1 when the token in the source sentence is not a `<pad>` token and 0 when it is a `<pad>` token. This is used in the encoder layers to mask the multi-head attention mechanisms, which are used to calculate and apply attention over the source sentence, so the model does not pay attention to `<pad>` tokens, which contain no useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "The encoder layers are where all of the \"meat\" of the encoder is contained. We first pass the source sentence and its mask into the *multi-head attention layer*, then perform dropout on it, apply a residual connection and pass it through a [Layer Normalization](https://arxiv.org/abs/1607.06450) layer. We then pass it through a *position-wise feedforward* layer and then, again, apply dropout, a residual connection and then layer normalization to get the output of this layer which is fed into the next layer. The parameters are not shared between layers. \n",
    "\n",
    "The mutli head attention layer is used by the encoder layer to attend to the source sentence, i.e. it is calculating and applying attention over itself instead of another sequence, hence we call it *self attention*.\n",
    "\n",
    "[This](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/) article goes into more detail about layer normalization, but the gist is that it normalizes the values of the features, i.e. across the hidden dimension, so each feature has a mean of 0 and a standard deviation of 1. This allows neural networks with a larger number of layers, like the Transformer, to be trained easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, src len]\n",
    "                \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "One of the key, novel concepts introduced by the Transformer paper is the *multi-head attention layer*. \n",
    "\n",
    "![](assets/transformer-attention.png)\n",
    "\n",
    "Attention can be though of as *queries*, *keys* and *values* - where the query is used with the key to get an attention vector (usually the output of a *softmax* operation and has all values between 0 and 1 which sum to 1) which is then used to get a weighted sum of the values.\n",
    "\n",
    "The Transformer uses *scaled dot-product attention*, where the query and key are combined by taking the dot product between them, then applying the softmax operation and scaling by $d_k$ before finally then multiplying by the value. $d_k$ is the *head dimension*, `head_dim`, which we will shortly explain further.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ \n",
    "\n",
    "This is similar to standard *dot product attention* but is scaled by $d_k$, which the paper states is used to stop the results of the dot products growing large, causing gradients to become too small.\n",
    "\n",
    "However, the scaled dot-product attention isn't simply applied to the queries, keys and values. Instead of doing a single attention application the queries, keys and values have their `hid_dim` split into $h$ *heads* and the scaled dot-product attention is calculated over all heads in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$. We then re-combine the heads into their `hid_dim` shape, thus each `hid_dim` is potentially paying attention to $h$ different concepts.\n",
    "\n",
    "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
    "\n",
    "$W^O$ is the linear layer applied at the end of the multi-head attention layer, `fc`. $W^Q, W^K, W^V$ are the linear layers `fc_q`, `fc_k` and `fc_v`.\n",
    "\n",
    "Walking through the module, first we calculate $QW^Q$, $KW^K$ and $VW^V$ with the linear layers, `fc_q`, `fc_k` and `fc_v`, to give us `Q`, `K` and `V`. Next, we split the `hid_dim` of the query, key and value into `n_heads` using `.view` and correctly permute them so they can be multiplied together. We then calculate the `energy` (the un-normalized attention) by multiplying `Q` and `K` together and scaling it by the square root of `head_dim`, which is calulated as `hid_dim // n_heads`. We then mask the energy so we do not pay attention over any elements of the sequeuence we shouldn't, then apply the softmax and dropout. We then apply the attention to the value heads, `V`, before combining the `n_heads` together. Finally, we multiply this $W^O$, represented by `fc_o`. \n",
    "\n",
    "Note that in our implementation the lengths of the keys and values are always the same, thus when matrix multiplying the output of the softmax, `attention`, with `V` we will always have valid dimension sizes for matrix multiplication. This multiplication is carried out using `torch.matmul` which, when both tensors are >2-dimensional, does a batched matrix multiplication over the last two dimensions of each tensor. This will be a **[query len, key len] x [value len, head dim]** batched matrix multiplication over the batch size and each head which provides the **[batch size, n heads, query len, head dim]** result.\n",
    "\n",
    "One thing that looks strange at first is that dropout is applied directly to the attention. This means that our attention vector will most probably not sum to 1 and we may pay full attention to a token but the attention over that token is set to 0 by dropout. This is never explained, or even mentioned, in the paper however is used by the [official implementation](https://github.com/tensorflow/tensor2tensor/) and every Transformer implementation since, [including BERT](https://github.com/google-research/bert/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer\n",
    "\n",
    "The other main block inside the encoder layer is the *position-wise feedforward layer* This is relatively simple compared to the multi-head attention layer. The input is transformed from `hid_dim` to `pf_dim`, where `pf_dim` is usually a lot larger than `hid_dim`. The original Transformer used a `hid_dim` of 512 and a `pf_dim` of 2048. The ReLU activation function and dropout are applied before it is transformed back into a `hid_dim` representation. \n",
    "\n",
    "Why is this used? Unfortunately, it is never explained in the paper.\n",
    "\n",
    "BERT uses the [GELU](https://arxiv.org/abs/1606.08415) activation function, which can be used by simply switching `torch.relu` for `F.gelu`. Why did they use GELU? Again, it is never explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The objective of the decoder is to take the encoded representation of the source sentence, $Z$, and convert it into predicted tokens in the target sentence, $\\hat{Y}$. We then compare $\\hat{Y}$ with the actual tokens in the target sentence, $Y$, to calculate our loss, which will be used to calculate the gradients of our parameters and then use our optimizer to update our weights in order to improve our predictions. \n",
    "\n",
    "![](assets/transformer-decoder.png)\n",
    "\n",
    "The decoder is similar to encoder, however it now has two multi-head attention layers. A *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value.\n",
    "\n",
    "The decoder uses positional embeddings and combines - via an elementwise sum - them with the scaled embedded target tokens, followed by dropout. Again, our positional encodings have a \"vocabulary\" of 100, which means they can accept sequences up to 100 tokens long. This can be increased if desired.\n",
    "\n",
    "The combined embeddings are then passed through the $N$ decoder layers, along with the encoded source, `enc_src`, and the source and target masks. Note that the number of layers in the encoder does not have to be equal to the number of layers in the decoder, even though they are both denoted by $N$.\n",
    "\n",
    "The decoder representation after the $N^{th}$ layer is then passed through a linear layer, `fc_out`. In PyTorch, the softmax operation is contained within our loss function, so we do not explicitly need to use a softmax layer here.\n",
    "\n",
    "As well as using the source mask, as we did in the encoder to prevent our model attending to `<pad>` tokens, we also use a target mask. This will be explained further in the `Seq2Seq` model which encapsulates both the encoder and decoder, but the gist of it is that it performs a similar operation as the decoder padding in the convolutional sequence-to-sequence model. As we are processing all of the target tokens at once in parallel we need a method of stopping the decoder from \"cheating\" by simply \"looking\" at what the next token in the target sequence is and outputting it. \n",
    "\n",
    "Our decoder layer also outputs the normalized attention values so we can later plot them to see what our model is actually paying attention to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Decoder Transformer class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "                            \n",
    "        #pos = [batch size, trg len]\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "                \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "As mentioned previously, the decoder layer is similar to the encoder layer except that it now has two multi-head attention layers, `self_attention` and `encoder_attention`. \n",
    "\n",
    "The first performs self-attention, as in the encoder, by using the decoder representation so far as the query, key and value. This is followed by dropout, residual connection and layer normalization. This `self_attention` layer uses the target sequence mask, `trg_mask`, in order to prevent the decoder from \"cheating\" by paying attention to tokens that are \"ahead\" of the one it is currently processing as it processes all tokens in the target sentence in parallel.\n",
    "\n",
    "The second is how we actually feed the encoded source sentence, `enc_src`, into our decoder. In this multi-head attention layer the queries are the decoder representations and the keys and values are the decoder representations. Here, the source mask, `src_mask` is used to prevent the multi-head attention layer from attending to `<pad>` tokens within the source sentence. This is then followed by the dropout, residual connection and layer normalization layers. \n",
    "\n",
    "Finally, we pass this through the position-wise feedforward layer and yet another sequence of dropout, residual connection and layer normalization.\n",
    "\n",
    "The decoder layer isn't introducing any new concepts, just using the same set of layers as the encoder in a slightly different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Decoder Layer Transformer class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "Finally, we have the `Seq2Seq` module which encapsulates the encoder and decoder, as well as handling the creation of the masks.\n",
    "\n",
    "The source mask is created by checking where the source sequence is not equal to a `<pad>` token. It is 1 where the token is not a `<pad>` token and 0 when it is. It is then unsqueezed so it can be correctly broadcast when applying the mask to the `energy`, which of shape **_[batch size, n heads, seq len, seq len]_**.\n",
    "\n",
    "The target mask is slightly more complicated. First, we create a mask for the `<pad>` tokens, as we did for the source mask. Next, we create a \"subsequent\" mask, `trg_sub_mask`, using `torch.tril`. This creates a diagonal matrix where the elements above the diagonal will be zero and the elements below the diagonal will be set to whatever the input tensor is. In this case, the input tensor will be a tensor filled with ones. So this means our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "This shows what each target token (row) is allowed to look at (column). The first target token has a mask of **_[1, 0, 0, 0, 0]_** which means it can only look at the first target token. The second target token has a mask of **_[1, 1, 0, 0, 0]_** which it means it can look at both the first and second target tokens. \n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "After the masks are created, they used with the encoder and decoder along with the source and target sentences to get our predicted target sentence, `output`, along with the decoder's attention over the source sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Seq2Seq Model\n",
    "\n",
    "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but is able to be run on a single GPU quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab: 41240\n",
      "Target vocab: 41768\n"
     ]
    }
   ],
   "source": [
    "print(f\"Source vocab: {INPUT_DIM}\")\n",
    "print(f\"Target vocab: {OUTPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use them to define our whole sequence-to-sequence encapsulating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of parameters, noticing it is significantly less than the 37M for the convolutional sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 35,989,288 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper does not mention which weight initialization scheme was used, however Xavier uniform seems to be common amongst Transformer models, so we use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer used in the original Transformer paper uses Adam with a learning rate that has a \"warm-up\" and then a \"cool-down\" period. BERT and other Transformer models use Adam with a fixed learning rate, so we will implement that. Check [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#optimizer) link for more details about the original Transformer's learning rate schedule.\n",
    "\n",
    "Note that the learning rate needs to be lower than the default used by Adam or else learning is unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function, making sure to ignore losses calculated over `<pad>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "#         if output.shape[0] != trg.shape[0] or output.shape[1]!=6256:\n",
    "#             ipdb.set_trace()\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation loop is the same as the training loop, just without the gradient calculations and parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a small function that we can use to tell us how long an epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CUDA_LAUNCH_BLOCKING=1\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model ...\n",
      "*****************************************************\n",
      "best loss: [epoch: 0], [valid loss 1.4111332705792259]\n",
      "*****************************************************\n",
      "Epoch: 01 | Time: 19m 28s\n",
      "\tTrain Loss: 3.026 | Train PPL:  20.618\n",
      "\t Val. Loss: 1.411 |  Val. PPL:   4.101\n",
      "Saving Model ...\n",
      "*****************************************************\n",
      "best loss: [epoch: 1], [valid loss 1.1523114689132747]\n",
      "*****************************************************\n",
      "Epoch: 02 | Time: 19m 18s\n",
      "\tTrain Loss: 2.163 | Train PPL:   8.696\n",
      "\t Val. Loss: 1.152 |  Val. PPL:   3.166\n",
      "Saving Model ...\n",
      "*****************************************************\n",
      "best loss: [epoch: 2], [valid loss 1.0556168593028012]\n",
      "*****************************************************\n",
      "Epoch: 03 | Time: 19m 12s\n",
      "\tTrain Loss: 1.938 | Train PPL:   6.947\n",
      "\t Val. Loss: 1.056 |  Val. PPL:   2.874\n",
      "Saving Model ...\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss        \n",
    "        print('Saving Model ...')\n",
    "        torch.save(model.state_dict(), 'model/Model_MT_'+str(best_valid_loss)[:4]+'.pt')\n",
    "        print('*****************************************************')\n",
    "        print(f'best loss: [epoch: {epoch}], [valid loss {best_valid_loss}]')\n",
    "        print('*****************************************************')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our \"best\" parameters and manage to achieve a better test perplexity than all previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Valid Loss: 0.672 | Test PPL:   1.958 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model/Model_MT.pt'))\n",
    "\n",
    "valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "print(f'| Valid Loss: {valid_loss:.3f} | Test PPL: {math.exp(valid_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now we can can translations from our model with the `translate_sentence` function below.\n",
    "\n",
    "The steps taken are:\n",
    "- tokenize the source sentence if it has not been tokenized (is a string)\n",
    "- append the `<sos>` and `<eos>` tokens\n",
    "- numericalize the source sentence\n",
    "- convert it to a tensor and add a batch dimension\n",
    "- create the source sentence mask\n",
    "- feed the source sentence and mask into the encoder\n",
    "- create a list to hold the output sentence, initialized with an `<sos>` token\n",
    "- while we have not hit a maximum length\n",
    "  - convert the current output sentence prediction into a tensor with a batch dimension\n",
    "  - create a target sentence mask\n",
    "  - place the current output, encoder output and both masks into the decoder\n",
    "  - get next output token prediction from decoder along with attention\n",
    "  - add prediction to current output sentence prediction\n",
    "  - break if the prediction was an `<eos>` token\n",
    "- convert the output sentence from indexes to tokens\n",
    "- return the output sentence (with the `<sos>` token removed) and the attention from the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define a function that displays the attention over the source sentence for each step of the decoding. As this model has 8 heads our model we can view the attention for each of the heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
    "    \n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                           rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll get an example from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['I', 'am', 'from', 'ghana']\n",
      "predicted trg = ['nazali', 'moto', 'ya', 'ghana', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "Sentence = \"I am from ghana\"\n",
    "src = tokenize_ln(Sentence)\n",
    "\n",
    "print(f'src = {src}')\n",
    "\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the attention from each head below. Each is certainly different, but it's difficult (perhaps impossible) to reason about what head has actually learned to pay attention to. Some heads pay full attention to \"eine\" when translating \"a\", some don't at all, and some do a little. They all seem to follow the similar \"downward staircase\" pattern and the attention when outputting the last two tokens is equally spread over the final two tokens in the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get an example the model has not been trained on from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['jehovah', 'is', 'the', 'name', 'of', 'god', 'as', 'revealed', 'in', 'the', 'bible', '.']\n",
      "trg = ['yehova', 'ezali', 'nkombo', 'ya', 'nzambe', 'na', 'biblia', '.']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 6\n",
    "\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model translates it by switching *is running* to just *running*, but it is an acceptable swap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['yehova', 'ezali', 'nkombo', 'ya', 'nzambe', 'oyo', 'biblia', 'elobeli', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, some heads pay full attention to \"ein\" whilst some pay no attention to it. Again, most of the heads seem to spread their attention over both the period and `<eos>` tokens in the source sentence when outputting the period and `<eos>` sentence in the predicted target sentence, though some seem to pay attention to tokens from near the start of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/anaconda3/envs/s2s/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/home/aims/anaconda3/envs/s2s/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1800 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll look at an example from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_idx = 10\n",
    "\n",
    "# src = vars(test_data.examples[example_idx])['src']\n",
    "# trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "# print(f'src = {src}')\n",
    "# print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decent translation with *young* being omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "# print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "Finally we calculate the BLEU score for the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a BLEU score of 35.08, which beats the 33.3 of the convolutional sequence-to-sequence model and 28.2 of the attention based RNN model. All this whilst having the least amount of parameters and the fastest training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 54.74\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(valid_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations for finishing these tutorials! I hope you've found them useful.\n",
    "\n",
    "If you find any mistakes or want to ask any questions about any of the code or explanations used, feel free to submit a GitHub issue and I will try to correct it ASAP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition with Transformer \n",
    "\n",
    "In this notebook we will be implementing a (slightly modified version) of the Transformer model from the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. All images in this notebook will be taken from the Transformer paper.\n",
    "\n",
    "![](assets/ST.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import ipdb\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import torchaudio\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "import librosa\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from torch.distributed import get_rank\n",
    "from torch.distributed import get_world_size\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "from scipy import spatial\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "# from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "\n",
    "import fnmatch\n",
    "import io\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    sound, _ = torchaudio.load(path, normalization=True)\n",
    "    sound = sound.numpy().T\n",
    "    \n",
    "#     print(len(sound.shape))\n",
    "    \n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    return sound\n",
    "\n",
    "\n",
    "def get_audio_length(path):\n",
    "    output = subprocess.check_output(\n",
    "        ['soxi -D \\\"%s\\\"' % path.strip()], shell=True)\n",
    "    return float(output)\n",
    "\n",
    "def audio_with_sox(path, sample_rate, start_time, end_time):\n",
    "    \"\"\"\n",
    "    crop and resample the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as tar_file:\n",
    "        tar_filename = tar_file.name\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} trim {} ={} >/dev/null 2>&1\".format(\n",
    "            path, sample_rate,\n",
    "            tar_filename, start_time,\n",
    "            end_time)\n",
    "        \n",
    "        os.system(sox_params)\n",
    "        y = load_audio(tar_filename)\n",
    "        return y\n",
    "\n",
    "def augment_audio_with_sox(path, sample_rate, tempo, gain):\n",
    "    \"\"\"\n",
    "    Changes tempo and gain of the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as augmented_file:\n",
    "        augmented_filename = augmented_file.name\n",
    "        sox_augment_params = [\"tempo\", \"{:.3f}\".format(\n",
    "            tempo), \"gain\", \"{:.3f}\".format(gain)]\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} {} >/dev/null 2>&1\".format(\n",
    "            path, sample_rate, augmented_filename, \" \".join(sox_augment_params))\n",
    "        os.system(sox_params)\n",
    "        y = load_audio(augmented_filename)\n",
    "        return y\n",
    "\n",
    "\n",
    "def load_randomly_augmented_audio(path, sample_rate=16000, tempo_range=(0.85, 1.15), \n",
    "                                  gain_range=(-6, 8)):\n",
    "    \"\"\"\n",
    "    Picks tempo and gain uniformly, applies it to the utterance by using sox utility.\n",
    "    Returns the augmented utterance.\n",
    "    \"\"\"\n",
    "    low_tempo, high_tempo = tempo_range\n",
    "    tempo_value = np.random.uniform(low=low_tempo, high=high_tempo)\n",
    "    low_gain, high_gain = gain_range\n",
    "    gain_value = np.random.uniform(low=low_gain, high=high_gain)\n",
    "    audio = augment_audio_with_sox(path=path, sample_rate=sample_rate,\n",
    "                                   tempo=tempo_value, gain=gain_value)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = {'hamming': scipy.signal.hamming, 'hann': scipy.signal.hann, \n",
    "           'blackman': scipy.signal.blackman, 'bartlett': scipy.signal.bartlett}\n",
    "\n",
    "\n",
    "class AudioParser(object):\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        \"\"\"\n",
    "        :param transcript_path: Path where transcript is stored from the manifest file\n",
    "        :return: Transcript in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        :param audio_path: Path where audio is stored from the manifest file\n",
    "        :return: Audio in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectrogramParser(AudioParser):\n",
    "    def __init__(self, audio_conf, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Parses audio file into spectrogram with optional normalization and various augmentations\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param normalize(default False):  Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment(default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        super(SpectrogramParser, self).__init__()\n",
    "        self.window_stride = audio_conf['window_stride']\n",
    "        self.window_size = audio_conf['window_size']\n",
    "        self.sample_rate = audio_conf['sample_rate']\n",
    "        self.window = windows.get(audio_conf['window'], windows['hamming'])\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        self.noiseInjector = NoiseInjection(audio_conf['noise_dir'], self.sample_rate,\n",
    "                                            audio_conf['noise_levels']) if audio_conf.get(\n",
    "            'noise_dir') is not None else None\n",
    "        self.noise_prob = audio_conf.get('noise_prob')\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "        if self.augment:\n",
    "            y = load_randomly_augmented_audio(audio_path, self.sample_rate)\n",
    "        else:\n",
    "            y = load_audio(audio_path)\n",
    "\n",
    "        if self.noiseInjector:\n",
    "            logging.info(\"inject noise\")\n",
    "            add_noise = np.random.binomial(1, self.noise_prob)\n",
    "            if add_noise:\n",
    "                y = self.noiseInjector.inject_noise(y)\n",
    "\n",
    "        n_fft = int(self.sample_rate * self.window_size)\n",
    "        win_length = n_fft\n",
    "        hop_length = int(self.sample_rate * self.window_stride)\n",
    "\n",
    "        # Short-time Fourier transform (STFT)\n",
    "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                         win_length=win_length, window=self.window)\n",
    "                \n",
    "        spect, phase = librosa.magphase(D)\n",
    "\n",
    "        # S = log(S+1) = log1p(S)\n",
    "        spect = np.log1p(spect)\n",
    "        spect = torch.FloatTensor(spect)\n",
    "\n",
    "        if self.normalize:\n",
    "            mean = spect.mean()\n",
    "            std = spect.std()\n",
    "            spect.add_(-mean)\n",
    "            spect.div_(std)\n",
    "\n",
    "        return spect\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectrogramDataset(Dataset, SpectrogramParser):\n",
    "    def __init__(self, audio_conf, manifest_filepath_list, \n",
    "                 label2id, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Dataset that loads tensors via a csv containing file paths to audio files and transcripts separated by\n",
    "        a comma. Each new line is a different sample. Example below:\n",
    "        /path/to/audio.wav,/path/to/audio.txt\n",
    "        ...\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param manifest_filepath: Path to manifest csv as describe above\n",
    "        :param labels: String containing all the possible characters to map to\n",
    "        :param normalize (default False): Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment (default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        self.max_size = 0\n",
    "        self.ids_list = []\n",
    "        for i in range(len(manifest_filepath_list)):\n",
    "            manifest_filepath = manifest_filepath_list[i]\n",
    "            with open(manifest_filepath) as f:\n",
    "                ids = f.readlines()\n",
    "\n",
    "            ids = [x.strip().split(',') for x in ids]\n",
    "            self.ids_list.append(ids)\n",
    "            self.max_size = max(len(ids), self.max_size)\n",
    "\n",
    "        self.manifest_filepath_list = manifest_filepath_list\n",
    "        self.label2id = label2id\n",
    "        super(SpectrogramDataset, self).__init__(\n",
    "            audio_conf, normalize, augment)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        random_id = random.randint(0, len(self.ids_list)-1)\n",
    "        ids = self.ids_list[random_id]\n",
    "        sample = ids[index % len(ids)]\n",
    "        audio_path, transcript_path = sample[0], sample[1]\n",
    "        \n",
    "        # get the audio using Short-time Fourier transform (STFT)\n",
    "        # librosa.stft up to \"args.src_max_len\"\n",
    "        spect = self.parse_audio(audio_path)[:,:args.src_max_len] \n",
    "        \n",
    "        transcript = self.parse_transcript(transcript_path)\n",
    "        return spect, transcript\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "            # add start of sentense and end of sentence token\n",
    "            transcript = args.SOS_CHAR + transcript_file.read().replace('\\n', '').lower() +\\\n",
    "                            args.EOS_CHAR\n",
    "            \n",
    "        # return all index exept 0 (false), in this case\n",
    "        # there will be no 0 in the list of index (due to filter)\n",
    "        transcript = list(\n",
    "            filter(None, [self.label2id.get(x) for x in list(transcript)]))\n",
    "        return transcript\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_size\n",
    "\n",
    "\n",
    "class NoiseInjection(object):\n",
    "    def __init__(self,\n",
    "                 path=None,\n",
    "                 sample_rate=16000,\n",
    "                 noise_levels=(0, 0.5)):\n",
    "        \"\"\"\n",
    "        Adds noise to an input signal with specific SNR. Higher the noise level, the more noise added.\n",
    "        Modified code from https://github.com/willfrey/audio/blob/master/torchaudio/transforms.py\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            print(\"Directory doesn't exist: {}\".format(path))\n",
    "            raise IOError\n",
    "        self.paths = path is not None and librosa.util.find_files(path)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.noise_levels = noise_levels\n",
    "\n",
    "    def inject_noise(self, data):\n",
    "        noise_path = np.random.choice(self.paths)\n",
    "        noise_level = np.random.uniform(*self.noise_levels)\n",
    "        return self.inject_noise_sample(data, noise_path, noise_level)\n",
    "\n",
    "    def inject_noise_sample(self, data, noise_path, noise_level):\n",
    "        noise_len = get_audio_length(noise_path)\n",
    "        data_len = len(data) / self.sample_rate\n",
    "        noise_start = np.random.rand() * (noise_len - data_len)\n",
    "        noise_end = noise_start + data_len\n",
    "        noise_dst = audio_with_sox(\n",
    "            noise_path, self.sample_rate, noise_start, noise_end)\n",
    "        assert len(data) == len(noise_dst)\n",
    "        noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)\n",
    "        data_energy = np.sqrt(data.dot(data) / data.size)\n",
    "        data += noise_level * noise_dst * data_energy / noise_energy\n",
    "        return data\n",
    "\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    def func(p):\n",
    "        return p[0].size(1)\n",
    "\n",
    "    def func_tgt(p):\n",
    "        return len(p[1])\n",
    "\n",
    "    # descending sorted\n",
    "    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
    "\n",
    "    max_seq_len = max(batch, key=func)[0].size(1)\n",
    "    freq_size = max(batch, key=func)[0].size(0)\n",
    "    max_tgt_len = len(max(batch, key=func_tgt)[1])\n",
    "    \n",
    "    inputs = torch.zeros(len(batch), 1, freq_size, max_seq_len)\n",
    "    input_sizes = torch.IntTensor(len(batch))\n",
    "    input_percentages = torch.FloatTensor(len(batch))\n",
    "\n",
    "    targets = torch.zeros(len(batch), max_tgt_len).long()\n",
    "    target_sizes = torch.IntTensor(len(batch))\n",
    "    \n",
    "    for x in range(len(batch)):\n",
    "        sample = batch[x]\n",
    "        input_data = sample[0]\n",
    "        target = sample[1]\n",
    "        seq_length = input_data.size(1)\n",
    "        input_sizes[x] = seq_length\n",
    "        inputs[x][0].narrow(1, 0, seq_length).copy_(input_data)\n",
    "        input_percentages[x] = seq_length / float(max_seq_len)\n",
    "        target_sizes[x] = len(target)\n",
    "        targets[x][:len(target)] = torch.IntTensor(target)\n",
    "\n",
    "    return inputs, targets, input_percentages, input_sizes, target_sizes\n",
    "\n",
    "\n",
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn\n",
    "\n",
    "\n",
    "class BucketingSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size=1):\n",
    "        \"\"\"\n",
    "        Samples batches assuming they are in order of size to batch similarly \n",
    "        sized samples together.\n",
    "        \"\"\"\n",
    "        super(BucketingSampler, self).__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        ids = list(range(0, len(data_source)))\n",
    "        self.bins = [ids[i:i + batch_size]\n",
    "                     for i in range(0, len(ids), batch_size)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for ids in self.bins:\n",
    "            np.random.shuffle(ids)\n",
    "            yield ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bins)\n",
    "\n",
    "    def shuffle(self, epoch):\n",
    "        np.random.shuffle(self.bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LMDataset(Dataset):\n",
    "#     def __init__(self, path, label2id, id2label):\n",
    "#         self.label2id = label2id\n",
    "#         self.id2label = id2label\n",
    "#         self.texts, self.ids = self.read_manifest(path)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.ids)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.ids[index]\n",
    "        \n",
    "#     def read_manifest(self, path):\n",
    "#         \"\"\"Read manifest\"\"\"\n",
    "#         texts, ids = [], []\n",
    "#         with open(path, \"r\") as f:\n",
    "#             for line in f:\n",
    "#                 _, text_path = line.replace(\"\\n\", \"\").split(\",\")\n",
    "#                 with open(text_path, \"r\") as text_file:\n",
    "#                     for l in text_file:\n",
    "#                         texts.append(l.lower().replace(\"\\n\", \"\"))\n",
    "\n",
    "#             for text in texts:\n",
    "#                 for char in text:\n",
    "#                     if char not in self.label2id:\n",
    "#                         print(\">\", char)\n",
    "#                 ids.append(list(filter(None, [self.label2id.get(x) for x in list(text)])))\n",
    "#         return texts, ids    \n",
    "\n",
    "# def _collate_fn(batch):\n",
    "#     def func(p):\n",
    "#         return len(p)\n",
    "\n",
    "#     # print(\">\", batch)\n",
    "#     batch = sorted(batch, key=lambda x: len(x), reverse=True)\n",
    "#     # print(\">>\", batch)\n",
    "\n",
    "#     max_seq_len = len(max(batch, key=func))\n",
    "#     # print(\"max_seq_len:\", max_seq_len)\n",
    "#     inputs = torch.zeros(len(batch), max_seq_len).long()\n",
    "#     input_sizes = torch.IntTensor(len(batch))\n",
    "\n",
    "#     for i in range(len(batch)):\n",
    "#         sample = batch[i]\n",
    "# #         ipdb.set_trace()\n",
    "#         inputs[i][:len(sample)] = torch.IntTensor(sample)\n",
    "\n",
    "#         seq_length = len(sample)\n",
    "#         input_sizes[i] = seq_length\n",
    "\n",
    "#     return inputs, input_sizes\n",
    "\n",
    "# class LMDataLoader(DataLoader):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(LMDataLoader, self).__init__(*args, **kwargs)\n",
    "#         self.collate_fn = _collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, opt, metrics, label2id, id2label, best_model=False):\n",
    "    \"\"\"\n",
    "    Saving model\n",
    "    \"\"\"\n",
    "    if best_model:\n",
    "        save_path = f\"{args.save_folder}/{args.name}/best_model.th\"\n",
    "    else:\n",
    "        save_path = f\"{args.save_folder}/{args.name}/epoch_{epoch}.th\"\n",
    "        \n",
    "\n",
    "    if not os.path.exists(args.save_folder + \"/\" + args.name):\n",
    "        os.makedirs(args.save_folder + \"/\" + args.name)\n",
    "\n",
    "    print(\"SAVE MODEL to\", save_path)\n",
    "    \n",
    "    args_ = {'label2id': label2id,\n",
    "             'id2label': id2label,\n",
    "             'args': args,\n",
    "             'epoch': epoch,\n",
    "             'model_state_dict': model.state_dict(),\n",
    "             'optimizer_state_dict': opt.optimizer.state_dict(),\n",
    "             'metrics': metrics\n",
    "            }     \n",
    "        \n",
    "    \n",
    "    if args.loss == \"ce\": # for cross-entropy \n",
    "        args_['optimizer_params'] = {\n",
    "                '_step': opt._step,\n",
    "                '_rate': opt._rate,\n",
    "                'warmup': opt.warmup,\n",
    "                'factor': opt.factor,\n",
    "                'model_size': opt.model_size\n",
    "            }\n",
    "        \n",
    "    elif args.loss == \"ctc\": # for CTC loss        \n",
    "        args_['optimizer_params'] = {\n",
    "                'lr': opt.lr,\n",
    "                'lr_anneal': opt.lr_anneal\n",
    "            }\n",
    "        \n",
    "    else:\n",
    "        print(\"Loss is not defined\")\n",
    "        \n",
    "    torch.save(args_, save_path)\n",
    "\n",
    "\n",
    "def load_model(load_path):\n",
    "    \"\"\"\n",
    "    Loading model\n",
    "    \n",
    "    args:\n",
    "        load_path: string\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(load_path)\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metrics']\n",
    "#     if 'args' in checkpoint:\n",
    "    args = checkpoint['args']\n",
    "\n",
    "    label2id = checkpoint['label2id']\n",
    "    id2label = checkpoint['id2label']\n",
    "\n",
    "    # initialize model \n",
    "    model = init_transformer_model(args, label2id, id2label)\n",
    "    # load the saved model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if args.cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # initialize the optimizer \n",
    "    opt = init_optimizer(args, model)\n",
    "    \n",
    "    if opt is not None:\n",
    "        opt.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if args.loss == \"ce\":\n",
    "            opt._step = checkpoint['optimizer_params']['_step']\n",
    "            opt._rate = checkpoint['optimizer_params']['_rate']\n",
    "            opt.warmup = checkpoint['optimizer_params']['warmup']\n",
    "            opt.factor = checkpoint['optimizer_params']['factor']\n",
    "            opt.model_size = checkpoint['optimizer_params']['model_size']\n",
    "        elif args.loss == \"ctc\":\n",
    "            opt.lr = checkpoint['optimizer_params']['lr']\n",
    "            opt.lr_anneal = checkpoint['optimizer_params']['lr_anneal']\n",
    "        else:\n",
    "            print(\"Need to define loss type\")\n",
    "\n",
    "    return model, opt, epoch, metrics, args, label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_lm_score(seq, lm, id2label):\n",
    "#     \"\"\"\n",
    "#     seq: (1, seq_len)\n",
    "#     id2label: map\n",
    "#     \"\"\"\n",
    "#     # print(\"hello\")\n",
    "#     seq_str = \"\".join(id2label[char.item()] for char in seq[0]).replace(\n",
    "#         args.PAD_CHAR, \"\").replace(args.SOS_CHAR, \"\").replace(args.EOS_CHAR, \"\")\n",
    "#     seq_str = seq_str.replace(\"  \", \" \")\n",
    "\n",
    "#     seq_arr = get_word_segments_per_language(seq_str)\n",
    "#     seq_str = \"\"\n",
    "#     for i in range(len(seq_arr)):\n",
    "#         if is_contain_chinese_word(seq_arr[i]):\n",
    "#             for char in seq_arr[i]:\n",
    "#                 if seq_str != \"\":\n",
    "#                     seq_str += \" \"\n",
    "#                 seq_str += char\n",
    "#         else:\n",
    "#             if seq_str != \"\":\n",
    "#                 seq_str += \" \"\n",
    "#             seq_str += seq_arr[i]\n",
    "\n",
    "#     # print(\"seq_str:\", seq_str)\n",
    "#     seq_str = seq_str.replace(\"  \", \" \").replace(\"  \", \" \")\n",
    "#     # print(\"seq str:\", seq_str)\n",
    "\n",
    "#     if seq_str == \"\":\n",
    "#         return -999, 0, 0\n",
    "\n",
    "#     score, oov_token = lm.evaluate(seq_str)    \n",
    "    \n",
    "#     # a, b = lm.evaluate(\"   improve     esperience\")\n",
    "#     # a2, b2 = lm.evaluate(\"   improve     experience\")\n",
    "#     # print(a, a2)\n",
    "#     return -1 * score / len(seq_str.split()) + 1, len(seq_str.split()) + 1, oov_token\n",
    "\n",
    "\n",
    "# class LM(object):\n",
    "#     def __init__(self, model_path):\n",
    "#         self.model_path = model_path\n",
    "#         print(\"load model path:\", self.model_path)\n",
    "\n",
    "#         checkpoint = torch.load(model_path)\n",
    "#         self.word2idx = checkpoint[\"word2idx\"]\n",
    "#         self.idx2word = checkpoint[\"idx2word\"]\n",
    "#         ntokens = checkpoint[\"ntoken\"]\n",
    "#         ninp = checkpoint[\"ninp\"]\n",
    "#         nhid = checkpoint[\"nhid\"]\n",
    "#         nlayers = checkpoint[\"nlayers\"]\n",
    "#         dropout = checkpoint[\"dropout\"]\n",
    "#         tie_weights = checkpoint[\"tie_weights\"]\n",
    "\n",
    "#         self.model = RNNModel(\"LSTM\", ntoken=ntokens, ninp=ninp, nhid=nhid,\n",
    "#                               nlayers=nlayers, dropout=dropout, tie_weights=tie_weights)\n",
    "#         self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "#         if args.cuda:\n",
    "#             self.model = self.model.cuda()\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     def batchify(self, data, bsz, cuda):\n",
    "#         # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "#         nbatch = data.size(0) // bsz\n",
    "#         # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "#         data = data.narrow(0, 0, nbatch * bsz)\n",
    "#         # Evenly divide the data across the bsz batches.\n",
    "#         data = data.view(bsz, -1).t().contiguous()\n",
    "#         if cuda:\n",
    "#             data = data.cuda()\n",
    "#         return data\n",
    "\n",
    "#     def seq_to_tensor(self, seq):\n",
    "#         words = seq.split() + ['<eos>']\n",
    "\n",
    "#         ids = torch.LongTensor(len(words))\n",
    "#         token = 0\n",
    "#         oov_token = 0\n",
    "#         for word in words:\n",
    "#             if word in self.word2idx:\n",
    "#                 ids[token] = self.word2idx[word]\n",
    "#             else:\n",
    "#                 ids[token] = self.word2idx['<oov>']\n",
    "#                 oov_token += 1\n",
    "#             # print(\">\", word, ids[token])\n",
    "#             token += 1\n",
    "#         # print(\"ids\", ids)\n",
    "#         return ids, oov_token\n",
    "\n",
    "#     def get_batch(self, source, i, bptt, seq_len=None, evaluation=False):\n",
    "#         seq_len = min(seq_len if seq_len else bptt, len(source) - 1 - i)\n",
    "#         data = source[i:i+seq_len]\n",
    "#         target = source[i+1:i+1+seq_len].view(-1)\n",
    "#         return data, target\n",
    "\n",
    "#     def evaluate(self, seq):\n",
    "#         \"\"\"\n",
    "#         batch_size = 1\n",
    "#         \"\"\"\n",
    "#         tensor, oov_token = self.seq_to_tensor(seq)\n",
    "#         data_source = self.batchify(tensor\n",
    "#             , 1, args.cuda)\n",
    "#         self.model.eval()\n",
    "\n",
    "#         total_loss = 0\n",
    "#         ntokens = len(self.word2idx)\n",
    "#         hidden = self.model.init_hidden(1)\n",
    "#         data, targets = self.get_batch(\n",
    "#             data_source, 0, data_source.size(0), evaluation=True)\n",
    "#         output, hidden = self.model(data, hidden)\n",
    "\n",
    "#         # calculate probability\n",
    "#         # print(output.size()) # seq_len, vocab\n",
    "\n",
    "#         output_flat = output.view(-1, ntokens)\n",
    "#         total_loss += len(data) * self.criterion(output_flat, targets).data\n",
    "#         hidden = self.repackage_hidden(hidden)\n",
    "#         return total_loss, oov_token\n",
    "\n",
    "#     def repackage_hidden(self, h):\n",
    "#         \"\"\"Wraps hidden states in new Tensors,\n",
    "#         to detach them from their history.\"\"\"\n",
    "#         if isinstance(h, torch.Tensor):\n",
    "#             return h.detach()\n",
    "#         else:\n",
    "#             return tuple(self.repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "# class RNNModel(nn.Module):\n",
    "#     \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "#     def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "#         super(RNNModel, self).__init__()\n",
    "#         self.drop = nn.Dropout(dropout)\n",
    "#         self.encoder = nn.Embedding(ntoken, ninp)\n",
    "\n",
    "#         if rnn_type in ['LSTM', 'GRU']:\n",
    "#             self.rnn = getattr(nn, rnn_type)(\n",
    "#                 ninp, nhid, nlayers, dropout=dropout)\n",
    "#         else:\n",
    "#             try:\n",
    "#                 nonlinearity = {'RNN_TANH': 'tanh',\n",
    "#                                 'RNN_RELU': 'relu'}[rnn_type]\n",
    "#             except KeyError:\n",
    "#                 raise ValueError(\"\"\"An invalid option for `--model` was supplied,\n",
    "#                                  options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "#             self.rnn = nn.RNN(ninp, nhid, nlayers,\n",
    "#                               nonlinearity=nonlinearity, dropout=dropout)\n",
    "\n",
    "#         self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "#         # Optionally tie weights as in:\n",
    "#         # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "#         # https://arxiv.org/abs/1608.05859\n",
    "#         # and\n",
    "#         # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "#         # https://arxiv.org/abs/1611.01462\n",
    "#         if tie_weights:\n",
    "#             if nhid != ninp:\n",
    "#                 raise ValueError(\n",
    "#                     'When using the tied flag, nhid must be equal to emsize')\n",
    "#             self.decoder.weight = self.encoder.weight\n",
    "\n",
    "#         self.rnn_type = rnn_type\n",
    "#         self.nhid = nhid\n",
    "#         self.nlayers = nlayers\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.1\n",
    "#         self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.decoder.bias.data.fill_(0)\n",
    "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "#     def forward(self, input, hidden):\n",
    "#         emb = self.drop(self.encoder(input))\n",
    "\n",
    "#         output, hidden = self.rnn(emb, hidden)\n",
    "#         output = self.drop(output)\n",
    "\n",
    "#         decoded = self.decoder(output.view(\n",
    "#             output.size(0)*output.size(1), output.size(2)))\n",
    "#         return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "#     def init_hidden(self, bsz):\n",
    "#         weight = next(self.parameters()).data\n",
    "#         if self.rnn_type == 'LSTM':\n",
    "#             return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "#                     Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "#         else:\n",
    "#             return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as Lev\n",
    "\n",
    "# from data.helper import get_word_segments_per_language, is_contain_chinese_word\n",
    "\n",
    "def calculate_cer_en_zh(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence (hyp)\n",
    "        s2 (string): space-separated sentence (gold)\n",
    "    \"\"\"\n",
    "    s1_segments = get_word_segments_per_language(s1)\n",
    "    s2_segments = get_word_segments_per_language(s2)\n",
    "\n",
    "    en_s1_seq, en_s2_seq = \"\", \"\"\n",
    "    zh_s1_seq, zh_s2_seq = \"\", \"\"\n",
    "\n",
    "    for segment in s1_segments:\n",
    "        if is_contain_chinese_word(segment):\n",
    "            if zh_s1_seq != \"\":\n",
    "                zh_s1_seq += \" \"\n",
    "            zh_s1_seq += segment\n",
    "        else:\n",
    "            if en_s1_seq != \"\":\n",
    "                en_s1_seq += \" \"\n",
    "            en_s1_seq += segment\n",
    "    \n",
    "    for segment in s2_segments:\n",
    "        if is_contain_chinese_word(segment):\n",
    "            if zh_s2_seq != \"\":\n",
    "                zh_s2_seq += \" \"\n",
    "            zh_s2_seq += segment\n",
    "        else:\n",
    "            if en_s2_seq != \"\":\n",
    "                en_s2_seq += \" \"\n",
    "            en_s2_seq += segment\n",
    "\n",
    "#     print(\">\", en_s1_seq, \"||\", en_s2_seq, len(en_s2_seq), \"||\", calculate_cer(en_s1_seq, en_s2_seq) / max(1, len(en_s2_seq.replace(' ', ''))))\n",
    "    # print(\">>\", zh_s1_seq, \"||\", zh_s2_seq, len(zh_s2_seq), \"||\", calculate_cer(zh_s1_seq, zh_s2_seq) /  max(1, len(zh_s2_seq.replace(' ', ''))))\n",
    "\n",
    "    return calculate_cer(en_s1_seq, en_s2_seq), calculate_cer(zh_s1_seq, zh_s2_seq), len(en_s2_seq), len(zh_s2_seq)\n",
    "\n",
    "def calculate_cer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence (hyp)\n",
    "        s2 (string): space-separated sentence (gold)\n",
    "    \"\"\"\n",
    "    return Lev.distance(s1, s2)\n",
    "\n",
    "def calculate_wer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    \n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "def calculate_metrics(pred, gold, \n",
    "                      input_lengths=None, target_lengths=None, smoothing=0.0, loss_type=\"ce\"):\n",
    "    \"\"\"\n",
    "    Calculate metrics\n",
    "    \n",
    "    args:\n",
    "        pred: B x T x C\n",
    "        gold: B x T\n",
    "        input_lengths: B (for CTC)\n",
    "        target_lengths: B (for CTC)\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(pred, gold, input_lengths, target_lengths, smoothing, loss_type)\n",
    "    if loss_type == \"ce\":\n",
    "        pred = pred.view(-1, pred.size(2)) # (B*T) x C\n",
    "        gold = gold.contiguous().view(-1) # (B*T)\n",
    "        pred = pred.max(1)[1]\n",
    "        non_pad_mask = gold.ne(args.PAD_TOKEN)\n",
    "        num_correct = pred.eq(gold)\n",
    "        num_correct = num_correct.masked_select(non_pad_mask).sum().item()\n",
    "        return loss, num_correct\n",
    "    elif loss_type == \"ctc\":\n",
    "        return loss, None\n",
    "    else:\n",
    "        print(\"loss is not defined\")\n",
    "        return None, None\n",
    "\n",
    "def calculate_loss(pred, gold, \n",
    "                   input_lengths=None, target_lengths=None, smoothing=0.0, loss_type=\"ce\"):\n",
    "    \"\"\"\n",
    "    Calculate loss\n",
    "    \n",
    "    args:\n",
    "        pred: B x T x C\n",
    "        gold: B x T\n",
    "        input_lengths: B (for CTC)\n",
    "        target_lengths: B (for CTC)\n",
    "        smoothing:\n",
    "        type: ce|ctc (ctc => pytorch 1.0.0 or later)\n",
    "        input_lengths: B (only for ctc)\n",
    "        target_lengths: B (only for ctc)\n",
    "    \"\"\"\n",
    "    if loss_type == \"ce\":\n",
    "        pred = pred.view(-1, pred.size(2)) # (B*T) x C\n",
    "        gold = gold.contiguous().view(-1) # (B*T)\n",
    "        if smoothing > 0.0:\n",
    "            eps = smoothing\n",
    "            num_class = pred.size(1)\n",
    "\n",
    "            gold_for_scatter = gold.ne(args.PAD_TOKEN).long() * gold\n",
    "            one_hot = torch.zeros_like(pred).scatter(1, gold_for_scatter.view(-1, 1), 1)\n",
    "            one_hot = one_hot * (1-eps) + (1-one_hot) * eps / num_class\n",
    "            log_prob = F.log_softmax(pred, dim=1)\n",
    "\n",
    "            non_pad_mask = gold.ne(args.PAD_TOKEN)\n",
    "            num_word = non_pad_mask.sum().item()\n",
    "            loss = -(one_hot * log_prob).sum(dim=1)\n",
    "            loss = loss.masked_select(non_pad_mask).sum() / num_word\n",
    "        else:\n",
    "            loss = F.cross_entropy(pred, gold, ignore_index=args.PAD_TOKEN, reduction=\"mean\")\n",
    "    elif loss_type == \"ctc\":\n",
    "        log_probs = pred.transpose(0, 1) # T x B x C\n",
    "        # print(gold.size())\n",
    "        targets = gold\n",
    "        # targets = gold.contiguous().view(-1) # (B*T)\n",
    "\n",
    "        \"\"\"\n",
    "        log_probs: torch.Size([209, 8, 3793])\n",
    "        targets: torch.Size([8, 46])\n",
    "        input_lengths: torch.Size([8])\n",
    "        target_lengths: torch.Size([8])\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"log_probs:\", log_probs.size())\n",
    "        # print(\"targets:\", targets.size())\n",
    "        # print(\"input_lengths:\", input_lengths.size())\n",
    "        # print(\"target_lengths:\", target_lengths.size())\n",
    "        # print(input_lengths)\n",
    "        # print(target_lengths)\n",
    "\n",
    "        log_probs = F.log_softmax(log_probs, dim=2)\n",
    "        loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction=\"mean\")\n",
    "        # mask = loss.clone() # mask Inf loss\n",
    "        # # mask[mask != float(\"Inf\")] = 1\n",
    "        # mask[mask == float(\"Inf\")] = 0\n",
    "\n",
    "        # loss = mask\n",
    "        # print(loss)\n",
    "\n",
    "        # loss_size = len(loss)\n",
    "        # loss = loss.sum() / loss_size\n",
    "        # print(loss)\n",
    "    else:\n",
    "        print(\"loss is not defined\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer, min_lr=1e-5):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"Implement `lrate` above\"\n",
    "        step = self._step\n",
    "        return max(self.min_lr, self.factor * \\\n",
    "            (self.model_size ** (-0.5) * min(step **\n",
    "                                             (-0.5), step * self.warmup ** (-1.5))))\n",
    "\n",
    "class AnnealingOpt:\n",
    "    \"Optim wrapper for annealing opt\"\n",
    "\n",
    "    def __init__(self, lr, lr_anneal, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.lr_anneal = lr_anneal\n",
    "    \n",
    "    def step(self):\n",
    "        optim_state = self.optimizer.state_dict()\n",
    "        optim_state['param_groups'][0]['lr'] = optim_state['param_groups'][0]['lr'] / self.lr_anneal\n",
    "        self.optimizer.load_state_dict(optim_state)\n",
    "\n",
    "class SGDOpt:\n",
    "    \"Optim wrapper that implements SGD\"\n",
    "    \n",
    "    def __init__(self, parameters, lr, momentum, nesterov=True):\n",
    "        self.optimizer = torch.optim.SGD(parameters, lr=lr, momentum=momentum, nesterov=nesterov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "\n",
    "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "def load_stanford_core_nlp(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load stanford core NLP toolkit object\n",
    "    args:\n",
    "        path: String\n",
    "    output:\n",
    "        Stanford core NLP objects\n",
    "    \"\"\"\n",
    "    zh_nlp = StanfordCoreNLP(path, lang='zh')\n",
    "    en_nlp = StanfordCoreNLP(path, lang='en')\n",
    "    return zh_nlp, en_nlp\n",
    "\n",
    "\"\"\"\n",
    "################################################\n",
    "TEXT PREPROCESSING\n",
    "################################################\n",
    "\"\"\"\n",
    "\n",
    "def is_chinese_char(cc):\n",
    "    \"\"\"\n",
    "    Check if the character is Chinese\n",
    "    args:\n",
    "        cc: char\n",
    "    output:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    return unicodedata.category(cc) == 'Lo'\n",
    "\n",
    "def is_contain_chinese_word(seq):\n",
    "    \"\"\"\n",
    "    Check if the sequence has chinese character(s)\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    for i in range(len(seq)):\n",
    "        if is_chinese_char(seq[i]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_word_segments_per_language(seq):\n",
    "    \"\"\"\n",
    "    Get word segments \n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        word_segments: list of String\n",
    "    \"\"\"\n",
    "    cur_lang = -1 # cur_lang = 0 (english), 1 (chinese)\n",
    "    words = seq.split(\" \")\n",
    "    temp_words = \"\"\n",
    "    word_segments = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "\n",
    "        if is_contain_chinese_word(word):\n",
    "            if cur_lang == -1:\n",
    "                cur_lang = 1\n",
    "                temp_words = word\n",
    "            elif cur_lang == 0: # english\n",
    "                cur_lang = 1\n",
    "                word_segments.append(temp_words)\n",
    "                temp_words = word\n",
    "            else:\n",
    "                if temp_words != \"\":\n",
    "                    temp_words += \" \"\n",
    "                temp_words += word\n",
    "        else:\n",
    "            if cur_lang == -1:\n",
    "                cur_lang = 0\n",
    "                temp_words = word\n",
    "            elif cur_lang == 1: # chinese\n",
    "                cur_lang = 0\n",
    "                word_segments.append(temp_words)\n",
    "                temp_words = word\n",
    "            else:\n",
    "                if temp_words != \"\":\n",
    "                    temp_words += \" \"\n",
    "                temp_words += word\n",
    "\n",
    "    word_segments.append(temp_words)\n",
    "\n",
    "    return word_segments\n",
    "\n",
    "def get_word_segments_per_language_with_tokenization(seq, tokenize_lang=-1, \n",
    "                                                     zh_nlp=None, en_nlp=None):\n",
    "    \"\"\"\n",
    "    Get word segments and tokenize the sequence for selected language\n",
    "    We cannot run two different languages on stanford core nlp, will be very slow\n",
    "    so instead we do it as many times as the number of languages we want to tokenize\n",
    "    args:\n",
    "        seq: String\n",
    "        tokenize_lang: int (-1 means no language is selected, 0 (english), 1 (chinese))\n",
    "    \"\"\"\n",
    "    cur_lang = -1\n",
    "    words = seq.split(\" \")\n",
    "    temp_words = \"\"\n",
    "    word_segments = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "\n",
    "        if is_contain_chinese_word(word):\n",
    "            if cur_lang == -1:\n",
    "                cur_lang = 1\n",
    "                temp_words = word\n",
    "            elif cur_lang == 0: # english\n",
    "                cur_lang = 1\n",
    "\n",
    "                if tokenize_lang == 0:\n",
    "                    word_list = en_nlp.word_tokenize(temp_words)\n",
    "                    temp_words = ' '.join(word for word in word_list)\n",
    "\n",
    "                word_segments.append(temp_words)\n",
    "                temp_words = word\n",
    "            else:\n",
    "                if temp_words != \"\":\n",
    "                    temp_words += \" \"\n",
    "                temp_words += word\n",
    "        else:\n",
    "            if cur_lang == -1:\n",
    "                cur_lang = 0\n",
    "                temp_words = word\n",
    "            elif cur_lang == 1: # chinese\n",
    "                cur_lang = 0\n",
    "\n",
    "                if tokenize_lang == 1:\n",
    "                    word_list = zh_nlp.word_tokenize(temp_words.replace(\" \",\"\"))\n",
    "                    temp_words = ' '.join(word for word in word_list)\n",
    "\n",
    "                word_segments.append(temp_words)\n",
    "                temp_words = word\n",
    "            else:\n",
    "                if temp_words != \"\":\n",
    "                    temp_words += \" \"\n",
    "                temp_words += word\n",
    "\n",
    "    if tokenize_lang == 0 and cur_lang == 0:\n",
    "        word_list = en_nlp.word_tokenize(temp_words)\n",
    "        temp_words = ' '.join(word for word in word_list)\n",
    "    elif tokenize_lang == 1 and cur_lang == 1:\n",
    "        word_list = zh_nlp.word_tokenize(temp_words)\n",
    "        temp_words = ' '.join(word for word in word_list)\n",
    "\n",
    "    word_segments.append(temp_words)\n",
    "\n",
    "    # word_seq = \"\"\n",
    "    # for i in range(len(word_segments)):\n",
    "    #     if word_seq != \"\":\n",
    "    #         word_seq += \" \"\n",
    "    #     else:\n",
    "    #         word_seq = word_segments[i]\n",
    "\n",
    "    return word_segments\n",
    "\n",
    "def remove_emojis(seq):\n",
    "    \"\"\"\n",
    "    Remove emojis\n",
    "    \n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    seq = emoji_pattern.sub(r'', seq).strip()\n",
    "    return seq\n",
    "\n",
    "def merge_abbreviation(seq):\n",
    "    seq = seq.replace(\"  \", \" \")\n",
    "    words = seq.split(\" \")\n",
    "    final_seq = \"\"\n",
    "    temp = \"\"\n",
    "    for i in range(len(words)):\n",
    "        word_length = len(words[i])\n",
    "        if word_length == 0: # unknown character case\n",
    "            continue\n",
    "\n",
    "        if words[i][word_length-1] == \".\":\n",
    "            temp += words[i]\n",
    "        else:\n",
    "            if temp != \"\":\n",
    "                if final_seq != \"\":\n",
    "                    final_seq += \" \"\n",
    "                final_seq += temp\n",
    "                temp = \"\"\n",
    "            if final_seq != \"\":\n",
    "                final_seq += \" \"\n",
    "            final_seq += words[i]\n",
    "    if temp != \"\":\n",
    "        if final_seq != \"\":\n",
    "            final_seq += \" \"\n",
    "        final_seq += temp\n",
    "    return final_seq\n",
    "\n",
    "def remove_punctuation(seq):\n",
    "    \"\"\"\n",
    "    Remove english and chinese punctuation except hypen/dash, and full stop.\n",
    "    Also fix some typos and encoding issues\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    seq = re.sub(\"[\\s+\\\\!\\/_,$%=^*?:@&^~`(+\\\"]+|[+~@#%&*:;()]+\", \" \", seq)\n",
    "    seq = seq.replace(\" ' \", \" \")\n",
    "    seq = seq.replace(\"  \", \" \")\n",
    "    seq = seq.replace(\"  \", \" \")\n",
    "    seq = seq.replace(\" ` \", \" \")\n",
    "\n",
    "    seq = seq.replace(\" '\", \"'\")\n",
    "    seq = seq.replace(\" \", \"\")\n",
    "    seq = seq.replace(\" \", \"\")\n",
    "\n",
    "    seq = seq.replace(\"' \", \" \")\n",
    "    seq = seq.replace(\" \", \" \")\n",
    "    seq = seq.replace(\" \", \" \")\n",
    "    seq = seq.replace(\"` \", \" \")\n",
    "    seq = seq.replace(\".\", \"\")\n",
    "\n",
    "    seq = seq.replace(\"`\", \"\")\n",
    "    seq = seq.replace(\"-\", \" \")\n",
    "    seq = seq.replace(\"?\", \" \")\n",
    "    seq = seq.replace(\":\", \" \")\n",
    "    seq = seq.replace(\";\", \" \")\n",
    "    seq = seq.replace(\"]\", \" \")\n",
    "    seq = seq.replace(\"[\", \" \")\n",
    "    seq = seq.replace(\"}\", \" \")\n",
    "    seq = seq.replace(\"{\", \" \")\n",
    "    seq = seq.replace(\"|\", \" \")\n",
    "    seq = seq.replace(\"_\", \" \")\n",
    "    seq = seq.replace(\"(\", \" \")\n",
    "    seq = seq.replace(\")\", \" \")\n",
    "    seq = seq.replace(\"=\", \" \")\n",
    "\n",
    "    seq = seq.replace(\" dont \", \" don't \")\n",
    "    seq = seq.replace(\"welcome\", \"welcome \")\n",
    "    seq = seq.replace(\"doens't\", \"doesn't\")\n",
    "    seq = seq.replace(\"o' clock\", \"o'clock\")\n",
    "    seq = seq.replace(\"it's\", \" it's\")\n",
    "    seq = seq.replace(\"it' s\", \"it's\")\n",
    "    seq = seq.replace(\"it ' s\", \"it's\")\n",
    "    seq = seq.replace(\"it' s\", \"it's\")\n",
    "    seq = seq.replace(\"y'\", \"y\")\n",
    "    seq = seq.replace(\"y ' \", \"y\")\n",
    "    seq = seq.replace(\"different\", \" different\")\n",
    "    seq = seq.replace(\"it'self\", \"itself\")\n",
    "    seq = seq.replace(\"it'ss\", \"it's\")\n",
    "    seq = seq.replace(\"don'r\", \"don't\")\n",
    "    seq = seq.replace(\"has't\", \"hasn't\")\n",
    "    seq = seq.replace(\"don'know\", \"don't know\")\n",
    "    seq = seq.replace(\"i'll\", \"i will\")\n",
    "    seq = seq.replace(\"you're\", \"you are\")\n",
    "    seq = seq.replace(\"'re \", \" are \")\n",
    "    seq = seq.replace(\"'ll \", \" will \")\n",
    "    seq = seq.replace(\"'ve \", \" have \")\n",
    "    seq = seq.replace(\"'re\\n\", \" are\\n\")\n",
    "    seq = seq.replace(\"'ll\\n\", \" will\\n\")\n",
    "    seq = seq.replace(\"'ve\\n\", \" have\\n\")\n",
    "\n",
    "    seq = remove_space_in_between_words(seq)\n",
    "    return seq\n",
    "\n",
    "def remove_special_char(seq):\n",
    "    \"\"\"\n",
    "    Remove special characters from the corpus\n",
    "    \n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    seq = re.sub(\"[\u0001]+\", \" \", seq)\n",
    "    return seq\n",
    "\n",
    "def remove_space_in_between_words(seq):\n",
    "    \"\"\"\n",
    "    Remove space between words\n",
    "    \n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    return seq.replace(\"  \", \" \").replace(\"  \", \" \").replace(\"  \", \" \").replace(\"  \", \" \").strip().lstrip()\n",
    "\n",
    "def remove_return(seq):\n",
    "    \"\"\"\n",
    "    Remove return characters\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    return seq.replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"\\t\", \"\")\n",
    "\n",
    "def preprocess_mixed_language_sentence(seq, tokenize=False, \n",
    "                                       en_nlp=None, zh_nlp=None, tokenize_lang=-1):\n",
    "    \"\"\"\n",
    "    Preprocess function\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    if len(seq) == 0:\n",
    "        return \"\"\n",
    "        \n",
    "    seq = seq.lower()\n",
    "    seq = merge_abbreviation(seq)\n",
    "    seq = seq.replace(\"\\x7f\", \"\")\n",
    "    seq = seq.replace(\"\\x80\", \"\")\n",
    "    seq = seq.replace(\"\\u3000\", \" \")\n",
    "    seq = seq.replace(\"\\xa0\", \"\")\n",
    "    seq = seq.replace(\"[\", \" [\")\n",
    "    seq = seq.replace(\"]\", \"] \")\n",
    "    seq = seq.replace(\"#\", \"\")\n",
    "    seq = seq.replace(\",\", \"\")\n",
    "    seq = seq.replace(\"*\", \"\")\n",
    "    seq = seq.replace(\"\\n\", \"\")\n",
    "    seq = seq.replace(\"\\r\", \"\")\n",
    "    seq = seq.replace(\"\\t\", \"\")\n",
    "    seq = seq.replace(\"~\", \"\")\n",
    "    seq = seq.replace(\"\", \"\")\n",
    "    seq = seq.replace(\"  \", \" \").replace(\"  \", \" \")\n",
    "    seq = re.sub('\\<.*?\\>','', seq) # REMOVE < >\n",
    "    seq = re.sub('\\.*?\\','', seq) # REMOVE  \n",
    "    seq = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", seq) # REMOVE ALL WORDS WITH BRACKETS (HESITATION)\n",
    "    seq = re.sub(\"[\\{\\[].*?[\\}\\]]\", \"\", seq) # REMOVE ALL WORDS WITH BRACKETS (HESITATION)\n",
    "    seq = remove_special_char(seq)\n",
    "    seq = remove_space_in_between_words(seq)\n",
    "    seq = seq.strip()\n",
    "    seq = seq.lstrip()\n",
    "    \n",
    "    seq = remove_punctuation(seq)\n",
    "\n",
    "    temp_words =  \"\"\n",
    "    if not tokenize:\n",
    "        segments = get_word_segments_per_language(seq)\n",
    "    else:\n",
    "        segments = get_word_segments_per_language_with_tokenization(seq, en_nlp=en_nlp, zh_nlp=zh_nlp, tokenize_lang=tokenize_lang)\n",
    "\n",
    "    for j in range(len(segments)):\n",
    "        if not is_contain_chinese_word(segments[j]):\n",
    "            segments[j] = re.sub(r'[^\\x00-\\x7f]',r' ',segments[j])\n",
    "\n",
    "        if temp_words != \"\":\n",
    "            temp_words += \" \"\n",
    "        temp_words += segments[j].replace(\"\\n\", \"\")\n",
    "    seq = temp_words\n",
    "\n",
    "    seq = remove_space_in_between_words(seq)\n",
    "    seq = seq.strip()\n",
    "    seq = seq.lstrip()\n",
    "\n",
    "    # Tokenize chinese characters\n",
    "    if len(seq) <= 1:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return seq\n",
    "\n",
    "\"\"\"\n",
    "################################################\n",
    "AUDIO PREPROCESSING\n",
    "################################################\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_wav(root, dirc, filename):\n",
    "    source_audio = root + \"/\" + dirc + \"/audio/\" + filename + \".flac\"\n",
    "\n",
    "    with open(root + \"/\" + dirc + \"/proc_transcript/phaseII/\" + filename + \".txt\", \"r\", encoding=\"utf-8\") as transcript_file:\n",
    "        part_num = 0\n",
    "        for line in transcript_file:\n",
    "            data = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            start_time = float(data[1]) / 1000\n",
    "            end_time = float(data[2]) / 1000\n",
    "            dif_time = end_time-start_time\n",
    "            text = data[4]\n",
    "            target_flac_audio = root + \"/parts/\" + dirc + \"/flac/\" + filename + \"_\" + str(part_num) + \".flac\"\n",
    "            target_wav_audio = root + \"/parts/\" + dirc + \"/wav/\" + filename + \"_\" + str(part_num) + \".wav\"\n",
    "            # print(\"sox \" + source_audio + \" \" + target_flac_audio + \" trim \" + str(start_time) + \" \" + str(dif_time))\n",
    "\n",
    "            pipe = subprocess.check_output(\"sox \" + source_audio + \" \" +\\\n",
    "                                           target_flac_audio + \" trim \" + str(start_time) \\\n",
    "                                           + \" \" + str(dif_time), shell=True)\n",
    "            try:\n",
    "                # print(\"sox \" + target_flac_audio + \" \" + target_wav_audio)\n",
    "                out2 = os.popen(\"sox \" + target_flac_audio + \" \" + target_wav_audio).read()\n",
    "                sound, _ = torchaudio.load(target_wav_audio)\n",
    "\n",
    "                # print(\"Write transcript\")\n",
    "                with open(root + \"/parts/\" + dirc + \"/proc_transcript/\" + filename + \\\n",
    "                          \"_\" + str(part_num) + \".txt\", \"w+\", encoding=\"utf-8\") as text_file:\n",
    "                    text_file.write(text + \"\\n\")\n",
    "            except:\n",
    "                print(\"Error reading audio file: unknown length, the audio is not with proper length, skip, target_flac_audio {}\", target_flac_audio)\n",
    "\n",
    "            part_num += 1\n",
    "\n",
    "\"\"\"\n",
    "################################################\n",
    "COMMON FUNCTIONS\n",
    "################################################\n",
    "\"\"\"\n",
    "\n",
    "def traverse(root, path, dev_conversation_phase2, \n",
    "             test_conversation_phase2, dev_interview_phase2, \n",
    "             test_interview_phase2, search_fix=\".txt\"):\n",
    "    \n",
    "    f_train_list = []\n",
    "    f_dev_list = []\n",
    "    f_test_list = []\n",
    "\n",
    "    p = root + path\n",
    "    for sub_p in sorted(os.listdir(p)):\n",
    "        if sub_p[len(sub_p)-len(search_fix):] == search_fix:\n",
    "            if \"conversation\" in path:\n",
    "                print(\">\", path, sub_p)\n",
    "                if sub_p[2:6] in dev_conversation_phase2:\n",
    "                    f_dev_list.append(p + \"/\" + sub_p)\n",
    "                elif sub_p[2:6] in test_conversation_phase2:\n",
    "                    f_test_list.append(p + \"/\" + sub_p)\n",
    "                else:\n",
    "                    f_train_list.append(p + \"/\" + sub_p)\n",
    "            elif \"interview\" in path:\n",
    "                print(\">\", path, sub_p)\n",
    "                if sub_p[:4] in dev_interview_phase2:\n",
    "                    f_dev_list.append(p + \"/\" + sub_p)\n",
    "                elif sub_p[:4] in test_interview_phase2:\n",
    "                    f_test_list.append(p + \"/\" + sub_p)\n",
    "                else:\n",
    "                    f_train_list.append(p + \"/\" + sub_p)\n",
    "            else:\n",
    "                print(\"hoho\")\n",
    "\n",
    "    return f_train_list, f_dev_list, f_test_list\n",
    "\n",
    "def traverse_all(root, path):\n",
    "    f_list = []\n",
    "\n",
    "    p = root + path\n",
    "    for sub_p in sorted(os.listdir(p)):\n",
    "        f_list.append(p + \"/\" + sub_p)\n",
    "\n",
    "    return f_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wget\n",
    "# import tarfile\n",
    "# import argparse\n",
    "# import shutil\n",
    "# import ipdb\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Processes and downloads LibriSpeech dataset.')\n",
    "# parser.add_argument(\"--target-dir\", default='LibriSpeech_dataset/', type=str, help=\"Directory to store the dataset.\")\n",
    "# parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "# parser.add_argument('--files-to-use', default=\"train-clean-100.tar.gz,\"\n",
    "#                                               \"train-clean-360.tar.gz,train-other-500.tar.gz,\"\n",
    "#                                               \"dev-clean.tar.gz,dev-other.tar.gz,\"\n",
    "#                                               \"test-clean.tar.gz,test-other.tar.gz\", type=str,\n",
    "#                     help='list of file names to download')\n",
    "# parser.add_argument('--min-duration', default=1, type=int,\n",
    "#                     help='Prunes training samples shorter than the min duration (given in seconds, default 1)')\n",
    "# parser.add_argument('--max-duration', default=15, type=int,\n",
    "#                     help='Prunes training samples longer than the max duration (given in seconds, default 15)')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# LIBRI_SPEECH_URLS = {\n",
    "#     \"train\": [\"http://www.openslr.org/resources/12/train-clean-100.tar.gz\",\n",
    "#               \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\",\n",
    "#               \"http://www.openslr.org/resources/12/train-other-500.tar.gz\"],\n",
    "\n",
    "#     \"val\": [\"http://www.openslr.org/resources/12/dev-clean.tar.gz\",\n",
    "#             \"http://www.openslr.org/resources/12/dev-other.tar.gz\"],\n",
    "\n",
    "#     \"test_clean\": [\"http://www.openslr.org/resources/12/test-clean.tar.gz\"],\n",
    "#     \"test_other\": [\"http://www.openslr.org/resources/12/test-other.tar.gz\"]\n",
    "# }\n",
    "\n",
    "\n",
    "# def _preprocess_transcript(phrase):\n",
    "#     return phrase.strip().lower()\n",
    "\n",
    "\n",
    "# def _process_file(wav_dir, txt_dir, base_filename, root_dir):\n",
    "#     full_recording_path = os.path.join(root_dir, base_filename)\n",
    "#     assert os.path.exists(full_recording_path) and os.path.exists(root_dir)\n",
    "#     wav_recording_path = os.path.join(wav_dir, base_filename.replace(\".flac\", \".wav\"))\n",
    "    \n",
    "# #     subprocess.call([\"sox {}  -r {} -b 16 -c 1 {}\".format(full_recording_path, str(args.sample_rate),\n",
    "# #                                                           wav_recording_path)], shell=True)\n",
    "    \n",
    "#     subprocess.call([f\"sox {full_recording_path}  -r {str(args.sample_rate)} -b 16 -c 1 {wav_recording_path}\"], shell=True)\n",
    "    \n",
    "# #     ipdb.set_trace()\n",
    "#     # process transcript\n",
    "#     txt_transcript_path = os.path.join(txt_dir, base_filename.replace(\".flac\", \".txt\"))\n",
    "#     transcript_file = os.path.join(root_dir, \"-\".join(base_filename.split('-')[:-1]) + \".trans.txt\")\n",
    "#     assert os.path.exists(transcript_file), \"Transcript file {} does not exist.\".format(transcript_file)\n",
    "#     transcriptions = open(transcript_file).read().strip().split(\"\\n\")\n",
    "#     transcriptions = {t.split()[0].split(\"-\")[-1]: \" \".join(t.split()[1:]) for t in transcriptions}\n",
    "#     with open(txt_transcript_path, \"w\") as f:\n",
    "#         key = base_filename.replace(\".flac\", \"\").split(\"-\")[-1]\n",
    "#         assert key in transcriptions, \"{} is not in the transcriptions\".format(key)\n",
    "#         f.write(_preprocess_transcript(transcriptions[key]))\n",
    "#         f.flush()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     target_dl_dir = args.target_dir\n",
    "#     if not os.path.exists(target_dl_dir):\n",
    "#         os.makedirs(target_dl_dir)\n",
    "#     files_to_dl = args.files_to_use.strip().split(',')\n",
    "#     for split_type, lst_libri_urls in LIBRI_SPEECH_URLS.items():\n",
    "#         split_dir = os.path.join(target_dl_dir, split_type)\n",
    "#         if not os.path.exists(split_dir):\n",
    "#             os.makedirs(split_dir)\n",
    "#         split_wav_dir = os.path.join(split_dir, \"wav\")\n",
    "#         if not os.path.exists(split_wav_dir):\n",
    "#             os.makedirs(split_wav_dir)\n",
    "#         split_txt_dir = os.path.join(split_dir, \"txt\")\n",
    "#         if not os.path.exists(split_txt_dir):\n",
    "#             os.makedirs(split_txt_dir)\n",
    "#         extracted_dir = os.path.join(split_dir, \"LibriSpeech\")\n",
    "#         if os.path.exists(extracted_dir):\n",
    "#             shutil.rmtree(extracted_dir)\n",
    "#         for url in lst_libri_urls:\n",
    "#             # check if we want to dl this file\n",
    "#             dl_flag = False\n",
    "#             for f in files_to_dl:\n",
    "#                 if url.find(f) != -1:\n",
    "#                     dl_flag = True\n",
    "#             if not dl_flag:\n",
    "#                 print(\"Skipping url: {}\".format(url))\n",
    "#                 continue\n",
    "#             filename = url.split(\"/\")[-1]\n",
    "#             target_filename = os.path.join(split_dir, filename)\n",
    "# #             if not os.path.exists(target_filename):\n",
    "# #                 wget.download(url, split_dir)\n",
    "#             print(\"Unpacking {}...\".format(filename))\n",
    "#             tar = tarfile.open(target_filename)\n",
    "#             tar.extractall(split_dir)\n",
    "#             tar.close()\n",
    "#             os.remove(target_filename)\n",
    "#             print(\"Converting flac files to wav and extracting transcripts...\")\n",
    "#             assert os.path.exists(extracted_dir), \"Archive {} was not properly uncompressed.\".format(filename)\n",
    "#             for root, subdirs, files in tqdm(os.walk(extracted_dir)):\n",
    "#                 for f in files:\n",
    "#                     if f.find(\".flac\") != -1:\n",
    "#                         _process_file(wav_dir=split_wav_dir, txt_dir=split_txt_dir,\n",
    "#                                       base_filename=f, root_dir=root)\n",
    "\n",
    "#             print(\"Finished {}\".format(url))\n",
    "#             shutil.rmtree(extracted_dir)\n",
    "#         if split_type == 'train':  # Prune to min/max duration\n",
    "#             create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv', args.min_duration, args.max_duration)\n",
    "#         else:\n",
    "#             create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv')\n",
    "\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parser = argparse.ArgumentParser(description='Processes and downloads LibriSpeech dataset.')\n",
    "# parser.add_argument(\"--target-dir\", default='LibriSpeech_dataset/', type=str, help=\"Directory to store the dataset.\")\n",
    "# parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "# parser.add_argument('--files-to-use', default=\"train-clean-100.tar.gz,\"\n",
    "#                                               \"train-clean-360.tar.gz,train-other-500.tar.gz,\"\n",
    "#                                               \"dev-clean.tar.gz,dev-other.tar.gz,\"\n",
    "#                                               \"test-clean.tar.gz,test-other.tar.gz\", type=str,\n",
    "#                     help='list of file names to download')\n",
    "# parser.add_argument('--min-duration', default=1, type=int,\n",
    "#                     help='Prunes training samples shorter than the min duration (given in seconds, default 1)')\n",
    "# parser.add_argument('--max-duration', default=15, type=int,\n",
    "#                     help='Prunes training samples longer than the max duration (given in seconds, default 15)')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# LIBRI_SPEECH_URLS = {\n",
    "#     \"train\": [\"http://www.openslr.org/resources/12/train-clean-100.tar.gz\",\n",
    "#               \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\",\n",
    "#               \"http://www.openslr.org/resources/12/train-other-500.tar.gz\"],\n",
    "\n",
    "#     \"val\": [\"http://www.openslr.org/resources/12/dev-clean.tar.gz\",\n",
    "#             \"http://www.openslr.org/resources/12/dev-other.tar.gz\"],\n",
    "\n",
    "#     \"test_clean\": [\"http://www.openslr.org/resources/12/test-clean.tar.gz\"],\n",
    "#     \"test_other\": [\"http://www.openslr.org/resources/12/test-other.tar.gz\"]\n",
    "# }\n",
    "\n",
    "\n",
    "# def _preprocess_transcript(phrase):\n",
    "#     return phrase.strip().lower()\n",
    "\n",
    "\n",
    "# def _process_file(wav_dir, txt_dir, base_filename, root_dir):\n",
    "#     full_recording_path = os.path.join(root_dir, base_filename)\n",
    "#     assert os.path.exists(full_recording_path) and os.path.exists(root_dir)\n",
    "#     wav_recording_path = os.path.join(wav_dir, base_filename.replace(\".flac\", \".wav\"))\n",
    "# #     subprocess.call([\"sox {}  -r {} -b 16 -c 1 {}\".format(full_recording_path, str(args.sample_rate),\n",
    "# #                                                           wav_recording_path)], shell=True)\n",
    "    \n",
    "#     subprocess.call([f\"sox {full_recording_path}  -r {str(args.sample_rate)} -b 16 -c 1 {wav_recording_path}\"], shell=True)\n",
    "    \n",
    "#     # process transcript\n",
    "#     txt_transcript_path = os.path.join(txt_dir, base_filename.replace(\".flac\", \".txt\"))\n",
    "#     transcript_file = os.path.join(root_dir, \"-\".join(base_filename.split('-')[:-1]) + \".trans.txt\")\n",
    "#     assert os.path.exists(transcript_file), \"Transcript file {} does not exist.\".format(transcript_file)\n",
    "#     transcriptions = open(transcript_file).read().strip().split(\"\\n\")\n",
    "#     transcriptions = {t.split()[0].split(\"-\")[-1]: \" \".join(t.split()[1:]) for t in transcriptions}\n",
    "#     with open(txt_transcript_path, \"w\") as f:\n",
    "#         key = base_filename.replace(\".flac\", \"\").split(\"-\")[-1]\n",
    "#         assert key in transcriptions, \"{} is not in the transcriptions\".format(key)\n",
    "#         f.write(_preprocess_transcript(transcriptions[key]))\n",
    "#         f.flush()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     target_dl_dir = args.target_dir\n",
    "#     if not os.path.exists(target_dl_dir):\n",
    "#         os.makedirs(target_dl_dir)\n",
    "#     files_to_dl = args.files_to_use.strip().split(',')\n",
    "#     for split_type, lst_libri_urls in LIBRI_SPEECH_URLS.items():\n",
    "#         split_dir = os.path.join(target_dl_dir, split_type)\n",
    "#         if not os.path.exists(split_dir):\n",
    "#             os.makedirs(split_dir)\n",
    "#         split_wav_dir = os.path.join(split_dir, \"wav\")\n",
    "#         if not os.path.exists(split_wav_dir):\n",
    "#             os.makedirs(split_wav_dir)\n",
    "#         split_txt_dir = os.path.join(split_dir, \"txt\")\n",
    "#         if not os.path.exists(split_txt_dir):\n",
    "#             os.makedirs(split_txt_dir)\n",
    "#         extracted_dir = os.path.join(split_dir, \"LibriSpeech\")\n",
    "#         if os.path.exists(extracted_dir):\n",
    "#             shutil.rmtree(extracted_dir)\n",
    "#         for url in lst_libri_urls:\n",
    "#             # check if we want to dl this file\n",
    "#             dl_flag = False\n",
    "#             for f in files_to_dl:\n",
    "#                 if url.find(f) != -1:\n",
    "#                     dl_flag = True\n",
    "#             if not dl_flag:\n",
    "#                 print(\"Skipping url: {}\".format(url))\n",
    "#                 continue\n",
    "#             filename = url.split(\"/\")[-1]\n",
    "#             target_filename = os.path.join(split_dir, filename)\n",
    "#             if not os.path.exists(target_filename):\n",
    "#                 wget.download(url, split_dir)\n",
    "#             print(\"Unpacking {}...\".format(filename))\n",
    "#             tar = tarfile.open(target_filename)\n",
    "#             tar.extractall(split_dir)\n",
    "#             tar.close()\n",
    "#             os.remove(target_filename)\n",
    "#             print(\"Converting flac files to wav and extracting transcripts...\")\n",
    "#             assert os.path.exists(extracted_dir), \"Archive {} was not properly uncompressed.\".format(filename)\n",
    "#             for root, subdirs, files in tqdm(os.walk(extracted_dir)):\n",
    "#                 for f in files:\n",
    "#                     if f.find(\".flac\") != -1:\n",
    "#                         _process_file(wav_dir=split_wav_dir, txt_dir=split_txt_dir,\n",
    "#                                       base_filename=f, root_dir=root)\n",
    "\n",
    "#             print(\"Finished {}\".format(url))\n",
    "#             shutil.rmtree(extracted_dir)\n",
    "#         if split_type == 'train':  # Prune to min/max duration\n",
    "#             create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv', args.min_duration, args.max_duration)\n",
    "#         else:\n",
    "#             create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv')\n",
    "\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_SPACE_CHARACTERS = ['\\n', '\\t', '\\r']\n",
    "\n",
    "def generate_label_from_corpora(corpus_paths, output_path=None, lower_case=True):\n",
    "    \"\"\"Generating label data from a given corpus folder file path(s)\n",
    "\n",
    "    This function will generate label data by performing character level tokenization \n",
    "    over all path in the specified `corpus_paths` and store the result as a json formatted \n",
    "    file in the specified `output_path`. If path is a folder, the label file will be generated \n",
    "    based on all files with `.txt` format inside the given path recursively.\n",
    "\n",
    "    Args:\n",
    "        corpus_paths (list[str]): list of file or folder path of the text corpus\n",
    "        output_path (str): file path for the generated label file\n",
    "        lower_case (bool): flag for performing lower case on the text data\n",
    "\n",
    "    Returns:\n",
    "        list(str): The label list generated from the given `corpus_paths`\n",
    "    \"\"\"\n",
    "    \n",
    "    label_set = set()\n",
    "    for corpus_path in corpus_paths:\n",
    "        label_set |= retrieve_label_from_corpus(corpus_path, lower_case)\n",
    "    label_list = list(label_set)\n",
    "\n",
    "    if output_path:\n",
    "        with open(output_path, 'w') as outfile:\n",
    "            json.dump(label_list, outfile, ensure_ascii=False)\n",
    "\n",
    "    return label_list\n",
    "\n",
    "def retrieve_label_from_corpus(corpus_path, lower_case=True):\n",
    "    \"\"\"Retrieve all unique character labels from a given corpus folder or file path\n",
    "\n",
    "    This function will generate json label file by performing character level \n",
    "    tokenization over `corpus_path`. If `corpus_`path is a folder, the character labels\n",
    "    will be retrieved from  all files with `.txt` format inside the given `corpus_path`.\n",
    "\n",
    "    Args:\n",
    "        corpus_path (str): list of file or folder path of the text corpus\n",
    "        lower_case (bool): flag for performing lower case on the text data\n",
    "\n",
    "    Returns:\n",
    "        set(str): The return character labels\n",
    "    \"\"\"\n",
    "    label_set = set()\n",
    "    if os.path.isdir(corpus_path):\n",
    "        # Recursive search over folder\n",
    "        for f_path in os.listdir(corpus_path):\n",
    "            f_path = '{}/{}'.format(corpus_path, f_path)\n",
    "            if  os.path.isdir(f_path) or f_path[-4:] == '.txt':\n",
    "                label_set |= retrieve_label_from_corpus(f_path)\n",
    "            else:\n",
    "                # Skip non-folder and non-txt file\n",
    "                pass\n",
    "    elif corpus_path[-4:] == '.txt':\n",
    "        # Perform character level tokenization over corpus\n",
    "        with open(corpus_path,'r') as corpus_file:\n",
    "            data = corpus_file.read()\n",
    "\n",
    "            # Turn special character to space\n",
    "            for c in SPECIAL_SPACE_CHARACTERS:\n",
    "                data = data.replace(c,' ')\n",
    "\n",
    "            # Perform lower case if needed\n",
    "            if lower_case:\n",
    "                data = data.lower()\n",
    "\n",
    "            # Add to result set\n",
    "            label_set |= set(data)\n",
    "    else:\n",
    "        # Skip non-folder and non-txt file\n",
    "        pass\n",
    "    return label_set\n",
    "\n",
    "def create_manifest(data_path, output_path, min_duration=None, max_duration=None):\n",
    "    file_paths = [os.path.join(dirpath, f)\n",
    "                  for dirpath, dirnames, files in os.walk(data_path)\n",
    "                  for f in fnmatch.filter(files, '*.wav')]\n",
    "#     ipdb.set_trace()\n",
    "    file_paths = order_and_prune_files(file_paths, min_duration, max_duration)\n",
    "    with io.FileIO(output_path, \"w\") as file:\n",
    "        for wav_path in tqdm(file_paths, total=len(file_paths)):\n",
    "            transcript_path = wav_path.replace('/wav/', '/txt/').replace('.wav', '.txt')\n",
    "            sample = os.path.abspath(wav_path) + ',' + os.path.abspath(transcript_path) + '\\n'\n",
    "            file.write(sample.encode('utf-8'))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def order_and_prune_files(file_paths, min_duration, max_duration):\n",
    "    print(\"Sorting manifests...\")\n",
    "    duration_file_paths = [(path, float(subprocess.check_output(\n",
    "        ['soxi -D \\\"%s\\\"' % path.strip()], shell=True))) for path in file_paths]\n",
    "    if min_duration and max_duration:\n",
    "        print(\"Pruning manifests between %d and %d seconds\" % (min_duration, max_duration))\n",
    "        duration_file_paths = [(path, duration) for path, duration in duration_file_paths if\n",
    "                               min_duration <= duration <= max_duration]\n",
    "\n",
    "    def func(element):\n",
    "        return element[1]\n",
    "\n",
    "    duration_file_paths.sort(key=func)\n",
    "    return [x[0] for x in duration_file_paths]  # Remove durations\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Test for generating label file\n",
    "#     print('Test Gen Label File')\n",
    "#     print(generate_label_from_corpora(['./test.txt'], output_path=None, lower_case=True))\n",
    "\n",
    "#     print('Test Gen Label File No Lower Case')\n",
    "#     print(generate_label_from_corpora(['./test.txt'], output_path=None, lower_case=False))\n",
    "\n",
    "#     print('Test Gen Label File to Json file')\n",
    "#     print(generate_label_from_corpora(['./test.txt'], output_path='./label_file.json', lower_case=True))\n",
    "\n",
    "#     print('Test Gen Label Folder')\n",
    "#     print(generate_label_from_corpora(['./test.txt', './test_folder'], output_path=None, lower_case=True))\n",
    "\n",
    "#     print('Test Gen Label Folder No Lower Case')\n",
    "#     print(generate_label_from_corpora(['./test.txt', './test_folder'], output_path=None, lower_case=False))\n",
    "\n",
    "#     print('Test Gen Label Folder to Json file')\n",
    "#     print(generate_label_from_corpora(['./test.txt', './test_folder'], output_path='./label_folder.json', lower_case=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as I\n",
    "\n",
    "\"\"\"\n",
    "General purpose functions\n",
    "\"\"\"\n",
    "\n",
    "def pad_list(xs, pad_value):\n",
    "    \"\"\"\n",
    "    Given a list of Tensor and the pad_value\n",
    "    the function will return a tensor of shape [len[xs], max_len]\n",
    "    \n",
    "    args:\n",
    "        xs        : List[Tensor]\n",
    "        pad_value : value to pad\n",
    "    output:\n",
    "        pad       : Tensor[len[xs], max_len]\n",
    "    \"\"\"\n",
    "    # From: espnet/src/nets/e2e_asr_th.py: pad_list()\n",
    "    n_batch = len(xs)\n",
    "    # max_len = max(x.size(0) for x in xs)\n",
    "    max_len = args.tgt_max_len # hypper-parameter here\n",
    "    pad = xs[0].new(n_batch, max_len, * xs[0].size()[1:]).fill_(pad_value)\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        pad[i, :xs[i].size(0)] = xs[i]\n",
    "        \n",
    "    return pad\n",
    "\n",
    "\"\"\" \n",
    "Transformer common layers\n",
    "\"\"\"\n",
    "\n",
    "def get_non_pad_mask(padded_input, input_lengths=None, pad_idx=None):\n",
    "    \"\"\"\n",
    "    padding position is set to 0, either use input_lengths or pad_idx\n",
    "    \n",
    "    args:\n",
    "        padded_input   : Tensor[B, T [, ...]]\n",
    "        input_lengths  : Tensor[B]\n",
    "        pad_idx        : int\n",
    "    output:\n",
    "        non_pad_mask   : Tensor[B, T, 1]\n",
    "        \n",
    "    \"\"\"\n",
    "    assert input_lengths is not None or pad_idx is not None\n",
    "    if input_lengths is not None:\n",
    "        # padded_input: N x T x ..\n",
    "        N = padded_input.size(0)\n",
    "        non_pad_mask = padded_input.new_ones(padded_input.size()[:-1])  # B x T\n",
    "        for i in range(N):\n",
    "            non_pad_mask[i, input_lengths[i]:] = 0\n",
    "    if pad_idx is not None:\n",
    "        # padded_input: N x T\n",
    "        assert padded_input.dim() == 2\n",
    "        # Give True when  padded_input != pad_idx element-wise. or\n",
    "        # if pad_idx is not None, check \n",
    "        # https://pytorch.org/docs/stable/generated/torch.ne.html \n",
    "        non_pad_mask = padded_input.ne(pad_idx).float()\n",
    "        \n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "    # unsqueeze(-1) for broadcast\n",
    "    return non_pad_mask.unsqueeze(-1)\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q, pad_idx):\n",
    "    \"\"\"\n",
    "    For masking out the padding part of key sequence.\n",
    "    \n",
    "    args:\n",
    "        seq_k        : Tensor[B, T [, ...]]\n",
    "        seq_q        : Tensor[B, T [, ...]]\n",
    "        pad_idx      : int\n",
    "    output:\n",
    "        padding_mask : Tensor[B, T, T]\n",
    "        \n",
    "    \"\"\"\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = seq_k.eq(pad_idx)\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # B x T_Q x T_K\n",
    "\n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "    return padding_mask\n",
    "\n",
    "def get_attn_pad_mask(padded_input, input_lengths, expand_length):\n",
    "    \"\"\"\n",
    "    mask position is set to 1 (True)\n",
    "    \n",
    "    args:\n",
    "        padded_input   : Tensor[B, T, ...]\n",
    "        input_lengths  : Tensor[B]\n",
    "        expand_length  : int[T]\n",
    "    output:\n",
    "        attn_mask      : Tensor[B, T, T]\n",
    "        \n",
    "    \"\"\"\n",
    "    # N x Ti x 1\n",
    "    non_pad_mask = get_non_pad_mask(padded_input, input_lengths=input_lengths)\n",
    "    # N x Ti, lt(1) like not operation\n",
    "    pad_mask = non_pad_mask.squeeze(-1).lt(1)\n",
    "    attn_mask = pad_mask.unsqueeze(1).expand(-1, expand_length, -1)\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "    return attn_mask\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    \"\"\"\n",
    "    For masking out the subsequent info. \n",
    "    \n",
    "    args:\n",
    "        seq             : Tensor[B, T [, ...]]\n",
    "\n",
    "    output:\n",
    "        subsequent_mask : Tensor[B, T, T]\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = torch.triu(\n",
    "        torch.ones((len_s, len_s), device=seq.device, dtype=torch.uint8), diagonal=1)\n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n",
    "\n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding class\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, max_length=2000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_length, dim_model, requires_grad=False)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "        exp_term = torch.exp(torch.arange(0, dim_model, 2).float() * -(math.log(10000.0) / dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * exp_term) # take the odd (jump by 2)\n",
    "        pe[:, 1::2] = torch.cos(position * exp_term) # take the even (jump by 2)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            input: B x T x D\n",
    "        output:\n",
    "            tensor: B x T\n",
    "        \"\"\"\n",
    "        return self.pe[:, :input.size(1)]\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feedforward Layer class\n",
    "    FFN(x) = max(0, xW1 + b1) W2+ b2\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, dim_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(dim_model, dim_ff)\n",
    "        self.linear_2 = nn.Linear(dim_ff, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: tensor\n",
    "        output:\n",
    "            y: tensor\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        output = self.dropout(self.linear_2(F.relu(self.linear_1(x))))\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output\n",
    "\n",
    "class PositionwiseFeedForwardWithConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feedforward Layer Implementation with Convolution class\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, dim_hidden, dropout=0.1):\n",
    "        super(PositionwiseFeedForwardWithConv, self).__init__()\n",
    "        self.conv_1 = nn.Conv1d(dim_model, dim_hidden, 1)\n",
    "        self.conv_2 = nn.Conv1d(dim_hidden, dim_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.conv_2(F.relu(self.conv_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_model, dim_key, dim_value, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_value = dim_value\n",
    "\n",
    "        self.query_linear = nn.Linear(dim_model, num_heads * dim_key)\n",
    "        self.key_linear = nn.Linear(dim_model, num_heads * dim_key)\n",
    "        self.value_linear = nn.Linear(dim_model, num_heads * dim_value)\n",
    "\n",
    "        nn.init.normal_(self.query_linear.weight, mean=0, std=np.sqrt(2.0 / (self.dim_model + self.dim_key)))\n",
    "        nn.init.normal_(self.key_linear.weight, mean=0, std=np.sqrt(2.0 / (self.dim_model + self.dim_key)))\n",
    "        nn.init.normal_(self.value_linear.weight, mean=0, std=np.sqrt(2.0 / (self.dim_model + self.dim_value)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(dim_key, 0.5), attn_dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "        self.output_linear = nn.Linear(num_heads * dim_value, dim_model)\n",
    "        nn.init.xavier_normal_(self.output_linear.weight)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: B x T_Q x H, key: B x T_K x H, value: B x T_V x H\n",
    "        mask: B x T x T (attention mask)\n",
    "        \"\"\"\n",
    "        batch_size, len_query, _ = query.size()\n",
    "        batch_size, len_key, _ = key.size()\n",
    "        batch_size, len_value, _ = value.size()\n",
    "\n",
    "        residual = query\n",
    "\n",
    "        query = self.query_linear(query).view(batch_size, len_query, self.num_heads, self.dim_key) # B x T_Q x num_heads x H_K\n",
    "        key = self.key_linear(key).view(batch_size, len_key, self.num_heads, self.dim_key) # B x T_K x num_heads x H_K\n",
    "        value = self.value_linear(value).view(batch_size, len_value, self.num_heads, self.dim_value) # B x T_V x num_heads x H_V\n",
    "\n",
    "        query = query.permute(2, 0, 1, 3).contiguous().view(-1, len_query, self.dim_key) # (num_heads * B) x T_Q x H_K\n",
    "        key = key.permute(2, 0, 1, 3).contiguous().view(-1, len_key, self.dim_key) # (num_heads * B) x T_K x H_K\n",
    "        value = value.permute(2, 0, 1, 3).contiguous().view(-1, len_value, self.dim_value) # (num_heads * B) x T_V x H_V\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(self.num_heads, 1, 1) # (B * num_head) x T x T\n",
    "        \n",
    "        output, attn = self.attention(query, key, value, mask=mask)\n",
    "\n",
    "        output = output.view(self.num_heads, batch_size, len_query, self.dim_value) # num_heads x B x T_Q x H_V\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(batch_size, len_query, -1) # B x T_Q x (num_heads * H_V)\n",
    "\n",
    "        output = self.dropout(self.output_linear(output)) # B x T_Q x H_O\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask.bool(), -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\"\"\"\n",
    "LAS common layers\n",
    "\"\"\"\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Dot product attention.\n",
    "    Given a set of vector values, and a vector query, attention is a technique\n",
    "    to compute a weighted sum of the values, dependent on the query.\n",
    "    NOTE: Here we use the terminology in Stanford cs224n-2018-lecture11.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        # TODO: move this out of this class?\n",
    "        # self.linear_out = nn.Linear(dim*2, dim)\n",
    "\n",
    "    def forward(self, queries, values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries: N x To x H\n",
    "            values : N x Ti x H\n",
    "        Returns:\n",
    "            output: N x To x H\n",
    "            attention_distribution: N x To x Ti\n",
    "        \"\"\"\n",
    "        batch_size = queries.size(0)\n",
    "        hidden_size = queries.size(2)\n",
    "        input_lengths = values.size(1)\n",
    "        # (N, To, H) * (N, H, Ti) -> (N, To, Ti)\n",
    "        attention_scores = torch.bmm(queries, values.transpose(1, 2))\n",
    "        attention_distribution = F.softmax(\n",
    "            attention_scores.view(-1, input_lengths), dim=1).view(batch_size, -1, input_lengths)\n",
    "        # (N, To, Ti) * (N, Ti, H) -> (N, To, H)\n",
    "        attention_output = torch.bmm(attention_distribution, values)\n",
    "        # # concat -> (N, To, 2*H)\n",
    "        # concated = torch.cat((attention_output, queries), dim=2)\n",
    "        # # TODO: Move this out of this class?\n",
    "        # # output -> (N, To, H)\n",
    "        # output = torch.tanh(self.linear_out(\n",
    "        #     concated.view(-1, 2*hidden_size))).view(batch_size, -1, hidden_size)\n",
    "\n",
    "        return attention_output, attention_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder Transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 num_heads, \n",
    "                 dim_model, \n",
    "                 dim_key, \n",
    "                 dim_value, \n",
    "                 dim_input, \n",
    "                 dim_inner, \n",
    "                 dropout=0.1, \n",
    "                 src_max_length=2500):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.dim_input = dim_input\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_value = dim_value\n",
    "        self.dim_inner = dim_inner\n",
    "\n",
    "        self.src_max_length = src_max_length\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        self.input_linear = nn.Linear(dim_input, dim_model)\n",
    "        self.layer_norm_input = nn.LayerNorm(dim_model)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            dim_model, src_max_length)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(num_heads, dim_model, dim_inner, dim_key, dim_value, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, padded_input, input_lengths):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            padded_input: B x T x D\n",
    "            input_lengths: B\n",
    "        return:\n",
    "            output: B x T x H\n",
    "        \"\"\"\n",
    "        encoder_self_attn_list = []\n",
    "\n",
    "        # Prepare masks\n",
    "        non_pad_mask = get_non_pad_mask(padded_input, input_lengths=input_lengths)  # B x T x D\n",
    "        seq_len = padded_input.size(1)\n",
    "        self_attn_mask = get_attn_pad_mask(padded_input, input_lengths, seq_len)  # B x T x T\n",
    "\n",
    "        encoder_output = self.layer_norm_input(self.input_linear(\n",
    "            padded_input)) + self.positional_encoding(padded_input)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            encoder_output, self_attn = layer(\n",
    "                encoder_output, non_pad_mask=non_pad_mask, self_attn_mask=self_attn_mask)\n",
    "            encoder_self_attn_list += [self_attn]\n",
    "\n",
    "        return encoder_output, encoder_self_attn_list\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Layer Transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_heads, \n",
    "                 dim_model, \n",
    "                 dim_inner, \n",
    "                 dim_key, \n",
    "                 dim_value, \n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            num_heads, dim_model, dim_key, dim_value, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForwardWithConv(\n",
    "            dim_model, dim_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, non_pad_mask=None, self_attn_mask=None):\n",
    "        enc_output, self_attn = self.self_attn(\n",
    "            enc_input, enc_input, enc_input, mask=self_attn_mask)\n",
    "        enc_output *= non_pad_mask\n",
    "\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        enc_output *= non_pad_mask\n",
    "\n",
    "        return enc_output, self_attn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 id2label, \n",
    "                 num_src_vocab, \n",
    "                 num_trg_vocab, \n",
    "                 num_layers, \n",
    "                 num_heads, \n",
    "                 dim_emb, \n",
    "                 dim_model, \n",
    "                 dim_inner, \n",
    "                 dim_key, \n",
    "                 dim_value, \n",
    "                 dropout=0.1, \n",
    "                 trg_max_length=1000, \n",
    "                 emb_trg_sharing=False):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.sos_id = args.SOS_TOKEN\n",
    "        self.eos_id = args.EOS_TOKEN\n",
    "\n",
    "        self.id2label = id2label\n",
    "\n",
    "        self.num_src_vocab = num_src_vocab\n",
    "        self.num_trg_vocab = num_trg_vocab\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_inner = dim_inner\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_value = dim_value\n",
    "\n",
    "        self.dropout_rate = dropout\n",
    "        self.emb_trg_sharing = emb_trg_sharing\n",
    "\n",
    "        self.trg_max_length = trg_max_length\n",
    "\n",
    "        self.trg_embedding = nn.Embedding(num_trg_vocab, dim_emb, padding_idx=args.PAD_TOKEN)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            dim_model, max_length=trg_max_length)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(dim_model, dim_inner, num_heads,\n",
    "                         dim_key, dim_value, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_linear = nn.Linear(dim_model, num_trg_vocab, bias=False)\n",
    "        nn.init.xavier_normal_(self.output_linear.weight)\n",
    "\n",
    "        if emb_trg_sharing:\n",
    "            self.output_linear.weight = self.trg_embedding.weight\n",
    "            self.x_logit_scale = (dim_model ** -0.5)\n",
    "        else:\n",
    "            self.x_logit_scale = 1.0\n",
    "\n",
    "    def preprocess(self, padded_input):\n",
    "        \"\"\"\n",
    "        Add SOS TOKEN and EOS TOKEN into padded_input\n",
    "        \"\"\"\n",
    "        seq = [y[y != args.PAD_TOKEN] for y in padded_input]\n",
    "        eos = seq[0].new([self.eos_id])\n",
    "        sos = seq[0].new([self.sos_id])\n",
    "        seq_in = [torch.cat([sos, y], dim=0) for y in seq]\n",
    "        seq_out = [torch.cat([y, eos], dim=0) for y in seq]\n",
    "        seq_in_pad = pad_list(seq_in, self.eos_id)\n",
    "        seq_out_pad = pad_list(seq_out, args.PAD_TOKEN)\n",
    "        assert seq_in_pad.size() == seq_out_pad.size()\n",
    "        return seq_in_pad, seq_out_pad\n",
    "\n",
    "    def forward(self, padded_input, encoder_padded_outputs, encoder_input_lengths):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            padded_input: B x T\n",
    "            encoder_padded_outputs: B x T x H\n",
    "            encoder_input_lengths: B\n",
    "        returns:\n",
    "            pred: B x T x vocab\n",
    "            gold: B x T\n",
    "        \"\"\"\n",
    "        decoder_self_attn_list, decoder_encoder_attn_list = [], []\n",
    "        seq_in_pad, seq_out_pad = self.preprocess(padded_input)\n",
    "\n",
    "        # Prepare masks\n",
    "        non_pad_mask = get_non_pad_mask(seq_in_pad, pad_idx=args.EOS_TOKEN)\n",
    "        self_attn_mask_subseq = get_subsequent_mask(seq_in_pad)\n",
    "        self_attn_mask_keypad = get_attn_key_pad_mask(\n",
    "            seq_k=seq_in_pad, seq_q=seq_in_pad, pad_idx=args.EOS_TOKEN)\n",
    "        self_attn_mask = (self_attn_mask_keypad + self_attn_mask_subseq).gt(0)\n",
    "\n",
    "        output_length = seq_in_pad.size(1)\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(\n",
    "            encoder_padded_outputs, encoder_input_lengths, output_length)\n",
    "\n",
    "        decoder_output = self.dropout(self.trg_embedding(\n",
    "            seq_in_pad) * self.x_logit_scale + self.positional_encoding(seq_in_pad))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            decoder_output, decoder_self_attn, decoder_enc_attn = layer(\n",
    "                decoder_output, encoder_padded_outputs, non_pad_mask=non_pad_mask, self_attn_mask=self_attn_mask, dec_enc_attn_mask=dec_enc_attn_mask)\n",
    "\n",
    "            decoder_self_attn_list += [decoder_self_attn]\n",
    "            decoder_encoder_attn_list += [decoder_enc_attn]\n",
    "\n",
    "        seq_logit = self.output_linear(decoder_output)\n",
    "        pred, gold = seq_logit, seq_out_pad\n",
    "\n",
    "        return pred, gold, decoder_self_attn_list, decoder_encoder_attn_list\n",
    "    \n",
    "#     def predict(self, padded_input, encoder_padded_outputs, encoder_input_lengths):\n",
    "#         \"\"\"\n",
    "#         args:\n",
    "#             padded_input: B x T\n",
    "#             encoder_padded_outputs: B x T x H\n",
    "#             encoder_input_lengths: B\n",
    "#         returns:\n",
    "#             pred: B x T x vocab\n",
    "#             gold: B x T\n",
    "#         \"\"\"\n",
    "#         decoder_self_attn_list, decoder_encoder_attn_list = [], []\n",
    "#         seq_in_pad, seq_out_pad = self.preprocess(padded_input)\n",
    "\n",
    "#         Prepare masks\n",
    "#         non_pad_mask = get_non_pad_mask(seq_in_pad, pad_idx=args.EOS_TOKEN)\n",
    "#         self_attn_mask_subseq = get_subsequent_mask(seq_in_pad)\n",
    "#         self_attn_mask_keypad = get_attn_key_pad_mask(\n",
    "#             seq_k=seq_in_pad, seq_q=seq_in_pad, pad_idx=args.EOS_TOKEN)\n",
    "#         self_attn_mask = (self_attn_mask_keypad + self_attn_mask_subseq).gt(0)\n",
    "\n",
    "#         output_length = seq_in_pad.size(1)\n",
    "#         dec_enc_attn_mask = get_attn_pad_mask(\n",
    "#             encoder_padded_outputs, encoder_input_lengths, output_length)\n",
    "\n",
    "#         decoder_output = self.dropout(self.trg_embedding(\n",
    "#             seq_in_pad) * self.x_logit_scale + self.positional_encoding(seq_in_pad))\n",
    "\n",
    "#         for layer in self.layers:\n",
    "#             decoder_output, decoder_self_attn, decoder_enc_attn = layer(\n",
    "#                 decoder_output, encoder_padded_outputs, non_pad_mask=non_pad_mask, self_attn_mask=self_attn_mask, dec_enc_attn_mask=dec_enc_attn_mask)\n",
    "\n",
    "#             decoder_self_attn_list += [decoder_self_attn]\n",
    "#             decoder_encoder_attn_list += [decoder_enc_attn]\n",
    "\n",
    "#         seq_logit = self.output_linear(decoder_output)\n",
    "#         pred, gold = seq_logit, seq_out_pad\n",
    "\n",
    "#         return pred, gold, decoder_self_attn_list, decoder_encoder_attn_list\n",
    "\n",
    "    def post_process_hyp(self, hyp):\n",
    "        \"\"\"\n",
    "        args: \n",
    "            hyp: list of hypothesis\n",
    "        output:\n",
    "            list of hypothesis (string)>\n",
    "        \"\"\"\n",
    "        return \"\".join([self.id2label[int(x)] for x in hyp['yseq'][1:]])\n",
    "\n",
    "    def greedy_search(self, \n",
    "                      encoder_padded_outputs, \n",
    "                      beam_width=2, \n",
    "                      lm_rescoring=False, \n",
    "                      lm=None, \n",
    "                      lm_weight=0.1, \n",
    "                      c_weight=1):\n",
    "        \"\"\"\n",
    "        Greedy search, decode 1-best utterance\n",
    "        args:\n",
    "            encoder_padded_outputs: B x T x H\n",
    "        output:\n",
    "            batch_ids_nbest_hyps: list of nbest in ids (size B)\n",
    "            batch_strs_nbest_hyps: list of nbest in strings (size B)\n",
    "        \"\"\"\n",
    "        max_seq_len = self.trg_max_length\n",
    "        \n",
    "        ys = torch.ones(encoder_padded_outputs.size(0),1).fill_(args.SOS_TOKEN).long() # batch_size x 1\n",
    "        if args.cuda:\n",
    "            ys = ys.cuda()\n",
    "\n",
    "        decoded_words = []\n",
    "        for t in range(300):\n",
    "        # for t in range(max_seq_len):\n",
    "            # print(t)\n",
    "            # Prepare masks\n",
    "            non_pad_mask = torch.ones_like(ys).float().unsqueeze(-1) # batch_size x t x 1\n",
    "            self_attn_mask = get_subsequent_mask(ys) # batch_size x t x t\n",
    "\n",
    "            decoder_output = self.dropout(self.trg_embedding(ys) * self.x_logit_scale \n",
    "                                        + self.positional_encoding(ys))\n",
    "\n",
    "            for layer in self.layers:\n",
    "                decoder_output, _, _ = layer(\n",
    "                    decoder_output, encoder_padded_outputs,\n",
    "                    non_pad_mask=non_pad_mask,\n",
    "                    self_attn_mask=self_attn_mask,\n",
    "                    dec_enc_attn_mask=None\n",
    "                )\n",
    "\n",
    "            prob = self.output_linear(decoder_output) # batch_size x t x label_size\n",
    "            # _, next_word = torch.max(prob[:, -1], dim=1)\n",
    "            # decoded_words.append([constant.EOS_CHAR if ni.item() == constant.EOS_TOKEN else self.id2label[ni.item()] for ni in next_word.view(-1)])\n",
    "            # next_word = next_word.unsqueeze(-1)\n",
    "\n",
    "            # local_best_scores, local_best_ids = torch.topk(local_scores, beam_width, dim=1)\n",
    "\n",
    "            if lm_rescoring:\n",
    "                local_scores = F.log_softmax(prob, dim=1)\n",
    "                local_best_scores, local_best_ids = torch.topk(local_scores, beam_width, dim=1)\n",
    "\n",
    "                best_score = -1\n",
    "                best_word = None\n",
    "\n",
    "                # calculate beam scores\n",
    "                for j in range(beam_width):\n",
    "                    cur_seq = \" \".join(word for word in decoded_words)\n",
    "                    lm_score, num_words, oov_token = calculate_lm_score(cur_seq, lm, self.id2label)\n",
    "                    score = local_best_scores[0, j] + lm_score\n",
    "                    if best_score < score:\n",
    "                        best_score = score\n",
    "                        best_word = local_best_ids[0, j]\n",
    "                        next_word = best_word.unsqueeze(-1)\n",
    "                decoded_words.append(self.id2label[int(best_word)])\n",
    "            else:\n",
    "                _, next_word = torch.max(prob[:, -1], dim=1)\n",
    "                decoded_words.append([args.EOS_CHAR if ni.item() == args.EOS_TOKEN else self.id2label[ni.item()] for ni in next_word.view(-1)])\n",
    "                next_word = next_word.unsqueeze(-1)\n",
    "\n",
    "            if args.cuda:\n",
    "                ys = torch.cat([ys, next_word.cuda()], dim=1)\n",
    "                ys = ys.cuda()\n",
    "            else:\n",
    "                ys = torch.cat([ys, next_word], dim=1)\n",
    "\n",
    "        sent = []\n",
    "        for _, row in enumerate(np.transpose(decoded_words)):\n",
    "            st = ''\n",
    "            for e in row:\n",
    "                if e == args.EOS_CHAR: \n",
    "                    break\n",
    "                else: \n",
    "                    st += e\n",
    "            sent.append(st)\n",
    "        return sent\n",
    "\n",
    "    def beam_search(self, \n",
    "                    encoder_padded_outputs, \n",
    "                    beam_width=2, \n",
    "                    nbest=5, \n",
    "                    lm_rescoring=False, \n",
    "                    lm=None, \n",
    "                    lm_weight=0.1, \n",
    "                    c_weight=1, \n",
    "                    prob_weight=1.0):\n",
    "        \"\"\"\n",
    "        Beam search, decode nbest utterances\n",
    "        args:\n",
    "            encoder_padded_outputs: B x T x H\n",
    "            beam_size: int\n",
    "            nbest: int\n",
    "        output:\n",
    "            batch_ids_nbest_hyps: list of nbest in ids (size B)\n",
    "            batch_strs_nbest_hyps: list of nbest in strings (size B)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_padded_outputs.size(0)\n",
    "        max_len = encoder_padded_outputs.size(1)\n",
    "\n",
    "        batch_ids_nbest_hyps = []\n",
    "        batch_strs_nbest_hyps = []\n",
    "\n",
    "        for x in range(batch_size):\n",
    "            encoder_output = encoder_padded_outputs[x].unsqueeze(0) # 1 x T x H\n",
    "\n",
    "            # add SOS_TOKEN\n",
    "            ys = torch.ones(1, 1).fill_(args.SOS_TOKEN).type_as(encoder_output).long()\n",
    "            \n",
    "            hyp = {'score': 0.0, 'yseq':ys}\n",
    "            hyps = [hyp]\n",
    "            ended_hyps = []\n",
    "\n",
    "            for i in range(300):\n",
    "            # for i in range(self.trg_max_length):\n",
    "                hyps_best_kept = []\n",
    "                for hyp in hyps:\n",
    "                    ys = hyp['yseq'] # 1 x i\n",
    "\n",
    "                    # Prepare masks\n",
    "                    non_pad_mask = torch.ones_like(ys).float().unsqueeze(-1) # 1xix1\n",
    "                    self_attn_mask = get_subsequent_mask(ys)\n",
    "\n",
    "                    decoder_output = self.dropout(self.trg_embedding(ys) * self.x_logit_scale \n",
    "                                                + self.positional_encoding(ys))\n",
    "\n",
    "                    for layer in self.layers:\n",
    "                        # print(decoder_output.size(), encoder_output.size())\n",
    "                        decoder_output, _, _ = layer(\n",
    "                            decoder_output, encoder_output,\n",
    "                            non_pad_mask=non_pad_mask,\n",
    "                            self_attn_mask=self_attn_mask,\n",
    "                            dec_enc_attn_mask=None\n",
    "                        )\n",
    "\n",
    "                    seq_logit = self.output_linear(decoder_output[:, -1])\n",
    "                    local_scores = F.log_softmax(seq_logit, dim=1)\n",
    "                    local_best_scores, local_best_ids = torch.topk(local_scores, beam_width, dim=1)\n",
    "\n",
    "                    # calculate beam scores\n",
    "                    for j in range(beam_width):\n",
    "                        new_hyp = {}\n",
    "                        new_hyp[\"score\"] = hyp[\"score\"] + local_best_scores[0, j]\n",
    "\n",
    "                        new_hyp[\"yseq\"] = torch.ones(1, (1+ys.size(1))).type_as(encoder_output).long()\n",
    "                        new_hyp[\"yseq\"][:, :ys.size(1)] = hyp[\"yseq\"].cpu()\n",
    "                        new_hyp[\"yseq\"][:, ys.size(1)] = int(local_best_ids[0, j]) # adding new word\n",
    "                        \n",
    "                        hyps_best_kept.append(new_hyp)\n",
    "\n",
    "                    hyps_best_kept = sorted(hyps_best_kept, key=lambda x:x[\"score\"], reverse=True)[:beam_width]\n",
    "                \n",
    "                hyps = hyps_best_kept\n",
    "\n",
    "                # add EOS_TOKEN\n",
    "                if i == max_len - 1:\n",
    "                    for hyp in hyps:\n",
    "                        hyp[\"yseq\"] = torch.cat([hyp[\"yseq\"], torch.ones(1,1).fill_(args.EOS_TOKEN).type_as(encoder_output).long()], dim=1)\n",
    "\n",
    "                # add hypothesis that have EOS_TOKEN to ended_hyps list\n",
    "                unended_hyps = []\n",
    "                for hyp in hyps:\n",
    "                    if hyp[\"yseq\"][0, -1] == args.EOS_TOKEN:\n",
    "                        if lm_rescoring:\n",
    "                            # seq_str = \"\".join(self.id2label[char.item()] for char in hyp[\"yseq\"][0]).replace(constant.PAD_CHAR,\"\").replace(constant.SOS_CHAR,\"\").replace(constant.EOS_CHAR,\"\")\n",
    "                            # seq_str = seq_str.replace(\"  \", \" \")\n",
    "                            # num_words = len(seq_str.split())\n",
    "\n",
    "                            hyp[\"lm_score\"], hyp[\"num_words\"], oov_token = calculate_lm_score(hyp[\"yseq\"], lm, self.id2label)\n",
    "                            num_words = hyp[\"num_words\"]\n",
    "                            hyp[\"lm_score\"] -= oov_token * 2\n",
    "                            hyp[\"final_score\"] = hyp[\"score\"] + lm_weight * hyp[\"lm_score\"] + math.sqrt(num_words) * c_weight\n",
    "                        else:\n",
    "                            seq_str = \"\".join(self.id2label[char.item()] for char in hyp[\"yseq\"][0]).replace(args.PAD_CHAR,\"\").replace(args.SOS_CHAR,\"\").replace(args.EOS_CHAR,\"\")\n",
    "                            seq_str = seq_str.replace(\"  \", \" \")\n",
    "                            num_words = len(seq_str.split())\n",
    "                            hyp[\"final_score\"] = hyp[\"score\"] + math.sqrt(num_words) * c_weight\n",
    "                        \n",
    "                        ended_hyps.append(hyp)\n",
    "                        \n",
    "                    else:\n",
    "                        unended_hyps.append(hyp)\n",
    "                hyps = unended_hyps\n",
    "\n",
    "                if len(hyps) == 0:\n",
    "                    # decoding process is finished\n",
    "                    break\n",
    "                \n",
    "            num_nbest = min(len(ended_hyps), nbest)\n",
    "            nbest_hyps = sorted(ended_hyps, key=lambda x:x[\"final_score\"], reverse=True)[:num_nbest]\n",
    "\n",
    "            a_nbest_hyps = sorted(ended_hyps, key=lambda x:x[\"final_score\"], reverse=True)[:beam_width]\n",
    "\n",
    "            if lm_rescoring:\n",
    "                for hyp in a_nbest_hyps:\n",
    "                    seq_str = \"\".join(self.id2label[char.item()] for char in hyp[\"yseq\"][0]).replace(args.PAD_CHAR,\"\").replace(args.SOS_CHAR,\"\").replace(args.EOS_CHAR,\"\")\n",
    "                    seq_str = seq_str.replace(\"  \", \" \")\n",
    "                    num_words = len(seq_str.split())\n",
    "                    # print(\"{}  || final:{} e2e:{} lm:{} num words:{}\".format(seq_str, hyp[\"final_score\"], hyp[\"score\"], hyp[\"lm_score\"], hyp[\"num_words\"]))\n",
    "\n",
    "            for hyp in nbest_hyps:                \n",
    "                hyp[\"yseq\"] = hyp[\"yseq\"][0].cpu().numpy().tolist()\n",
    "                hyp_strs = self.post_process_hyp(hyp)\n",
    "\n",
    "                batch_ids_nbest_hyps.append(hyp[\"yseq\"])\n",
    "                batch_strs_nbest_hyps.append(hyp_strs)\n",
    "                # print(hyp[\"yseq\"], hyp_strs)\n",
    "        return batch_ids_nbest_hyps, batch_strs_nbest_hyps\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Layer Transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_model, dim_inner, num_heads, dim_key, dim_value, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            num_heads, dim_model, dim_key, dim_value, dropout=dropout)\n",
    "        self.encoder_attn = MultiHeadAttention(\n",
    "            num_heads, dim_model, dim_key, dim_value, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForwardWithConv(\n",
    "            dim_model, dim_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, decoder_input, encoder_output, non_pad_mask=None, self_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        decoder_output, decoder_self_attn = self.self_attn(\n",
    "            decoder_input, decoder_input, decoder_input, mask=self_attn_mask)\n",
    "        decoder_output *= non_pad_mask\n",
    "\n",
    "        decoder_output, decoder_encoder_attn = self.encoder_attn(\n",
    "            decoder_output, encoder_output, encoder_output, mask=dec_enc_attn_mask)\n",
    "        decoder_output *= non_pad_mask\n",
    "\n",
    "        decoder_output = self.pos_ffn(decoder_output)\n",
    "        decoder_output *= non_pad_mask\n",
    "\n",
    "        return decoder_output, decoder_self_attn, decoder_encoder_attn        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer class\n",
    "    args:\n",
    "        encoder: Encoder object\n",
    "        decoder: Decoder object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, feat_extractor='vgg_cnn'):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.id2label = decoder.id2label\n",
    "        self.feat_extractor = feat_extractor\n",
    "\n",
    "        # feature embedding\n",
    "        if feat_extractor == 'emb_cnn':\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(0, 10)),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.Hardtanh(0, 20, inplace=True),\n",
    "                nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), ),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.Hardtanh(0, 20, inplace=True)\n",
    "            )\n",
    "        elif feat_extractor == 'vgg_cnn':\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, stride=2)\n",
    "            )\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, padded_input, input_lengths, padded_target, verbose=False):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            padded_input: B x 1 (channel for spectrogram=1) x (freq) x T\n",
    "            padded_input: B x T x D\n",
    "            input_lengths: B\n",
    "            padded_target: B x T\n",
    "        output:\n",
    "            pred: B x T x vocab\n",
    "            gold: B x T\n",
    "        \"\"\"\n",
    "        if self.feat_extractor == 'emb_cnn' or self.feat_extractor == 'vgg_cnn':\n",
    "            padded_input = self.conv(padded_input)\n",
    "\n",
    "        # Reshaping features\n",
    "        sizes = padded_input.size() # B x H_1 (channel?) x H_2 x T\n",
    "        padded_input = padded_input.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
    "        padded_input = padded_input.transpose(1, 2).contiguous()  # BxTxH\n",
    "\n",
    "        encoder_padded_outputs, _ = self.encoder(padded_input, input_lengths)\n",
    "        pred, gold, *_ = self.decoder(padded_target, encoder_padded_outputs, input_lengths)\n",
    "        hyp_best_scores, hyp_best_ids = torch.topk(pred, 1, dim=2)\n",
    "\n",
    "        hyp_seq = hyp_best_ids.squeeze(2)\n",
    "        gold_seq = gold\n",
    "\n",
    "        return pred, gold, hyp_seq, gold_seq\n",
    "    \n",
    "    def predict(self, padded_input, input_lengths, padded_target, verbose=False):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            padded_input: B x 1 (channel for spectrogram=1) x (freq) x T\n",
    "            padded_input: B x T x D\n",
    "            input_lengths: B\n",
    "            padded_target: B x T\n",
    "        output:\n",
    "            pred: B x T x vocab\n",
    "            gold: B x T\n",
    "        \"\"\"\n",
    "        if self.feat_extractor == 'emb_cnn' or self.feat_extractor == 'vgg_cnn':\n",
    "            padded_input = self.conv(padded_input)\n",
    "\n",
    "        # Reshaping features\n",
    "        sizes = padded_input.size() # B x H_1 (channel?) x H_2 x T\n",
    "        padded_input = padded_input.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
    "        padded_input = padded_input.transpose(1, 2).contiguous()  # BxTxH\n",
    "\n",
    "        encoder_padded_outputs, _ = self.encoder(padded_input, input_lengths)\n",
    "        pred, gold, *_ = self.decoder(padded_target, encoder_padded_outputs, input_lengths)\n",
    "        hyp_best_scores, hyp_best_ids = torch.topk(pred, 1, dim=2)\n",
    "\n",
    "        hyp_seq = hyp_best_ids.squeeze(2)\n",
    "        gold_seq = gold\n",
    "\n",
    "        return pred, hyp_seq\n",
    "\n",
    "    def evaluate(self,\n",
    "                 padded_input, \n",
    "                 input_lengths, \n",
    "                 padded_target, \n",
    "                 beam_search=False, \n",
    "                 beam_width=0, \n",
    "                 beam_nbest=0, \n",
    "                 lm=None, \n",
    "                 lm_rescoring=False, \n",
    "                 lm_weight=0.1, \n",
    "                 c_weight=1, \n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            padded_input: B x T x D\n",
    "            input_lengths: B\n",
    "            padded_target: B x T\n",
    "        output:\n",
    "            batch_ids_nbest_hyps: list of nbest id\n",
    "            batch_strs_nbest_hyps: list of nbest str\n",
    "            batch_strs_gold: list of gold str\n",
    "        \"\"\"\n",
    "        if self.feat_extractor == 'emb_cnn' or self.feat_extractor == 'vgg_cnn':\n",
    "            padded_input = self.conv(padded_input)\n",
    "\n",
    "        # Reshaping features\n",
    "        sizes = padded_input.size() # B x H_1 (channel?) x H_2 x T\n",
    "        padded_input = padded_input.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
    "        padded_input = padded_input.transpose(1, 2).contiguous()  # BxTxH\n",
    "\n",
    "        encoder_padded_outputs, _ = self.encoder(padded_input, input_lengths)\n",
    "        hyp, gold, *_ = self.decoder(padded_target, encoder_padded_outputs, input_lengths)\n",
    "        hyp_best_scores, hyp_best_ids = torch.topk(hyp, 1, dim=2)\n",
    "        \n",
    "        strs_gold = [\"\".join([self.id2label[int(x)] for x in gold_seq]) for gold_seq in gold]\n",
    "\n",
    "        if beam_search:\n",
    "            ids_hyps, strs_hyps = self.decoder.beam_search(encoder_padded_outputs, beam_width=beam_width, nbest=1, lm=lm, lm_rescoring=lm_rescoring, lm_weight=lm_weight, c_weight=c_weight)\n",
    "            if len(strs_hyps) != sizes[0]:\n",
    "                print(\">>>>>>> switch to greedy\")\n",
    "                strs_hyps = self.decoder.greedy_search(encoder_padded_outputs)\n",
    "        else:\n",
    "            strs_hyps = self.decoder.greedy_search(encoder_padded_outputs)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"GOLD\", strs_gold)\n",
    "            print(\"HYP\", strs_hyps)\n",
    "\n",
    "        return _, strs_hyps, strs_gold\n",
    "\n",
    "    \n",
    "def init_optimizer(args, model, opt_type=\"noam\"):\n",
    "    dim_input = args.dim_input\n",
    "    warmup = args.warmup\n",
    "    lr = args.lr\n",
    "\n",
    "    if opt_type == \"noam\":\n",
    "        opt = NoamOpt(dim_input, args.k_lr, warmup, torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9), min_lr=args.min_lr)\n",
    "    elif opt_type == \"sgd\":\n",
    "        opt = AnnealingOpt(lr, args.lr_anneal, torch.optim.SGD(model.parameters(), lr=lr, momentum=args.momentum, nesterov=True))\n",
    "    else:\n",
    "        opt = None\n",
    "        print(\"Optimizer is not defined\")\n",
    "\n",
    "    return opt\n",
    "\n",
    "def init_transformer_model(args, label2id, id2label):\n",
    "    \"\"\"\n",
    "    Initiate a new transformer object\n",
    "    \"\"\"\n",
    "    if args.feat_extractor == 'emb_cnn':\n",
    "        hidden_size = int(math.floor(\n",
    "            (args.sample_rate * args.window_size) / 2) + 1)\n",
    "        hidden_size = int(math.floor(hidden_size - 41) / 2 + 1)\n",
    "        hidden_size = int(math.floor(hidden_size - 21) / 2 + 1)\n",
    "        hidden_size *= 32\n",
    "        args.dim_input = hidden_size\n",
    "    elif args.feat_extractor == 'vgg_cnn':\n",
    "        hidden_size = int(math.floor((args.sample_rate * args.window_size) / 2) + 1) # 161\n",
    "        hidden_size = int(math.floor(int(math.floor(hidden_size)/2)/2)) * 128 # divide by 2 for maxpooling\n",
    "        args.dim_input = hidden_size\n",
    "    else:\n",
    "        print(\"the model is initialized without feature extractor\")\n",
    "\n",
    "    num_layers = args.num_layers\n",
    "    num_heads = args.num_heads\n",
    "    dim_model = args.dim_model\n",
    "    dim_key = args.dim_key\n",
    "    dim_value = args.dim_value\n",
    "    dim_input = args.dim_input\n",
    "    dim_inner = args.dim_inner\n",
    "    dim_emb = args.dim_emb\n",
    "    src_max_len = args.src_max_len\n",
    "    tgt_max_len = args.tgt_max_len\n",
    "    dropout = args.dropout\n",
    "    emb_trg_sharing = args.emb_trg_sharing\n",
    "    feat_extractor = args.feat_extractor\n",
    "\n",
    "    encoder = Encoder(num_layers, num_heads=num_heads, dim_model=dim_model, dim_key=dim_key,\n",
    "                      dim_value=dim_value, dim_input=dim_input, dim_inner=dim_inner, src_max_length=src_max_len, dropout=dropout)\n",
    "    decoder = Decoder(id2label, num_src_vocab=len(label2id), num_trg_vocab=len(label2id), num_layers=num_layers, num_heads=num_heads,\n",
    "                      dim_emb=dim_emb, dim_model=dim_model, dim_inner=dim_inner, dim_key=dim_key, dim_value=dim_value, trg_max_length=tgt_max_len, dropout=dropout, emb_trg_sharing=emb_trg_sharing)\n",
    "    model = Transformer(encoder, decoder, feat_extractor=feat_extractor)\n",
    "\n",
    "    if args.parallel:\n",
    "        device_ids = args.device_ids\n",
    "        if args.device_ids:\n",
    "            print(\"load with device_ids\", args.device_ids)\n",
    "            model = nn.DataParallel(model, device_ids=args.device_ids)\n",
    "        else:\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\"\n",
    "    Trainer class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        logging.info(\"Trainer is initialized\")\n",
    "\n",
    "    def train(self, \n",
    "              model, \n",
    "              train_loader, \n",
    "              train_sampler, \n",
    "              valid_loader_list, \n",
    "              opt, \n",
    "              loss_type, \n",
    "              start_epoch, \n",
    "              num_epochs, \n",
    "              label2id, \n",
    "              id2label, \n",
    "              last_metrics=None):\n",
    "        \"\"\"\n",
    "        Training\n",
    "        args:\n",
    "            model: Model object\n",
    "            train_loader: DataLoader object of the training set\n",
    "            valid_loader_list: a list of Validation DataLoader objects\n",
    "            opt: Optimizer object\n",
    "            start_epoch: start epoch (> 0 if you resume the process)\n",
    "            num_epochs: last epoch\n",
    "            last_metrics: (if resume)\n",
    "        \"\"\"\n",
    "        history = []\n",
    "        start_time = time.time()\n",
    "        best_valid_loss = 1000000000 if last_metrics is None else last_metrics['valid_loss']\n",
    "        smoothing = args.label_smoothing\n",
    "\n",
    "        logging.info(\"name \" +  args.name)\n",
    "\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            sys.stdout.flush()\n",
    "            total_loss, total_cer, total_wer, total_char, total_word = 0, 0, 0, 0, 0\n",
    "\n",
    "            start_iter = 0\n",
    "\n",
    "            logging.info(\"TRAIN\")\n",
    "            model.train()\n",
    "            pbar = tqdm(iter(train_loader), leave=True, total=len(train_loader))\n",
    "            for i, (data) in enumerate(pbar, start=start_iter):\n",
    "                src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "                if args.cuda:\n",
    "                    src = src.cuda()\n",
    "                    tgt = tgt.cuda()\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "                pred, gold, hyp_seq, gold_seq = model(src, src_lengths, tgt, verbose=False)\n",
    "                try: # handle case for CTC\n",
    "                    strs_gold, strs_hyps = [], []\n",
    "                    for ut_gold in gold_seq:\n",
    "                        str_gold = \"\"\n",
    "                        for x in ut_gold:\n",
    "                            if int(x) == args.PAD_TOKEN:\n",
    "                                break\n",
    "                            str_gold = str_gold + id2label[int(x)]\n",
    "                        strs_gold.append(str_gold)\n",
    "                    for ut_hyp in hyp_seq:\n",
    "                        str_hyp = \"\"\n",
    "                        for x in ut_hyp:\n",
    "                            if int(x) == args.PAD_TOKEN:\n",
    "                                break\n",
    "                            str_hyp = str_hyp + id2label[int(x)]\n",
    "                        strs_hyps.append(str_hyp)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    logging.info(\"NaN predictions\")\n",
    "                    continue\n",
    "\n",
    "                seq_length = pred.size(1)\n",
    "                sizes = Variable(src_percentages.mul_(int(seq_length)).int(), requires_grad=False)\n",
    "\n",
    "                loss, num_correct = calculate_metrics(\n",
    "                    pred, gold, input_lengths=sizes, target_lengths=tgt_lengths, \\\n",
    "                        smoothing=smoothing, loss_type=loss_type)\n",
    "\n",
    "                if loss.item() == float('Inf'):\n",
    "                    logging.info(\"Found infinity loss, masking\")\n",
    "                    loss = torch.where(loss != loss, torch.zeros_like(loss), loss) # NaN masking\n",
    "                    continue\n",
    "\n",
    "                # if constant.args.verbose:\n",
    "                #     logging.info(\"GOLD\", strs_gold)\n",
    "                #     logging.info(\"HYP\", strs_hyps)\n",
    "\n",
    "                for j in range(len(strs_hyps)):\n",
    "                    strs_hyps[j] = strs_hyps[j].replace(args.SOS_CHAR, '').replace(args.EOS_CHAR, '')\n",
    "                    strs_gold[j] = strs_gold[j].replace(args.SOS_CHAR, '').replace(args.EOS_CHAR, '')                    \n",
    "                    cer = calculate_cer(strs_hyps[j].replace(' ', ''), strs_gold[j].replace(' ', ''))\n",
    "                    wer = calculate_wer(strs_hyps[j], strs_gold[j])\n",
    "                    total_cer += cer\n",
    "                    total_wer += wer\n",
    "                    total_char += len(strs_gold[j].replace(' ', ''))\n",
    "                    total_word += len(strs_gold[j].split(\" \"))\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if args.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n",
    "                \n",
    "                opt.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                non_pad_mask = gold.ne(args.PAD_TOKEN)\n",
    "                num_word = non_pad_mask.sum().item()\n",
    "\n",
    "#                 pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} CER:{:.2f}% LR:{:.7f}\".format(\n",
    "#                     (epoch+1), total_loss/(i+1), total_cer*100/total_char, opt._rate))\n",
    "                \n",
    "                pbar.set_description(\n",
    "                    f\"Epoch {epoch+1} || TRAIN LOSS: {total_loss/(i+1):.4f} || CER:{total_cer*100/total_char:.2f}% || WER:{total_wer*100/total_word:.2f}% || LR: {opt._rate:.7f}\")\n",
    "                \n",
    "#             logging.info(\"(Epoch {}) TRAIN LOSS:{:.4f} CER:{:.2f}% LR:{:.7f}\".format(\n",
    "#                 (epoch+1), total_loss/(len(train_loader)), total_cer*100/total_char, opt._rate))\n",
    "            \n",
    "            logging.info(\n",
    "                    f\"Epoch {epoch+1} || TRAIN LOSS: {total_loss/(len(train_loader)):.4f} || CER: {total_cer*100/total_char:.2f}% || WER: {total_wer*100/total_word:.2f}% || LR: {opt._rate:.7f}\")\n",
    "            \n",
    "            # evaluate\n",
    "            print(\"\")\n",
    "            logging.info(\"VALID\")\n",
    "            model.eval()\n",
    "\n",
    "            for ind in range(len(valid_loader_list)):\n",
    "                valid_loader = valid_loader_list[ind]\n",
    "\n",
    "                total_valid_loss, total_valid_cer, total_valid_wer, total_valid_char, total_valid_word = 0, 0, 0, 0, 0\n",
    "                valid_pbar = tqdm(iter(valid_loader), leave=True, total=len(valid_loader))\n",
    "                for i, (data) in enumerate(valid_pbar):\n",
    "                    src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "                    if args.cuda:\n",
    "                        src = src.cuda()\n",
    "                        tgt = tgt.cuda()\n",
    "\n",
    "                    pred, gold, hyp_seq, gold_seq = model(src, src_lengths, tgt, verbose=False)\n",
    "\n",
    "                    seq_length = pred.size(1)\n",
    "                    sizes = Variable(src_percentages.mul_(int(seq_length)).int(), requires_grad=False)\n",
    "\n",
    "                    loss, num_correct = calculate_metrics(\n",
    "                        pred, gold, input_lengths=sizes, target_lengths=tgt_lengths, smoothing=smoothing, loss_type=loss_type)\n",
    "\n",
    "                    if loss.item() == float('Inf'):\n",
    "                        logging.info(\"Found infinity loss, masking\")\n",
    "                        loss = torch.where(loss != loss, torch.zeros_like(loss), loss) # NaN masking\n",
    "                        continue\n",
    "\n",
    "                    try: # handle case for CTC\n",
    "                        strs_gold, strs_hyps = [], []\n",
    "                        for ut_gold in gold_seq:\n",
    "                            str_gold = \"\"\n",
    "                            for x in ut_gold:\n",
    "                                if int(x) == args.PAD_TOKEN:\n",
    "                                    break\n",
    "                                str_gold = str_gold + id2label[int(x)]\n",
    "                            strs_gold.append(str_gold)\n",
    "                        for ut_hyp in hyp_seq:\n",
    "                            str_hyp = \"\"\n",
    "                            for x in ut_hyp:\n",
    "                                if int(x) == args.PAD_TOKEN:\n",
    "                                    break\n",
    "                                str_hyp = str_hyp + id2label[int(x)]\n",
    "                            strs_hyps.append(str_hyp)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        logging.info(\"NaN predictions\")\n",
    "                        continue\n",
    "\n",
    "                    for j in range(len(strs_hyps)):\n",
    "                        strs_hyps[j] = strs_hyps[j].replace(args.SOS_CHAR, '').replace(args.EOS_CHAR, '')\n",
    "                        strs_gold[j] = strs_gold[j].replace(args.SOS_CHAR, '').replace(args.EOS_CHAR, '')\n",
    "#                         ipdb.set_trace()\n",
    "                        cer = calculate_cer(strs_hyps[j].replace(' ', ''), strs_gold[j].replace(' ', ''))\n",
    "                        wer = calculate_wer(strs_hyps[j], strs_gold[j])\n",
    "                        total_valid_cer += cer\n",
    "                        total_valid_wer += wer\n",
    "                        total_valid_char += len(strs_gold[j].replace(' ', ''))\n",
    "                        total_valid_word += len(strs_gold[j].split(\" \"))\n",
    "\n",
    "                    total_valid_loss += loss.item()\n",
    "#                     valid_pbar.set_description(\"VALID SET {} LOSS:{:.4f} CER:{:.2f}%\".format(ind,\n",
    "#                         total_valid_loss/(i+1), total_valid_cer*100/total_valid_char))\n",
    "                    \n",
    "                    valid_pbar.set_description(\n",
    "                        f\"VALID SET {epoch+1} || LOSS: {total_valid_loss/(i+1):.4f} || CER:{total_valid_cer*100/total_valid_char:.2f}% || WER:{total_valid_wer*100/total_valid_word:.2f}%\")\n",
    "        \n",
    "#                 logging.info(\"VALID SET {} LOSS:{:.4f} CER:{:.2f}%\".format(ind,\n",
    "#                         total_valid_loss/(len(valid_loader)), total_valid_cer*100/total_valid_char))\n",
    "                \n",
    "#                 logging.info(\"VALID SET {} LOSS:{:.4f} CER:{:.2f}%\".format((epoch+1),\n",
    "#                         total_valid_loss/(len(valid_loader)), total_valid_cer*100/total_valid_char))\n",
    "\n",
    "                logging.info(\n",
    "                    f\"VALID SET {epoch+1}||LOSS: {total_valid_loss/(len(valid_loader)):.4f} || CER: {total_valid_cer*100/total_valid_char:.2f}% || WER: {total_valid_wer*100/total_valid_word:.2f}%\")\n",
    "            \n",
    "            \n",
    "            metrics = {}\n",
    "            metrics[\"train_loss\"] = total_loss / len(train_loader)\n",
    "            metrics[\"valid_loss\"] = total_valid_loss / (len(valid_loader))\n",
    "            metrics[\"train_cer\"] = total_cer\n",
    "            metrics[\"train_wer\"] = total_wer\n",
    "            metrics[\"valid_cer\"] = total_valid_cer\n",
    "            metrics[\"valid_wer\"] = total_valid_wer\n",
    "            metrics[\"history\"] = history\n",
    "            history.append(metrics)\n",
    "\n",
    "            if epoch % args.save_every == 0:\n",
    "                save_model(model, (epoch+1), opt, metrics,\n",
    "                        label2id, id2label, best_model=False)\n",
    "\n",
    "            # save the best model\n",
    "            if best_valid_loss > total_valid_loss/len(valid_loader):\n",
    "                best_valid_loss = total_valid_loss/len(valid_loader)\n",
    "                save_model(model, (epoch+1), opt, metrics,\n",
    "                        label2id, id2label, best_model=True)\n",
    "\n",
    "            if args.shuffle:\n",
    "                logging.info(\"SHUFFLE\")\n",
    "                print(\"SHUFFLE\")\n",
    "                train_sampler.shuffle(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, lm=None):\n",
    "    \"\"\"\n",
    "    Evaluation\n",
    "    args:\n",
    "        model: Model object\n",
    "        test_loader: DataLoader object\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_word, total_char, total_cer, total_wer = 0, 0, 0, 0\n",
    "    total_en_cer, total_zh_cer, total_en_char, total_zh_char = 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(iter(test_loader), leave=True, total=len(test_loader))\n",
    "        for i, (data) in enumerate(test_pbar):\n",
    "            src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "            if args.cuda:\n",
    "                src = src.cuda()\n",
    "                tgt = tgt.cuda()\n",
    "\n",
    "#             batch_ids_hyps, batch_strs_hyps, batch_strs_gold = model.evaluate(\n",
    "#                 src, src_lengths, tgt, beam_search=args.beam_search, \n",
    "#                 beam_width=args.beam_width, beam_nbest=args.beam_nbest, lm=lm, \n",
    "#                 lm_rescoring=args.lm_rescoring, lm_weight=args.lm_weight, \n",
    "#                 c_weight=args.c_weight, verbose=args.verbose)\n",
    "\n",
    "#             for x in range(len(batch_strs_gold)):\n",
    "#                 hyp = batch_strs_hyps[x].replace(\n",
    "#                     args.EOS_CHAR, \"\").replace(args.SOS_CHAR, \"\").replace(\n",
    "#                     args.PAD_CHAR, \"\")\n",
    "#                 gold = batch_strs_gold[x].replace(args.EOS_CHAR, \"\").replace(\n",
    "#                     args.SOS_CHAR, \"\").replace(args.PAD_CHAR, \"\")\n",
    "\n",
    "#                 wer = calculate_wer(hyp, gold)\n",
    "#                 cer = calculate_cer(hyp.strip(), gold.strip())\n",
    "\n",
    "#                 en_cer, zh_cer, num_en_char, num_zh_char = calculate_cer_en_zh(hyp, gold)\n",
    "#                 total_en_cer += en_cer\n",
    "#                 total_zh_cer += zh_cer\n",
    "#                 total_en_char += num_en_char\n",
    "#                 total_zh_char += num_zh_char\n",
    "\n",
    "#                 total_wer += wer\n",
    "#                 total_cer += cer\n",
    "#                 total_word += len(gold.split(\" \"))\n",
    "#                 total_char += len(gold)\n",
    "            \n",
    "                pred, gold, hyp_seq, gold_seq = model(src, src_lengths, tgt, verbose=False)\n",
    "\n",
    "                try: # handle case for CTC\n",
    "                    strs_gold, strs_hyps = [], []\n",
    "                    for ut_gold in gold_seq:\n",
    "                        str_gold = \"\"\n",
    "                        for x in ut_gold:\n",
    "                            if int(x) == args.PAD_TOKEN:\n",
    "                                break\n",
    "                            str_gold = str_gold + id2label[int(x)]\n",
    "                        strs_gold.append(str_gold)\n",
    "                    for ut_hyp in hyp_seq:\n",
    "                        str_hyp = \"\"\n",
    "                        for x in ut_hyp:\n",
    "                            if int(x) == args.PAD_TOKEN:\n",
    "                                break\n",
    "                            str_hyp = str_hyp + id2label[int(x)]\n",
    "                        strs_hyps.append(str_hyp)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    logging.info(\"NaN predictions\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"prediction {strs_hyps[0]} || gold : {strs_gold[0]}\")\n",
    "                  \n",
    "            for j in range(len(strs_hyps)):\n",
    "                strs_hyps[j] = strs_hyps[j].replace(args.SOS_CHAR, '').replace(args.EOS_CHAR, '')\n",
    "                strs_gold[j] = strs_gold[j].replace(args.SOS_CHAR, '').replace(args.EOS_CHAR, '')\n",
    "                cer = calculate_cer(strs_hyps[j].replace(' ', ''), strs_gold[j].replace(' ', ''))\n",
    "                wer = calculate_wer(strs_hyps[j], strs_gold[j])\n",
    "                total_cer += cer\n",
    "                total_wer += wer\n",
    "                total_char += len(strs_gold[j].replace(' ', ''))\n",
    "                total_word += len(strs_gold[j].split(\" \"))\n",
    "#                 ipdb.set_trace()\n",
    "                test_pbar.set_description(\n",
    "                    f\"VALIDATION CER: {total_cer*100/total_char:.2f}%||WER: {total_wer*100/total_word:.2f}%\")\n",
    "\n",
    "            logging.info(\n",
    "            f\"VALID SET {epoch+1} || CER: {total_cer*100/total_char:.2f}% || WER: {total_wer*100/total_word:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(model, test_loader, lm=None):\n",
    "#     \"\"\"\n",
    "#     Evaluation\n",
    "#     args:\n",
    "#         model: Model object\n",
    "#         test_loader: DataLoader object\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         test_pbar = tqdm(iter(test_loader), leave=True, total=len(test_loader))\n",
    "#         for i, (data) in enumerate(test_pbar):\n",
    "#             src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "#             tgt_word = np.vectorize(id2label.get)(tgt.numpy())\n",
    "            \n",
    "#             if args.cuda:\n",
    "#                 src = src.cuda()\n",
    "#                 tgt = tgt.cuda()\n",
    "\n",
    "#             batch_ids_hyps, batch_strs_hyps, batch_strs_gold = model.evaluate(\n",
    "#                 src, src_lengths, tgt, beam_search=args.beam_search, \n",
    "#                 beam_width=args.beam_width, beam_nbest=args.beam_nbest, lm=lm, \n",
    "#                 lm_rescoring=args.lm_rescoring, lm_weight=args.lm_weight, \n",
    "#                 c_weight=args.c_weight, verbose=args.verbose)\n",
    "\n",
    "#             for x in range(len(batch_strs_gold)):\n",
    "#                 hyp = batch_strs_hyps[x].replace(\n",
    "#                     args.EOS_CHAR, \"\").replace(args.SOS_CHAR, \"\").replace(\n",
    "#                     args.PAD_CHAR, \"\")\n",
    "#                 gold = batch_strs_gold[x].replace(args.EOS_CHAR, \"\").replace(\n",
    "#                     args.SOS_CHAR, \"\").replace(args.PAD_CHAR, \"\")\n",
    "# #                 ipdb.set_trace()\n",
    "            \n",
    "            \n",
    "# #             test_pbar.set_description(\n",
    "# #                 \"TEST CER:{:.2f}% WER:{:.2f}% CER_EN:{:.2f}% CER_ZH:{:.2f}%\".format(\n",
    "# #                 total_cer*100/total_char, total_wer*100/total_word, total_en_cer*100/max(\n",
    "# #                     1, total_en_char), total_zh_cer*100/max(1, total_zh_char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(model, test_loader, lm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='ASR training')\n",
    "\n",
    "parser.add_argument('--model', default='TRFS', type=str, help=\"TRFS:transformer\")\n",
    "parser.add_argument('--name', default='model', help=\"Name of the model for saving\")\n",
    "\n",
    "# Train\n",
    "parser.add_argument('--train-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--valid-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--test-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--lang-list', nargs='+', type=str)\n",
    "\n",
    "parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "parser.add_argument('--batch-size', default=20, type=int, help='Batch size for training') # 20\n",
    "parser.add_argument('--num-workers', default=4, type=int, \n",
    "                    help='Number of workers used in data-loading')\n",
    "\n",
    "parser.add_argument('--labels-path', default='labels.json', \n",
    "                    help='Contains all characters for transcription')\n",
    "parser.add_argument('--label-smoothing', default=0.0, type=float, help='Label smoothing')\n",
    "\n",
    "# Speech\n",
    "parser.add_argument('--window-size', default=.02, type=float, \n",
    "                    help='Window size for spectrogram in seconds')\n",
    "parser.add_argument('--window-stride', default=.01, type=float, \n",
    "                    help='Window stride for spectrogram in seconds')\n",
    "parser.add_argument('--window', default='hamming', \n",
    "                    help='Window type for spectrogram generation')\n",
    "\n",
    "parser.add_argument('--epochs', default=100, type=int, \n",
    "                    help='Number of training epochs') # 1000\n",
    "parser.add_argument('--cuda', dest='cuda', action='store_true', \n",
    "                    help='Use cuda to train model')\n",
    "\n",
    "parser.add_argument('--device-ids', default=None, nargs='+', type=int,\n",
    "                    help='If using cuda, sets the GPU devices for the process')\n",
    "parser.add_argument('--lr', '--learning-rate', default=3e-4, type=float, \n",
    "                    help='initial learning rate')\n",
    "\n",
    "parser.add_argument('--save-every', default=5, type=int, \n",
    "                    help='Save model every certain number of epochs')\n",
    "parser.add_argument('--save-folder', default='models/', \n",
    "                    help='Location to save epoch models')\n",
    "\n",
    "parser.add_argument('--emb_trg_sharing', action='store_true', \n",
    "                    help='Share embedding weight source and target')\n",
    "parser.add_argument('--feat_extractor', default='vgg_cnn', type=str, \n",
    "                    help='emb_cnn or vgg_cnn')\n",
    "\n",
    "parser.add_argument('--verbose', action='store_true', \n",
    "                    help='Verbose')\n",
    "\n",
    "parser.add_argument('--continue-from', default='', \n",
    "                    help='Continue from checkpoint model')\n",
    "parser.add_argument('--augment', dest='augment', action='store_true', \n",
    "                    help='Use random tempo and gain perturbations.')\n",
    "parser.add_argument('--noise-dir', default=None,\n",
    "                    help='Directory to inject noise into audio. If default, noise Inject not added')\n",
    "parser.add_argument('--noise-prob', default=0.4, \n",
    "                    help='Probability of noise being added per sample')\n",
    "parser.add_argument('--noise-min', default=0.0,\n",
    "                    help='Minimum noise level to sample from. (1.0 means all noise, not original signal)', type=float)\n",
    "parser.add_argument('--noise-max', default=0.5,\n",
    "                    help='Maximum noise levels to sample from. Maximum 1.0', type=float)\n",
    "\n",
    "# Transformer\n",
    "parser.add_argument('--num-layers', default=3, type=int, help='Number of layers')\n",
    "parser.add_argument('--num-heads', default=5, type=int, help='Number of heads')\n",
    "parser.add_argument('--dim-model', default=512, type=int, help='Model dimension')\n",
    "parser.add_argument('--dim-key', default=64, type=int, help='Key dimension')\n",
    "parser.add_argument('--dim-value', default=64, type=int, help='Value dimension')\n",
    "parser.add_argument('--dim-input', default=161, type=int, help='Input dimension')\n",
    "parser.add_argument('--dim-inner', default=1024, type=int, help='Inner dimension')\n",
    "parser.add_argument('--dim-emb', default=512, type=int, help='Embedding dimension')\n",
    "\n",
    "parser.add_argument('--src-max-len', default=4000, type=int, help='Source max length')\n",
    "parser.add_argument('--tgt-max-len', default=1000, type=int, help='Target max length')\n",
    "\n",
    "# Noam optimizer\n",
    "parser.add_argument('--warmup', default=4000, type=int, help='Warmup')\n",
    "parser.add_argument('--min-lr', default=1e-5, type=float, help='min lr')\n",
    "parser.add_argument('--k-lr', default=1, type=float, help='factor lr')\n",
    "\n",
    "# SGD optimizer\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--lr-anneal', default=1.1, type=float, help='lr anneal')\n",
    "\n",
    "# Decoder search\n",
    "parser.add_argument('--beam-search', action='store_true', help='Beam search')\n",
    "parser.add_argument('--beam-width', default=3, type=int, help='Beam size')\n",
    "parser.add_argument('--beam-nbest', default=5, type=int, help='Number of best sequences')\n",
    "parser.add_argument('--lm-rescoring', action='store_true', help='Rescore using LM')\n",
    "parser.add_argument('--lm-path', type=str, default=\"lm_model.pt\", help=\"Path to LM model\")\n",
    "parser.add_argument('--lm-weight', default=0.1, type=float, help='LM weight')\n",
    "parser.add_argument('--c-weight', default=0.1, type=float, help='Word count weight')\n",
    "parser.add_argument('--prob-weight', default=1.0, type=float, help='Probability E2E weight')\n",
    "\n",
    "# Loss\n",
    "parser.add_argument('--loss', type=str, default='ce', help='ce or ctc')\n",
    "parser.add_argument('--clip', action='store_true', help=\"clip\")\n",
    "parser.add_argument('--max-norm', default=400, type=float, help=\"max norm for clipping\")\n",
    "\n",
    "parser.add_argument('--dropout', default=0.1, type=float, help='Dropout')\n",
    "\n",
    "# Parallelize model\n",
    "parser.add_argument('--parallel', action='store_true', help='Parallelize the model')\n",
    "\n",
    "# shuffle\n",
    "parser.add_argument('--shuffle', action='store_true', help='Shuffle')\n",
    "\n",
    "# PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "parser.add_argument('--PAD_CHAR', default=\"\", type=str, help='PAD_CHAR')\n",
    "parser.add_argument('--SOS_CHAR', default=\"\", type=str, help='SOS_CHAR')\n",
    "parser.add_argument('--EOS_CHAR', default=\"\", type=str, help='EOS_CHAR')\n",
    "parser.add_argument('--PAD_TOKEN', default=0, type=int, help='PAD_TOKEN')\n",
    "parser.add_argument('--SOS_TOKEN', default=1, type=int, help='SOS_TOKEN')\n",
    "parser.add_argument('--EOS_TOKEN', default=2, type=int, help='EOS_TOKEN')\n",
    "\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "torch.cuda.manual_seed_all(123456)\n",
    "\n",
    "# https://github.com/spyder-ide/spyder/issues/3883\n",
    "import sys\n",
    "sys.argv=['']; del sys \n",
    "\n",
    "args = parser.parse_args()\n",
    "USE_CUDA = args.cuda\n",
    "\n",
    "# PAD_TOKEN = 0\n",
    "# SOS_TOKEN = 1\n",
    "# EOS_TOKEN = 2\n",
    "\n",
    "# PAD_CHAR = \"\"\n",
    "# SOS_CHAR = \"\"\n",
    "# EOS_CHAR = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py --train-manifest-list data/manifests/libri_train_manifest.csv \n",
    "# --valid-manifest-list data/manifests/libri_val_manifest.csv --test-manifest-list data/manifests/libri_test_manifest.csv \n",
    "# --cuda --batch-size 12 --labels-path data/labels/labels.json --lr 1e-4 --name libri_drop0.1_cnn_batch12_4_vgg_layer4 \n",
    "# --save-folder save/ --save-every 5 --feat_extractor vgg_cnn --dropout 0.1 --num-layers 2 --num-heads 6 \n",
    "# --dim-model 512 --dim-key 64 --dim-value 64 --dim-input 161 --dim-inner 2048 --dim-emb 512 --shuffle \n",
    "# --min-lr 1e-6 --k-lr 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../../../../big_data/end2end-asr-pytorch/LibriSpeech_dataset/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.feat_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train_manifest_list = ['data/manifests/libri_train_manifest.csv']\n",
    "args.valid_manifest_list = ['data/manifests/libri_val_manifest.csv']\n",
    "args.test_manifest_list = ['data/manifests/libri_test_manifest.csv']\n",
    "\n",
    "args.batch_size = 6\n",
    "args.labels_path = 'data/labels/labels.json'\n",
    "args.lr = 1e-4\n",
    "args.name = 'libri_drop0.1_cnn_batch12_6_vgg_layer_notebook'\n",
    "args.save_folder = 'save/'\n",
    "args.save_every = 10\n",
    "args.feat_extractor = 'vgg_cnn'\n",
    "args.dropout = 0.1\n",
    "args.num_layers = 2\n",
    "args.num_heads = 6\n",
    "args.dim_model = 512\n",
    "args.dim_key = 64\n",
    "args.dim_value = 64\n",
    "args.dim_input = 161\n",
    "args.dim_inner = 2048\n",
    "args.dim_emb = 512\n",
    "args.shuffle = True\n",
    "args.min_lr = 1e-6\n",
    "args.k_lr = 1\n",
    "args.target_dir = '../../../../big_data/end2end-asr-pytorch/LibriSpeech_dataset/'\n",
    "args.sample_rate = 1600 #16000\n",
    "\n",
    "args.window_size = 0.02\n",
    "args.window_stride = 0.01 #0.01\n",
    "args.window = 'hamming'\n",
    "# args.noise_dir = \n",
    "args.noise_prob = 0.4\n",
    "args.noise_min = 0.0\n",
    "args.noise_max = 0.5\n",
    "args.cuda = torch.cuda.is_available()\n",
    "args.epochs = 1000\n",
    "args.continue_from = 'save/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook/best_model.th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "THE EXPERIMENT LOG IS SAVED IN: log/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook\n",
      "TRAINING MANIFEST:  ['data/manifests/libri_train_manifest.csv']\n",
      "VALID MANIFEST:  ['data/manifests/libri_val_manifest.csv']\n",
      "TEST MANIFEST:  ['data/manifests/libri_test_manifest.csv']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"THE EXPERIMENT LOG IS SAVED IN: \" + \"log/\" + args.name)\n",
    "print(\"TRAINING MANIFEST: \", args.train_manifest_list)\n",
    "print(\"VALID MANIFEST: \", args.valid_manifest_list)\n",
    "print(\"TEST MANIFEST: \", args.test_manifest_list)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/aims/2020-AMMI-salomon/notebooks/MT-ASR/log/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e0c788fe1a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m logging.basicConfig(filename=\"log/\" + args.name, filemode='w+', \n\u001b[0;32m----> 2\u001b[0;31m                     format='%(asctime)s - %(message)s', level=logging.INFO)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m audio_conf = dict(sample_rate=args.sample_rate,\n\u001b[1;32m      5\u001b[0m                   \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mbasicConfig\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m                 \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filemode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1808\u001b[0;31m                     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1809\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m                     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, encoding, delay)\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mStreamHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresulting\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \"\"\"\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/aims/2020-AMMI-salomon/notebooks/MT-ASR/log/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook'"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(filename=\"log/\" + args.name, filemode='w+', \n",
    "                    format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "audio_conf = dict(sample_rate=args.sample_rate,\n",
    "                  window_size=args.window_size,\n",
    "                  window_stride=args.window_stride,\n",
    "                  window=args.window,\n",
    "                  noise_dir=args.noise_dir,\n",
    "                  noise_prob=args.noise_prob,\n",
    "                  noise_levels=(args.noise_min, args.noise_max))\n",
    "\n",
    "logging.info(audio_conf)\n",
    "\n",
    "with open(args.labels_path) as label_file:\n",
    "        labels = str(''.join(json.load(label_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d7288eed5666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add PAD_CHAR, SOS_CHAR, EOS_CHAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPAD_CHAR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOS_CHAR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEOS_CHAR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Add PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "\n",
    "labels = args.PAD_CHAR + args.SOS_CHAR + args.EOS_CHAR + labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f1a65057418a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel2id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlabel2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "label2id, id2label = {}, {}\n",
    "count = 0\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] not in label2id:\n",
    "        label2id[labels[i]] = count\n",
    "        id2label[count] = labels[i]\n",
    "        count += 1\n",
    "    else:\n",
    "        print(\"multiple label: \", labels[i])\n",
    "        \n",
    "# label2id = dict([(labels[i], i) for i in range(len(labels))])\n",
    "# id2label = dict([(i, labels[i]) for i in range(len(labels))])\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '../../../../big_data/end2end-asr-pytorch/LibriSpeech_dataset/train/wav/103-1240-0000.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a2ffbeee7bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"train/wav/103-1240-0000.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/site-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate must be specified when data is a numpy array or list of audio samples.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/site-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m_make_wav\u001b[0;34m(data, rate, normalize)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_and_normalize_with_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_and_normalize_without_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/site-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m_validate_and_normalize_with_numpy\u001b[0;34m(data, normalize)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mnchan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '../../../../big_data/end2end-asr-pytorch/LibriSpeech_dataset/train/wav/103-1240-0000.wav'"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(args.target_dir+\"train/wav/103-1240-0000.wav\", rate= args.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_21epoch_21epoch_21\"+P:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython.display.Audio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_audio_length(path):\n",
    "#     output = subprocess.check_output(\n",
    "#         ['soxi -D \\\"%s\\\"' % path.strip()], shell=True)\n",
    "#     return float(output)\n",
    "\n",
    "# get_audio_length(args.target_dir+'train/wav/103-1240-0000.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenght_audios(path, mode='train'):\n",
    "    \"\"\"\n",
    "    This function return the average lengh of audio in the dataset\n",
    "    \"\"\"\n",
    "    lenght = []\n",
    "    for i, audio in enumerate(os.listdir(path+mode+'/wav/')):\n",
    "#         ipdb.set_trace()\n",
    "        lenght.append(get_audio_length(path+mode+\"/wav/\"+audio))\n",
    "    return np.mean(lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.continue_from = 'save/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook/best_model.th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_rate': 1600,\n",
       " 'window_size': 0.02,\n",
       " 'window_stride': 0.01,\n",
       " 'window': 'hamming',\n",
       " 'noise_dir': None,\n",
       " 'noise_prob': 0.4,\n",
       " 'noise_levels': (0.0, 0.5)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook/best_model.th\n"
     ]
    }
   ],
   "source": [
    "train_data = SpectrogramDataset(audio_conf, manifest_filepath_list=args.train_manifest_list,\n",
    "                                label2id=label2id, normalize=True, augment=args.augment)\n",
    "\n",
    "train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "\n",
    "train_loader = AudioDataLoader(\n",
    "    train_data, num_workers=args.num_workers, batch_sampler=train_sampler)\n",
    "\n",
    "valid_loader_list, test_loader_list = [], []\n",
    "for i in range(len(args.valid_manifest_list)):\n",
    "    valid_data = SpectrogramDataset(\n",
    "        audio_conf, manifest_filepath_list=[args.valid_manifest_list[i]], \n",
    "        label2id=label2id, normalize=True, augment=False)\n",
    "    \n",
    "    valid_loader = AudioDataLoader(valid_data, num_workers=args.num_workers, \n",
    "                                   batch_size=args.batch_size)\n",
    "    valid_loader_list.append(valid_loader)\n",
    "    \n",
    "\n",
    "for i in range(len(args.test_manifest_list)):\n",
    "    test_data = SpectrogramDataset(\n",
    "        audio_conf, manifest_filepath_list=[args.test_manifest_list[i]], \n",
    "        label2id=label2id, normalize=True, augment=False)\n",
    "    \n",
    "    test_loader = AudioDataLoader(test_data, num_workers=args.num_workers)\n",
    "    test_loader_list.append(test_loader)\n",
    "\n",
    "start_epoch = 0\n",
    "metrics = None\n",
    "loaded_args = None\n",
    "print(args.continue_from)\n",
    "if args.continue_from != \"\":\n",
    "    logging.info(\"Continue from checkpoint: \" + args.continue_from)\n",
    "    model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(\n",
    "        args.continue_from)\n",
    "    start_epoch = epoch  # index starts from zero\n",
    "    verbose = args.verbose\n",
    "\n",
    "    if loaded_args != None:\n",
    "        # Unwrap nn.DataParallel\n",
    "        if loaded_args.parallel:\n",
    "            logging.info(\"unwrap from DataParallel\")\n",
    "            model = model.module\n",
    "\n",
    "        # Parallelize the batch\n",
    "        if args.parallel:\n",
    "            model = nn.DataParallel(model, device_ids=args.device_ids)\n",
    "else:\n",
    "    if args.model == \"TRFS\":\n",
    "        model = init_transformer_model(args, label2id, id2label)\n",
    "        opt = init_optimizer(args, model, \"noam\")\n",
    "    else:\n",
    "        logging.info(\"The model is not supported, check args --h\")\n",
    "\n",
    "loss_type = args.loss\n",
    "\n",
    "if args.cuda:\n",
    "    model = model.cuda(0)\n",
    "\n",
    "logging.info(model)\n",
    "num_epochs = args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer()\n",
    "# import sys\n",
    "\n",
    "# trainer.train(model, train_loader, train_sampler, valid_loader_list, opt, \n",
    "#               loss_type, start_epoch, num_epochs, label2id, id2label, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'save/'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(EOS_CHAR='', EOS_TOKEN=2, PAD_CHAR='', PAD_TOKEN=0, SOS_CHAR='', SOS_TOKEN=1, augment=False, batch_size=6, beam_nbest=5, beam_search=False, beam_width=3, c_weight=0.1, clip=False, continue_from='save/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook/best_model.th', cuda=True, device_ids=None, dim_emb=512, dim_inner=2048, dim_input=161, dim_key=64, dim_model=512, dim_value=64, dropout=0.1, emb_trg_sharing=False, epochs=1000, feat_extractor='vgg_cnn', k_lr=1, label_smoothing=0.0, labels_path='data/labels/labels.json', lang_list=None, lm_path='lm_model.pt', lm_rescoring=False, lm_weight=0.1, loss='ce', lr=0.0001, lr_anneal=1.1, max_norm=400, min_lr=1e-06, model='TRFS', momentum=0.9, name='libri_drop0.1_cnn_batch12_6_vgg_layer_notebook', noise_dir=None, noise_max=0.5, noise_min=0.0, noise_prob=0.4, num_heads=6, num_layers=2, num_workers=4, parallel=False, prob_weight=1.0, sample_rate=1600, save_every=10, save_folder='save/', shuffle=True, src_max_len=4000, target_dir='../../../../big_data/end2end-asr-pytorch/LibriSpeech_dataset/', test_manifest_list=['data/manifests/libri_test_manifest.csv'], tgt_max_len=1000, train_manifest_list=['data/manifests/libri_train_manifest.csv'], valid_manifest_list=['data/manifests/libri_val_manifest.csv'], verbose=False, warmup=4000, window='hamming', window_size=0.02, window_stride=0.01)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_iter = 0\n",
    "\n",
    "# # valid_loader_ = valid_loader_list[0]\n",
    "# valid_loader_ = test_loader_list[0]\n",
    "\n",
    "# # Load the model\n",
    "# load_path = args.continue_from\n",
    "# model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(args.continue_from)\n",
    "# evaluate(model, valid_loader_, lm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_iter = 0\n",
    "\n",
    "# # Load the model\n",
    "# load_path = args.continue_from\n",
    "# model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(args.continue_from)\n",
    "# test(model, valid_loader, lm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.path_manifest = '/home/aims/2020-AMMI-salomon/notebooks/MT-ASR/end2end-asr/data/manifests/'\n",
    "# args.path_manifest = args.target_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00, 1433.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting manifests...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_manifest_pred(data_path, output_path, min_duration=None, max_duration=None):\n",
    "    file_paths = [os.path.join(dirpath, f)\n",
    "                  for dirpath, dirnames, files in os.walk(data_path)\n",
    "                  for f in fnmatch.filter(files, '*.wav')]\n",
    "#     ipdb.set_trace()\n",
    "    file_paths = order_and_prune_files(file_paths, min_duration, max_duration)\n",
    "    with io.FileIO(output_path, \"w\") as file:\n",
    "        for wav_path in tqdm(file_paths, total=len(file_paths)):\n",
    "#             transcript_path = wav_path.replace('/wav/', '/txt/').replace('.wav', '.txt')\n",
    "#             sample = os.path.abspath(wav_path) + ',' + 'Empty' + '\\n'\n",
    "#             file.write(sample.encode('utf-8'))\n",
    "        \n",
    "            transcript_path = wav_path.replace('/wav/', '/txt/').replace('.wav', '.txt')\n",
    "            sample = os.path.abspath(wav_path) + ',' + os.path.abspath(transcript_path) + '\\n'\n",
    "            file.write(sample.encode('utf-8'))\n",
    "    print('\\n')\n",
    "    \n",
    "create_manifest_pred(args.path_manifest,\n",
    "                args.path_manifest+'record_manifest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Audio(args.path_manifest+\"1183-124566-0005.wav\", rate= args.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred = ['data/manifests/record_manifest.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SpectrogramDatasetPred(Dataset, SpectrogramParser):\n",
    "#     def __init__(self, audio_conf, manifest_filepath_list, \n",
    "#                  label2id, normalize=False, augment=False):\n",
    "#         \"\"\"\n",
    "#         Dataset that loads tensors via a csv containing file paths to audio files and transcripts separated by\n",
    "#         a comma. Each new line is a different sample. Example below:\n",
    "#         /path/to/audio.wav,/path/to/audio.txt\n",
    "#         ...\n",
    "#         :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "#         :param manifest_filepath: Path to manifest csv as describe above\n",
    "#         :param labels: String containing all the possible characters to map to\n",
    "#         :param normalize (default False): Apply standard mean and deviation normalization to audio tensor\n",
    "#         :param augment (default False):  Apply random tempo and gain perturbations\n",
    "#         \"\"\"\n",
    "#         self.max_size = 0\n",
    "#         self.ids_list = []\n",
    "#         for i in range(len(manifest_filepath_list)):\n",
    "#             manifest_filepath = manifest_filepath_list[i]\n",
    "#             with open(manifest_filepath) as f:\n",
    "#                 ids = f.readlines()\n",
    "\n",
    "#             ids = [x.strip().split(',') for x in ids]\n",
    "#             self.ids_list.append(ids)\n",
    "#             self.max_size = max(len(ids), self.max_size)\n",
    "\n",
    "#         self.manifest_filepath_list = manifest_filepath_list\n",
    "#         self.label2id = label2id\n",
    "#         super(SpectrogramDatasetPred, self).__init__(\n",
    "#             audio_conf, normalize, augment)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         random_id = random.randint(0, len(self.ids_list)-1)\n",
    "#         ids = self.ids_list[random_id]\n",
    "#         sample = ids[index % len(ids)]\n",
    "#         audio_path, transcript_path = sample[0], sample[1]\n",
    "        \n",
    "#         # get the audio using Short-time Fourier transform (STFT)\n",
    "#         # librosa.stft up to \"args.src_max_len\"\n",
    "#         spect = self.parse_audio(audio_path)[:,:args.src_max_len] \n",
    "        \n",
    "# #         transcript = self.parse_transcript(transcript_path)\n",
    "#         return spect, _\n",
    "\n",
    "#     def parse_transcript(self, transcript_path):\n",
    "#         with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "#             # add start of sentense and end of sentence token\n",
    "#             transcript = args.SOS_CHAR + transcript_file.read().replace('\\n', '').lower() +\\\n",
    "#                             args.EOS_CHAR\n",
    "            \n",
    "#         # return all index exept 0 (false), in this case\n",
    "#         # there will be no 0 in the list of index (due to filter)\n",
    "#         transcript = list(\n",
    "#             filter(None, [self.label2id.get(x) for x in list(transcript)]))\n",
    "#         return transcript\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.max_size\n",
    "\n",
    "# def _collate_fn(batch):\n",
    "#     def func(p):\n",
    "#         return p[0].size(1)\n",
    "\n",
    "#     def func_tgt(p):\n",
    "#         return len(p[1])\n",
    "\n",
    "#     # descending sorted\n",
    "#     batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
    "\n",
    "#     max_seq_len = max(batch, key=func)[0].size(1)\n",
    "#     freq_size = max(batch, key=func)[0].size(0)\n",
    "# #     max_tgt_len = len(max(batch, key=func_tgt)[1])\n",
    "    \n",
    "#     inputs = torch.zeros(len(batch), 1, freq_size, max_seq_len)\n",
    "#     input_sizes = torch.IntTensor(len(batch))\n",
    "#     input_percentages = torch.FloatTensor(len(batch))\n",
    "\n",
    "# #     targets = torch.zeros(len(batch), max_tgt_len).long()\n",
    "# #     target_sizes = torch.IntTensor(len(batch))\n",
    "    \n",
    "#     for x in range(len(batch)):\n",
    "#         sample = batch[x]\n",
    "#         input_data = sample[0]\n",
    "# #         target = sample[1]\n",
    "#         seq_length = input_data.size(1)\n",
    "#         input_sizes[x] = seq_length\n",
    "#         inputs[x][0].narrow(1, 0, seq_length).copy_(input_data)\n",
    "#         input_percentages[x] = seq_length / float(max_seq_len)\n",
    "# #         target_sizes[x] = len(target)\n",
    "# #         targets[x][:len(target)] = torch.IntTensor(target)\n",
    "\n",
    "#     return inputs, input_percentages, input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_data = SpectrogramDatasetPred(audio_conf, manifest_filepath_list=args.pred,\n",
    "#                                 label2id=label2id, normalize=True, augment=args.augment)\n",
    "pred_data = SpectrogramDataset(audio_conf, manifest_filepath_list=args.pred,\n",
    "                                label2id=label2id, normalize=True, augment=args.augment)\n",
    "\n",
    "pred_sampler = BucketingSampler(pred_data, batch_size=1)\n",
    "\n",
    "pred_loader = AudioDataLoader(\n",
    "    pred_data, num_workers=args.num_workers, batch_sampler=pred_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, lm=None):\n",
    "    \"\"\"\n",
    "    Evaluation\n",
    "    args:\n",
    "        model: Model object\n",
    "        test_loader: DataLoader object\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(iter(test_loader), leave=True, total=len(test_loader))\n",
    "        for i, (data) in enumerate(test_pbar):\n",
    "            src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "            tgt_word = np.vectorize(id2label.get)(tgt.numpy())\n",
    "            \n",
    "            if args.cuda:\n",
    "                src = src.cuda()\n",
    "                tgt = tgt.cuda()\n",
    "\n",
    "            pred, _, hyp_seq, _ = model(src, src_lengths, tgt, verbose=False)\n",
    "\n",
    "            try: # handle case for CTC\n",
    "                strs_gold, strs_hyps = [], []\n",
    "#                 for ut_gold in gold_seq:\n",
    "#                     str_gold = \"\"\n",
    "#                     for x in ut_gold:\n",
    "#                         if int(x) == args.PAD_TOKEN:\n",
    "#                             break\n",
    "#                         str_gold = str_gold + id2label[int(x)]\n",
    "#                     strs_gold.append(str_gold)\n",
    "                for ut_hyp in hyp_seq:\n",
    "                    str_hyp = \"\"\n",
    "                    for x in ut_hyp:\n",
    "                        if int(x) == args.PAD_TOKEN:\n",
    "                            break\n",
    "                        str_hyp = str_hyp + id2label[int(x)]\n",
    "                    strs_hyps.append(str_hyp)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                logging.info(\"NaN predictions\")\n",
    "                continue\n",
    "                \n",
    "            return str_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(model, pred_loader, lm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ahenty nine '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(args.continue_from)\n",
    "predict(model, pred_loader, lm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Audio(args.path_manifest+\"1183-124566-0005.wav\", rate= args.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record on device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myarray = next(iter(pred_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "import torchaudio\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2b7fd0066342a1b73b78a81e4a76d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera = CameraStream(constraints={'audio': True,'video':False})\n",
    "recorder = AudioRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./recording.webm\", rate= args.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR.ipynb\t   log\t       recording.webm\t     save      utils\n",
      "ASR-Working.ipynb  models      record__manifest.csv  test.py\n",
      "data\t\t   predict.py  record_manifest.csv   trainer\n",
      "img\t\t   README.md   requirement.sh\t     train.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "file.wav not found or is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d23c6223343e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ffmpeg -i ./recording.webm -ac 1 -f wav ./file.wav -y -hide_banner -loglevel panic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(sig.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Audio(data=sig, rate=sr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/site-packages/torchaudio/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0msignalinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignalinfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mencodinginfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencodinginfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mfiletype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiletype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/site-packages/torchaudio/_sox_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# check if valid file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found or is a directory\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# initialize output tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: file.wav not found or is a directory"
     ]
    }
   ],
   "source": [
    "with open('recording.webm', 'wb') as f:\n",
    "    f.write(recorder.audio.value)\n",
    "!ffmpeg -i ./recording.webm -ac 1 -f wav ./file.wav -y -hide_banner -loglevel panic\n",
    "sig, sr = torchaudio.load(\"file.wav\")\n",
    "# print(sig.shape)\n",
    "# Audio(data=sig, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    sound, _ = torchaudio.load(path, normalization=True)\n",
    "    sound = sound.numpy().T\n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    return sound\n",
    "\n",
    "\n",
    "def get_audio_length(path):\n",
    "    output = subprocess.check_output(\n",
    "        ['soxi -D \\\"%s\\\"' % path.strip()], shell=True)\n",
    "    return float(output)\n",
    "\n",
    "def audio_with_sox(path, sample_rate, start_time, end_time):\n",
    "    \"\"\"\n",
    "    crop and resample the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as tar_file:\n",
    "        tar_filename = tar_file.name\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} trim {} ={} >/dev/null 2>&1\".format(\n",
    "            path, sample_rate,\n",
    "            tar_filename, start_time,\n",
    "            end_time)\n",
    "        \n",
    "        os.system(sox_params)\n",
    "        y = load_audio(tar_filename)\n",
    "        return y\n",
    "\n",
    "def augment_audio_with_sox(path, sample_rate, tempo, gain):\n",
    "    \"\"\"\n",
    "    Changes tempo and gain of the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as augmented_file:\n",
    "        augmented_filename = augmented_file.name\n",
    "        sox_augment_params = [\"tempo\", \"{:.3f}\".format(\n",
    "            tempo), \"gain\", \"{:.3f}\".format(gain)]\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} {} >/dev/null 2>&1\".format(\n",
    "            path, sample_rate, augmented_filename, \" \".join(sox_augment_params))\n",
    "        os.system(sox_params)\n",
    "        y = load_audio(augmented_filename)\n",
    "        return y\n",
    "\n",
    "\n",
    "def load_randomly_augmented_audio(path, sample_rate=16000, tempo_range=(0.85, 1.15), \n",
    "                                  gain_range=(-6, 8)):\n",
    "    \"\"\"\n",
    "    Picks tempo and gain uniformly, applies it to the utterance by using sox utility.\n",
    "    Returns the augmented utterance.\n",
    "    \"\"\"\n",
    "    low_tempo, high_tempo = tempo_range\n",
    "    tempo_value = np.random.uniform(low=low_tempo, high=high_tempo)\n",
    "    low_gain, high_gain = gain_range\n",
    "    gain_value = np.random.uniform(low=low_gain, high=high_gain)\n",
    "    audio = augment_audio_with_sox(path=path, sample_rate=sample_rate,\n",
    "                                   tempo=tempo_value, gain=gain_value)\n",
    "    return audio\n",
    "\n",
    "def save_model(model, epoch, opt, metrics, label2id, id2label, best_model=False):\n",
    "    \"\"\"\n",
    "    Saving model, TODO adding history\n",
    "    \"\"\"\n",
    "    if best_model:\n",
    "        save_path = \"{}/{}/best_model.th\".format(\n",
    "            args.save_folder, args.name)\n",
    "    else:\n",
    "        save_path = \"{}/{}/epoch_{}.th\".format(args.save_folder,\n",
    "                                               args.name, epoch)\n",
    "\n",
    "    if not os.path.exists(args.save_folder + \"/\" + args.name):\n",
    "        os.makedirs(args.save_folder + \"/\" + args.name)\n",
    "\n",
    "    print(\"SAVE MODEL to\", save_path)\n",
    "    if args.loss == \"ce\":\n",
    "        args_ = {\n",
    "            'label2id': label2id,\n",
    "            'id2label': id2label,\n",
    "            'args': args,\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.optimizer.state_dict(),\n",
    "            'optimizer_params': {\n",
    "                '_step': opt._step,\n",
    "                '_rate': opt._rate,\n",
    "                'warmup': opt.warmup,\n",
    "                'factor': opt.factor,\n",
    "                'model_size': opt.model_size\n",
    "            },\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    elif args.loss == \"ctc\":\n",
    "        args_ = {\n",
    "            'label2id': label2id,\n",
    "            'id2label': id2label,\n",
    "            'args': args,\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.optimizer.state_dict(),\n",
    "            'optimizer_params': {\n",
    "                'lr': opt.lr,\n",
    "                'lr_anneal': opt.lr_anneal\n",
    "            },\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    else:\n",
    "        print(\"Loss is not defined\")\n",
    "    torch.save(args_, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.signal\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from torch.distributed import get_rank\n",
    "from torch.distributed import get_world_size\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "import logging\n",
    "\n",
    "windows = {'hamming': scipy.signal.hamming, 'hann': scipy.signal.hann, \n",
    "           'blackman': scipy.signal.blackman, 'bartlett': scipy.signal.bartlett}\n",
    "\n",
    "\n",
    "class AudioParser(object):\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        \"\"\"\n",
    "        :param transcript_path: Path where transcript is stored from the manifest file\n",
    "        :return: Transcript in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        :param audio_path: Path where audio is stored from the manifest file\n",
    "        :return: Audio in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectrogramParser(AudioParser):\n",
    "    def __init__(self, audio_conf, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Parses audio file into spectrogram with optional normalization and various augmentations\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param normalize(default False):  Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment(default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        super(SpectrogramParser, self).__init__()\n",
    "        self.window_stride = audio_conf['window_stride']\n",
    "        self.window_size = audio_conf['window_size']\n",
    "        self.sample_rate = audio_conf['sample_rate']\n",
    "        self.window = windows.get(audio_conf['window'], windows['hamming'])\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        self.noiseInjector = NoiseInjection(audio_conf['noise_dir'], self.sample_rate,\n",
    "                                            audio_conf['noise_levels']) if audio_conf.get(\n",
    "            'noise_dir') is not None else None\n",
    "        self.noise_prob = audio_conf.get('noise_prob')\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "        if self.augment:\n",
    "            y = load_randomly_augmented_audio(audio_path, self.sample_rate)\n",
    "        else:\n",
    "            y = load_audio(audio_path)\n",
    "\n",
    "        if self.noiseInjector:\n",
    "            logging.info(\"inject noise\")\n",
    "            add_noise = np.random.binomial(1, self.noise_prob)\n",
    "            if add_noise:\n",
    "                y = self.noiseInjector.inject_noise(y)\n",
    "\n",
    "        n_fft = int(self.sample_rate * self.window_size)\n",
    "        win_length = n_fft\n",
    "        hop_length = int(self.sample_rate * self.window_stride)\n",
    "\n",
    "        # Short-time Fourier transform (STFT)\n",
    "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                         win_length=win_length, window=self.window)\n",
    "                \n",
    "        spect, phase = librosa.magphase(D)\n",
    "\n",
    "        # S = log(S+1)\n",
    "        spect = np.log1p(spect)\n",
    "        spect = torch.FloatTensor(spect)\n",
    "\n",
    "        if self.normalize:\n",
    "            mean = spect.mean()\n",
    "            std = spect.std()\n",
    "            spect.add_(-mean)\n",
    "            spect.div_(std)\n",
    "\n",
    "        return spect\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectrogramDataset(Dataset, SpectrogramParser):\n",
    "    def __init__(self, audio_conf, manifest_filepath_list, \n",
    "                 label2id, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Dataset that loads tensors via a csv containing file paths to audio files and transcripts separated by\n",
    "        a comma. Each new line is a different sample. Example below:\n",
    "        /path/to/audio.wav,/path/to/audio.txt\n",
    "        ...\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param manifest_filepath: Path to manifest csv as describe above\n",
    "        :param labels: String containing all the possible characters to map to\n",
    "        :param normalize: Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment(default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        self.max_size = 0\n",
    "        self.ids_list = []\n",
    "        for i in range(len(manifest_filepath_list)):\n",
    "            manifest_filepath = manifest_filepath_list[i]\n",
    "            with open(manifest_filepath) as f:\n",
    "                ids = f.readlines()\n",
    "\n",
    "            ids = [x.strip().split(',') for x in ids]\n",
    "            self.ids_list.append(ids)\n",
    "            self.max_size = max(len(ids), self.max_size)\n",
    "\n",
    "        self.manifest_filepath_list = manifest_filepath_list\n",
    "        self.label2id = label2id\n",
    "        super(SpectrogramDataset, self).__init__(\n",
    "            audio_conf, normalize, augment)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        random_id = random.randint(0, len(self.ids_list)-1)\n",
    "        ids = self.ids_list[random_id]\n",
    "        sample = ids[index % len(ids)]\n",
    "        audio_path, transcript_path = sample[0], sample[1]\n",
    "        spect = self.parse_audio(audio_path)[:,:args.src_max_len]\n",
    "        transcript = self.parse_transcript(transcript_path)\n",
    "        return spect, transcript\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "            transcript = args.SOS_CHAR + transcript_file.read().replace('\\n', '').lower() + args.EOS_CHAR\n",
    "\n",
    "        transcript = list(\n",
    "            filter(None, [self.label2id.get(x) for x in list(transcript)]))\n",
    "        return transcript\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_size\n",
    "    \n",
    "\n",
    "class NoiseInjection(object):\n",
    "    def __init__(self,\n",
    "                 path=None,\n",
    "                 sample_rate=16000,\n",
    "                 noise_levels=(0, 0.5)):\n",
    "        \"\"\"\n",
    "        Adds noise to an input signal with specific SNR. Higher the noise level, the more noise added.\n",
    "        Modified code from https://github.com/willfrey/audio/blob/master/torchaudio/transforms.py\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            print(\"Directory doesn't exist: {}\".format(path))\n",
    "            raise IOError\n",
    "        self.paths = path is not None and librosa.util.find_files(path)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.noise_levels = noise_levels\n",
    "\n",
    "    def inject_noise(self, data):\n",
    "        noise_path = np.random.choice(self.paths)\n",
    "        noise_level = np.random.uniform(*self.noise_levels)\n",
    "        return self.inject_noise_sample(data, noise_path, noise_level)\n",
    "\n",
    "    def inject_noise_sample(self, data, noise_path, noise_level):\n",
    "        noise_len = get_audio_length(noise_path)\n",
    "        data_len = len(data) / self.sample_rate\n",
    "        noise_start = np.random.rand() * (noise_len - data_len)\n",
    "        noise_end = noise_start + data_len\n",
    "        noise_dst = audio_with_sox(\n",
    "            noise_path, self.sample_rate, noise_start, noise_end)\n",
    "        assert len(data) == len(noise_dst)\n",
    "        noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)\n",
    "        data_energy = np.sqrt(data.dot(data) / data.size)\n",
    "        data += noise_level * noise_dst * data_energy / noise_energy\n",
    "        return data    \n",
    "    \n",
    "\n",
    "def _collate_fn(batch):\n",
    "    def func(p):\n",
    "        return p[0].size(1)\n",
    "\n",
    "    def func_tgt(p):\n",
    "        return len(p[1])\n",
    "\n",
    "    # descending sorted\n",
    "    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
    "\n",
    "#     Max sequence: 236, Frequence size: 161 et Max taret lenght: 53\n",
    "\n",
    "    max_seq_len = max(batch, key=func)[0].size(1)\n",
    "    freq_size = max(batch, key=func)[0].size(0)\n",
    "    max_tgt_len = len(max(batch, key=func_tgt)[1])\n",
    "    \n",
    "#     print(f\"Max sequence: {max_seq_len}, Frequence size: {freq_size} et Max taret lenght: {max_tgt_len}\")\n",
    "\n",
    "    inputs = torch.zeros(len(batch), 1, freq_size, max_seq_len)\n",
    "    input_sizes = torch.IntTensor(len(batch))\n",
    "    input_percentages = torch.FloatTensor(len(batch))\n",
    "\n",
    "    targets = torch.zeros(len(batch), max_tgt_len).long()\n",
    "    target_sizes = torch.IntTensor(len(batch))\n",
    "    \n",
    "    for x in range(len(batch)):\n",
    "        sample = batch[x]\n",
    "        input_data = sample[0]\n",
    "        target = sample[1]\n",
    "        seq_length = input_data.size(1)\n",
    "        input_sizes[x] = seq_length\n",
    "        inputs[x][0].narrow(1, 0, seq_length).copy_(input_data)\n",
    "        input_percentages[x] = seq_length / float(max_seq_len)\n",
    "        target_sizes[x] = len(target)\n",
    "        targets[x][:len(target)] = torch.IntTensor(target)\n",
    "\n",
    "    return inputs, targets, input_percentages, input_sizes, target_sizes\n",
    "\n",
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn\n",
    "\n",
    "\n",
    "class BucketingSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size=1):\n",
    "        \"\"\"\n",
    "        Samples batches assuming they are in order of size to batch similarly sized samples together.\n",
    "        \"\"\"\n",
    "        super(BucketingSampler, self).__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        ids = list(range(0, len(data_source)))\n",
    "        self.bins = [ids[i:i + batch_size]\n",
    "                     for i in range(0, len(ids), batch_size)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for ids in self.bins:\n",
    "            np.random.shuffle(ids)\n",
    "            yield ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bins)\n",
    "\n",
    "    def shuffle(self, epoch):\n",
    "        np.random.shuffle(self.bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then create our tokenizers as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_ln = spacy.load('en')\n",
    "# spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_ln(text):\n",
    "#     \"\"\"\n",
    "#     Tokenizes German text from a string into a list of strings\n",
    "#     \"\"\"\n",
    "#     return text.split() # [tok.text for tok in spacy_ln.tokenizer(text)] #text.split()\n",
    "\n",
    "# def tokenize_en(text):\n",
    "#     \"\"\"\n",
    "#     Tokenizes English text from a string into a list of strings\n",
    "#     \"\"\"\n",
    "#     return text.split() #[tok.text for tok in spacy_en.tokenizer(text)] #text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fields are the same as the previous notebook. The model expects data to be fed in with the batch dimension first, so we use `batch_first = True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRC = Field(tokenize = tokenize_ln, \n",
    "#             init_token = '<sos>', \n",
    "#             eos_token = '<eos>', \n",
    "#             lower = True, \n",
    "#             batch_first = True)\n",
    "\n",
    "# TRG = Field(tokenize = tokenize_en, \n",
    "#             init_token = '<sos>', \n",
    "#             eos_token = '<eos>', \n",
    "#             lower = True, \n",
    "#             batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the Multi30k dataset and build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext import data, datasets\n",
    "\n",
    "# # train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "# #                                                     fields = (SRC, TRG))\n",
    "\n",
    "# MAX_LEN = 90\n",
    "\n",
    "\n",
    "# # load our dataset\n",
    "# train_data, valid_data = datasets.TranslationDataset.splits(train=\"train\", validation=\"valid\", test=None,\n",
    "#     path = \"./data\", exts = ('.en', '.ln'), fields=(SRC, TRG),\n",
    "#     filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "#     len(vars(x)['trg']) <= MAX_LEN\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRC.build_vocab(train_data, min_freq = 2)\n",
    "# TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the device and the data iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# train_iterator, valid_iterator = BucketIterator.splits(\n",
    "#     (train_data, valid_data), \n",
    "#      batch_size = BATCH_SIZE,\n",
    "#      device = device)\n",
    "\n",
    "# # train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "# #     (train_data, valid_data, test_data), \n",
    "# #      batch_size = BATCH_SIZE,\n",
    "# #      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='ASR training')\n",
    "\n",
    "parser.add_argument('--model', default='TRFS', type=str, help=\"TRFS:transformer\")\n",
    "parser.add_argument('--name', default='model', help=\"Name of the model for saving\")\n",
    "\n",
    "# Train\n",
    "parser.add_argument('--train-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--valid-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--test-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--lang-list', nargs='+', type=str)\n",
    "\n",
    "parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "parser.add_argument('--batch-size', default=20, type=int, help='Batch size for training') # 20\n",
    "parser.add_argument('--num-workers', default=4, type=int, \n",
    "                    help='Number of workers used in data-loading')\n",
    "\n",
    "parser.add_argument('--labels-path', default='labels.json', \n",
    "                    help='Contains all characters for transcription')\n",
    "parser.add_argument('--label-smoothing', default=0.0, type=float, help='Label smoothing')\n",
    "\n",
    "# Speech\n",
    "parser.add_argument('--window-size', default=.02, type=float, \n",
    "                    help='Window size for spectrogram in seconds')\n",
    "parser.add_argument('--window-stride', default=.01, type=float, \n",
    "                    help='Window stride for spectrogram in seconds')\n",
    "parser.add_argument('--window', default='hamming', \n",
    "                    help='Window type for spectrogram generation')\n",
    "\n",
    "parser.add_argument('--epochs', default=100, type=int, \n",
    "                    help='Number of training epochs') # 1000\n",
    "parser.add_argument('--cuda', dest='cuda', action='store_true', \n",
    "                    help='Use cuda to train model')\n",
    "\n",
    "parser.add_argument('--device-ids', default=None, nargs='+', type=int,\n",
    "                    help='If using cuda, sets the GPU devices for the process')\n",
    "parser.add_argument('--lr', '--learning-rate', default=3e-4, type=float, \n",
    "                    help='initial learning rate')\n",
    "\n",
    "parser.add_argument('--save-every', default=5, type=int, \n",
    "                    help='Save model every certain number of epochs')\n",
    "parser.add_argument('--save-folder', default='models/', \n",
    "                    help='Location to save epoch models')\n",
    "\n",
    "parser.add_argument('--emb_trg_sharing', action='store_true', \n",
    "                    help='Share embedding weight source and target')\n",
    "parser.add_argument('--feat_extractor', default='vgg_cnn', type=str, \n",
    "                    help='emb_cnn or vgg_cnn')\n",
    "\n",
    "parser.add_argument('--verbose', action='store_true', \n",
    "                    help='Verbose')\n",
    "\n",
    "parser.add_argument('--continue-from', default='', \n",
    "                    help='Continue from checkpoint model')\n",
    "parser.add_argument('--augment', dest='augment', action='store_true', \n",
    "                    help='Use random tempo and gain perturbations.')\n",
    "parser.add_argument('--noise-dir', default=None,\n",
    "                    help='Directory to inject noise into audio. If default, noise Inject not added')\n",
    "parser.add_argument('--noise-prob', default=0.4, \n",
    "                    help='Probability of noise being added per sample')\n",
    "parser.add_argument('--noise-min', default=0.0,\n",
    "                    help='Minimum noise level to sample from. (1.0 means all noise, not original signal)', type=float)\n",
    "parser.add_argument('--noise-max', default=0.5,\n",
    "                    help='Maximum noise levels to sample from. Maximum 1.0', type=float)\n",
    "\n",
    "# Transformer\n",
    "parser.add_argument('--num-layers', default=3, type=int, help='Number of layers')\n",
    "parser.add_argument('--num-heads', default=5, type=int, help='Number of heads')\n",
    "parser.add_argument('--dim-model', default=512, type=int, help='Model dimension')\n",
    "parser.add_argument('--dim-key', default=64, type=int, help='Key dimension')\n",
    "parser.add_argument('--dim-value', default=64, type=int, help='Value dimension')\n",
    "parser.add_argument('--dim-input', default=161, type=int, help='Input dimension')\n",
    "parser.add_argument('--dim-inner', default=1024, type=int, help='Inner dimension')\n",
    "parser.add_argument('--dim-emb', default=512, type=int, help='Embedding dimension')\n",
    "\n",
    "parser.add_argument('--src-max-len', default=4000, type=int, help='Source max length')\n",
    "parser.add_argument('--tgt-max-len', default=1000, type=int, help='Target max length')\n",
    "\n",
    "# Noam optimizer\n",
    "parser.add_argument('--warmup', default=4000, type=int, help='Warmup')\n",
    "parser.add_argument('--min-lr', default=1e-5, type=float, help='min lr')\n",
    "parser.add_argument('--k-lr', default=1, type=float, help='factor lr')\n",
    "\n",
    "# SGD optimizer\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--lr-anneal', default=1.1, type=float, help='lr anneal')\n",
    "\n",
    "# Decoder search\n",
    "parser.add_argument('--beam-search', action='store_true', help='Beam search')\n",
    "parser.add_argument('--beam-width', default=3, type=int, help='Beam size')\n",
    "parser.add_argument('--beam-nbest', default=5, type=int, help='Number of best sequences')\n",
    "parser.add_argument('--lm-rescoring', action='store_true', help='Rescore using LM')\n",
    "parser.add_argument('--lm-path', type=str, default=\"lm_model.pt\", help=\"Path to LM model\")\n",
    "parser.add_argument('--lm-weight', default=0.1, type=float, help='LM weight')\n",
    "parser.add_argument('--c-weight', default=0.1, type=float, help='Word count weight')\n",
    "parser.add_argument('--prob-weight', default=1.0, type=float, help='Probability E2E weight')\n",
    "\n",
    "# Loss\n",
    "parser.add_argument('--loss', type=str, default='ce', help='ce or ctc')\n",
    "parser.add_argument('--clip', action='store_true', help=\"clip\")\n",
    "parser.add_argument('--max-norm', default=400, type=float, help=\"max norm for clipping\")\n",
    "\n",
    "parser.add_argument('--dropout', default=0.1, type=float, help='Dropout')\n",
    "\n",
    "# Parallelize model\n",
    "parser.add_argument('--parallel', action='store_true', help='Parallelize the model')\n",
    "\n",
    "# shuffle\n",
    "parser.add_argument('--shuffle', action='store_true', help='Shuffle')\n",
    "\n",
    "# PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "parser.add_argument('--PAD_CHAR', default=\"\", type=str, help='PAD_CHAR')\n",
    "parser.add_argument('--SOS_CHAR', default=\"\", type=str, help='SOS_CHAR')\n",
    "parser.add_argument('--EOS_CHAR', default=\"\", type=str, help='EOS_CHAR')\n",
    "parser.add_argument('--PAD_TOKEN', default=0, type=int, help='PAD_TOKEN')\n",
    "parser.add_argument('--SOS_TOKEN', default=1, type=int, help='SOS_TOKEN')\n",
    "parser.add_argument('--EOS_TOKEN', default=2, type=int, help='EOS_TOKEN')\n",
    "\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "torch.cuda.manual_seed_all(123456)\n",
    "\n",
    "# https://github.com/spyder-ide/spyder/issues/3883\n",
    "import sys\n",
    "sys.argv=['']; del sys \n",
    "\n",
    "args = parser.parse_args()\n",
    "USE_CUDA = args.cuda\n",
    "\n",
    "# PAD_TOKEN = 0\n",
    "# SOS_TOKEN = 1\n",
    "# EOS_TOKEN = 2\n",
    "\n",
    "# PAD_CHAR = \"\"\n",
    "# SOS_CHAR = \"\"\n",
    "# EOS_CHAR = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train_manifest_list = ['data/manifests/libri_train_manifest.csv']\n",
    "args.valid_manifest_list = ['data/manifests/libri_val_manifest.csv']\n",
    "args.test_manifest_list = ['data/manifests/libri_test_manifest.csv']\n",
    "\n",
    "args.batch_size = 6\n",
    "args.labels_path = 'data/labels/labels.json'\n",
    "args.lr = 1e-4\n",
    "args.name = 'libri_drop0.1_cnn_batch12_6_vgg_layer_notebook'\n",
    "args.save_folder = 'save/'\n",
    "args.save_every = 10\n",
    "args.feat_extractor = 'vgg_cnn'\n",
    "args.dropout = 0.1\n",
    "args.num_layers = 2\n",
    "args.num_heads = 6\n",
    "args.dim_model = 512\n",
    "args.dim_key = 64\n",
    "args.dim_value = 64\n",
    "args.dim_input = 161\n",
    "args.dim_inner = 2048\n",
    "args.dim_emb = 512\n",
    "args.shuffle = True\n",
    "args.min_lr = 1e-6\n",
    "args.k_lr = 1\n",
    "args.target_dir = '../../../../big_data/end2end-asr-pytorch/LibriSpeech_dataset/'\n",
    "args.sample_rate = 1600 #16000\n",
    "\n",
    "args.window_size = 0.02\n",
    "args.window_stride = 0.01 #0.01\n",
    "args.window = 'hamming'\n",
    "# args.noise_dir = \n",
    "args.noise_prob = 0.4\n",
    "args.noise_min = 0.0\n",
    "args.noise_max = 0.5\n",
    "args.cuda = torch.cuda.is_available()\n",
    "args.epochs = 1000\n",
    "args.continue_from = 'save/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook/best_model.th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "THE EXPERIMENT LOG IS SAVED IN: log/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook\n",
      "TRAINING MANIFEST:  ['data/manifests/libri_train_manifest.csv']\n",
      "VALID MANIFEST:  ['data/manifests/libri_val_manifest.csv']\n",
      "TEST MANIFEST:  ['data/manifests/libri_test_manifest.csv']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"THE EXPERIMENT LOG IS SAVED IN: \" + \"log/\" + args.name)\n",
    "print(\"TRAINING MANIFEST: \", args.train_manifest_list)\n",
    "print(\"VALID MANIFEST: \", args.valid_manifest_list)\n",
    "print(\"TEST MANIFEST: \", args.test_manifest_list)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(filename=\"log/\" + args.name, filemode='w+', \n",
    "#                     format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "audio_conf = dict(sample_rate=args.sample_rate,\n",
    "                  window_size=args.window_size,\n",
    "                  window_stride=args.window_stride,\n",
    "                  window=args.window,\n",
    "                  noise_dir=args.noise_dir,\n",
    "                  noise_prob=args.noise_prob,\n",
    "                  noise_levels=(args.noise_min, args.noise_max))\n",
    "\n",
    "logging.info(audio_conf)\n",
    "\n",
    "with open(args.labels_path) as label_file:\n",
    "        labels = str(''.join(json.load(label_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '': 1,\n",
       " '': 2,\n",
       " '_': 3,\n",
       " \"'\": 4,\n",
       " 'a': 5,\n",
       " 'b': 6,\n",
       " 'c': 7,\n",
       " 'd': 8,\n",
       " 'e': 9,\n",
       " 'f': 10,\n",
       " 'g': 11,\n",
       " 'h': 12,\n",
       " 'i': 13,\n",
       " 'j': 14,\n",
       " 'k': 15,\n",
       " 'l': 16,\n",
       " 'm': 17,\n",
       " 'n': 18,\n",
       " 'o': 19,\n",
       " 'p': 20,\n",
       " 'q': 21,\n",
       " 'r': 22,\n",
       " 's': 23,\n",
       " 't': 24,\n",
       " 'u': 25,\n",
       " 'v': 26,\n",
       " 'w': 27,\n",
       " 'x': 28,\n",
       " 'y': 29,\n",
       " 'z': 30,\n",
       " ' ': 31}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "\n",
    "labels = args.PAD_CHAR + args.SOS_CHAR + args.EOS_CHAR + labels\n",
    "label2id, id2label = {}, {}\n",
    "count = 0\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] not in label2id:\n",
    "        label2id[labels[i]] = count\n",
    "        id2label[count] = labels[i]\n",
    "        count += 1\n",
    "    else:\n",
    "        print(\"multiple label: \", labels[i])\n",
    "        \n",
    "# label2id = dict([(labels[i], i) for i in range(len(labels))])\n",
    "# id2label = dict([(i, labels[i]) for i in range(len(labels))])\n",
    "label2id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "train_data = SpectrogramDataset(\n",
    "    audio_conf, manifest_filepath_list=args.train_manifest_list,\n",
    "    label2id=label2id, normalize=True, augment=args.augment)\n",
    "\n",
    "train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "# train_sampler = BucketingSampler(train_data, batch_size=10)\n",
    "\n",
    "\n",
    "train_loader = AudioDataLoader(\n",
    "    train_data, num_workers=args.num_workers, batch_sampler=train_sampler)\n",
    "\n",
    "valid_data = SpectrogramDataset(\n",
    "        audio_conf, manifest_filepath_list=args.valid_manifest_list, \n",
    "        label2id=label2id, normalize=True, augment=False)\n",
    "    \n",
    "valid_loader = AudioDataLoader(valid_data, num_workers=args.num_workers, \n",
    "                               batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(valid_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.ne(torch.tensor([[1, 2], \n",
    "                       [9, 4]]), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_list(xs, pad_value):\n",
    "    \"\"\"\n",
    "    Given a list of Tensor and the pad_value\n",
    "    the function will return a tensor of shape [len[xs], max_len]\n",
    "    \n",
    "    args:\n",
    "        xs        : List[Tensor]\n",
    "        pad_value : value to pad\n",
    "    output:\n",
    "        seq       : String\n",
    "    \"\"\"\n",
    "    # From: espnet/src/nets/e2e_asr_th.py: pad_list()\n",
    "    n_batch = len(xs)\n",
    "    # max_len = max(x.size(0) for x in xs)\n",
    "    max_len = args.tgt_max_len # hypper-parameter here\n",
    "    pad = xs[0].new(n_batch, max_len, * xs[0].size()[1:]).fill_(pad_value)\n",
    "    for i in range(n_batch):\n",
    "        pad[i, :xs[i].size(0)] = xs[i]\n",
    "    return pad\n",
    "\n",
    "\"\"\" \n",
    "Transformer common layers\n",
    "\"\"\"\n",
    "\n",
    "def get_non_pad_mask(padded_input, input_lengths=None, pad_idx=None):\n",
    "    \"\"\"\n",
    "    padding position is set to 0, either use input_lengths or pad_idx\n",
    "    \n",
    "    args:\n",
    "        padded_input   : Tensor[B, T [, ...]]\n",
    "        input_lengths  : Tensor[B]\n",
    "        pad_idx        : int\n",
    "    output:\n",
    "        non_pad_mask   : Tensor[B, T, 1]\n",
    "        \n",
    "    \"\"\"\n",
    "    assert input_lengths is not None or pad_idx is not None\n",
    "    if input_lengths is not None:\n",
    "        # padded_input: N x T x ..\n",
    "        N = padded_input.size(0)\n",
    "        non_pad_mask = padded_input.new_ones(padded_input.size()[:-1])  # B x T\n",
    "        for i in range(N):\n",
    "            non_pad_mask[i, input_lengths[i]:] = 0\n",
    "    if pad_idx is not None:\n",
    "        # padded_input: N x T\n",
    "        assert padded_input.dim() == 2\n",
    "        # Give True when  padded_input != pad_idx element-wise. or\n",
    "        # if pad_idx is not None, check \n",
    "        # https://pytorch.org/docs/stable/generated/torch.ne.html \n",
    "        non_pad_mask = padded_input.ne(pad_idx).float()\n",
    "        \n",
    "    ipdb.set_trace()\n",
    "    \n",
    "    # unsqueeze(-1) for broadcast\n",
    "    return non_pad_mask.unsqueeze(-1)\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q, pad_idx):\n",
    "    \"\"\"\n",
    "    For masking out the padding part of key sequence.\n",
    "    \n",
    "    args:\n",
    "        seq_k        : Tensor[B, T [, ...]]\n",
    "        seq_q        : Tensor[B, T [, ...]]\n",
    "        pad_idx      : int\n",
    "    output:\n",
    "        padding_mask : Tensor[B, T, T]\n",
    "        \n",
    "    \"\"\"\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = seq_k.eq(pad_idx)\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # B x T_Q x T_K\n",
    "\n",
    "    ipdb.set_trace()\n",
    "    \n",
    "    return padding_mask\n",
    "\n",
    "def get_attn_pad_mask(padded_input, input_lengths, expand_length):\n",
    "    \"\"\"\n",
    "    mask position is set to 1 (True)\n",
    "    \n",
    "    args:\n",
    "        padded_input   : Tensor[B, T, ...]\n",
    "        input_lengths  : Tensor[B]\n",
    "        expand_length  : int[T]\n",
    "    output:\n",
    "        attn_mask      : Tensor[B, T, T]\n",
    "        \n",
    "    \"\"\"\n",
    "    # N x Ti x 1\n",
    "    non_pad_mask = get_non_pad_mask(padded_input, input_lengths=input_lengths)\n",
    "    # N x Ti, lt(1) like not operation\n",
    "    pad_mask = non_pad_mask.squeeze(-1).lt(1)\n",
    "    attn_mask = pad_mask.unsqueeze(1).expand(-1, expand_length, -1)\n",
    "    \n",
    "    ipdb.set_trace()\n",
    "    \n",
    "    return attn_mask\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    \"\"\"\n",
    "    For masking out the subsequent info. \n",
    "    \n",
    "    args:\n",
    "        seq             : Tensor[B, T [, ...]]\n",
    "\n",
    "    output:\n",
    "        subsequent_mask : Tensor[B, T, T]\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = torch.triu(\n",
    "        torch.ones((len_s, len_s), device=seq.device, dtype=torch.uint8), diagonal=1)\n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n",
    "\n",
    "    ipdb.set_trace()\n",
    "    \n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Next, we'll build the model. Like previous notebooks it is made up of an *encoder* and a *decoder*, with the encoder *encoding* the input/source sentence (in German) into *context vector* and the decoder then *decoding* this context vector to output our output/target sentence (in English). \n",
    "\n",
    "### Encoder\n",
    "\n",
    "Similar to the ConvSeq2Seq model, the Transformer's encoder does not attempt to compress the entire source sentence, $X = (x_1, ... ,x_n)$, into a single context vector, $z$. Instead it produces a sequence of context vectors, $Z = (z_1, ... , z_n)$. So, if our input sequence was 5 tokens long we would have $Z = (z_1, z_2, z_3, z_4, z_5)$. Why do we call this a sequence of context vectors and not a sequence of hidden states? A hidden state at time $t$ in an RNN has only seen tokens $x_t$ and all the tokens before it. However, each context vector here has seen all tokens at all positions within the input sequence.\n",
    "\n",
    "![](assets/transformer-encoder.png)\n",
    "\n",
    "First, the tokens are passed through a standard embedding layer. Next, as the model has no recurrent it has no idea about the order of the tokens within the sequence. We solve this by using a second embedding layer called a *positional embedding layer*. This is a standard embedding layer where the input is not the token itself but the position of the token within the sequence, starting with the first token, the `<sos>` (start of sequence) token, in position 0. The position embedding has a \"vocabulary\" size of 100, which means our model can accept sentences up to 100 tokens long. This can be increased if we want to handle longer sentences.\n",
    "\n",
    "The original Transformer implementation from the Attention is All You Need paper does not learn positional embeddings. Instead it uses a fixed static embedding. Modern Transformer architectures, like BERT, use positional embeddings instead, hence we have decided to use them in these tutorials. Check out [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding) section to read more about the positional embeddings used in the original Transformer model.\n",
    "\n",
    "Next, the token and positional embeddings are elementwise summed together to get a vector which contains information about the token and also its position with in the sequence. However, before they are summed, the token embeddings are multiplied by a scaling factor which is $\\sqrt{d_{model}}$, where $d_{model}$ is the hidden dimension size, `hid_dim`. This supposedly reduces variance in the embeddings and the model is difficult to train reliably without this scaling factor. Dropout is then applied to the combined embeddings.\n",
    "\n",
    "The combined embeddings are then passed through $N$ *encoder layers* to get $Z$, which is then output and can be used by the decoder.\n",
    "\n",
    "The source mask, `src_mask`, is simply the same shape as the source sentence but has a value of 1 when the token in the source sentence is not a `<pad>` token and 0 when it is a `<pad>` token. This is used in the encoder layers to mask the multi-head attention mechanisms, which are used to calculate and apply attention over the source sentence, so the model does not pay attention to `<pad>` tokens, which contain no useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(len(valid_loader_list)):\n",
    "    valid_loader = valid_loader_list[ind]\n",
    "\n",
    "    total_valid_loss, total_valid_cer, total_valid_wer, total_valid_char, total_valid_word = 0, 0, 0, 0, 0\n",
    "    valid_pbar = tqdm(iter(valid_loader), leave=True, total=len(valid_loader))\n",
    "    for i, (data) in enumerate(valid_pbar):\n",
    "        src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "        if args.cuda:\n",
    "            src = src.cuda()\n",
    "            tgt = tgt.cuda()\n",
    "\n",
    "        pred, gold, hyp_seq, gold_seq = model(src, src_lengths, tgt, verbose=False)\n",
    "\n",
    "        seq_length = pred.size(1)\n",
    "        sizes = Variable(src_percentages.mul_(int(seq_length)).int(), requires_grad=False)\n",
    "\n",
    "        loss, num_correct = calculate_metrics(\n",
    "            pred, gold, input_lengths=sizes, target_lengths=tgt_lengths, smoothing=smoothing, loss_type=loss_type)\n",
    "\n",
    "        if loss.item() == float('Inf'):\n",
    "            logging.info(\"Found infinity loss, masking\")\n",
    "            loss = torch.where(loss != loss, torch.zeros_like(loss), loss) # NaN masking\n",
    "            continue\n",
    "\n",
    "        try: # handle case for CTC\n",
    "            strs_gold, strs_hyps = [], []\n",
    "            for ut_gold in gold_seq:\n",
    "                str_gold = \"\"\n",
    "                for x in ut_gold:\n",
    "                    if int(x) == args.PAD_TOKEN:\n",
    "                        break\n",
    "                    str_gold = str_gold + id2label[int(x)]\n",
    "                strs_gold.append(str_gold)\n",
    "            for ut_hyp in hyp_seq:\n",
    "                str_hyp = \"\"\n",
    "                for x in ut_hyp:\n",
    "                    if int(x) == args.PAD_TOKEN:\n",
    "                        break\n",
    "                    str_hyp = str_hyp + id2label[int(x)]\n",
    "                strs_hyps.append(str_hyp)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.info(\"NaN predictions\")\n",
    "            continue\n",
    "\n",
    "        for j in range(len(strs_hyps)):\n",
    "            strs_hyps[j] = strs_hyps[j].replace(args.SOS_CHAR, '').replace(args.EOS_CHAR, '')\n",
    "            strs_gold[j] = strs_gold[j].replace(args.SOS_CHAR, '').replace(args.EOS_CHAR, '')\n",
    "#                         ipdb.set_trace()\n",
    "            cer = calculate_cer(strs_hyps[j].replace(' ', ''), strs_gold[j].replace(' ', ''))\n",
    "            wer = calculate_wer(strs_hyps[j], strs_gold[j])\n",
    "            total_valid_cer += cer\n",
    "            total_valid_wer += wer\n",
    "            total_valid_char += len(strs_gold[j].replace(' ', ''))\n",
    "            total_valid_word += len(strs_gold[j].split(\" \"))\n",
    "\n",
    "        total_valid_loss += loss.item()\n",
    "#                     valid_pbar.set_description(\"VALID SET {} LOSS:{:.4f} CER:{:.2f}%\".format(ind,\n",
    "#                         total_valid_loss/(i+1), total_valid_cer*100/total_valid_char))\n",
    "\n",
    "        valid_pbar.set_description(\n",
    "            f\"VALID SET {epoch+1} || LOSS: {total_valid_loss/(i+1):.4f} || CER:{total_valid_cer*100/total_valid_char:.2f}% || WER:{total_valid_wer*100/total_valid_word:.2f}%\")\n",
    "\n",
    "#                 logging.info(\"VALID SET {} LOSS:{:.4f} CER:{:.2f}%\".format(ind,\n",
    "#                         total_valid_loss/(len(valid_loader)), total_valid_cer*100/total_valid_char))\n",
    "\n",
    "#                 logging.info(\"VALID SET {} LOSS:{:.4f} CER:{:.2f}%\".format((epoch+1),\n",
    "#                         total_valid_loss/(len(valid_loader)), total_valid_cer*100/total_valid_char))\n",
    "\n",
    "    logging.info(\n",
    "        f\"VALID SET {epoch+1}||LOSS: {total_valid_loss/(len(valid_loader)):.4f} || CER: {total_valid_cer*100/total_valid_char:.2f}% || WER: {total_valid_wer*100/total_valid_word:.2f}%\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(load_path):\n",
    "    \"\"\"\n",
    "    Loading model\n",
    "    args:\n",
    "        load_path: string\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(load_path)\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metrics']\n",
    "    if 'args' in checkpoint:\n",
    "        args = checkpoint['args']\n",
    "\n",
    "    label2id = checkpoint['label2id']\n",
    "    id2label = checkpoint['id2label']\n",
    "\n",
    "    model = init_transformer_model(args, label2id, id2label)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if args.cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    opt = init_optimizer(args, model)\n",
    "    if opt is not None:\n",
    "        opt.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if args.loss == \"ce\":\n",
    "            opt._step = checkpoint['optimizer_params']['_step']\n",
    "            opt._rate = checkpoint['optimizer_params']['_rate']\n",
    "            opt.warmup = checkpoint['optimizer_params']['warmup']\n",
    "            opt.factor = checkpoint['optimizer_params']['factor']\n",
    "            opt.model_size = checkpoint['optimizer_params']['model_size']\n",
    "        elif args.loss == \"ctc\":\n",
    "            opt.lr = checkpoint['optimizer_params']['lr']\n",
    "            opt.lr_anneal = checkpoint['optimizer_params']['lr_anneal']\n",
    "        else:\n",
    "            print(\"Need to define loss type\")\n",
    "\n",
    "    return model, opt, epoch, metrics, args, label2id, id2label\n",
    "\n",
    "def init_optimizer(args, model, opt_type=\"noam\"):\n",
    "    dim_input = args.dim_input\n",
    "    warmup = args.warmup\n",
    "    lr = args.lr\n",
    "\n",
    "    if opt_type == \"noam\":\n",
    "        opt = NoamOpt(dim_input, args.k_lr, warmup, torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9), min_lr=args.min_lr)\n",
    "    elif opt_type == \"sgd\":\n",
    "        opt = AnnealingOpt(lr, args.lr_anneal, torch.optim.SGD(model.parameters(), lr=lr, momentum=args.momentum, nesterov=True))\n",
    "    else:\n",
    "        opt = None\n",
    "        print(\"Optimizer is not defined\")\n",
    "\n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        encoder_self_attn_list = []\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src, self_attn = layer(src, src_mask)\n",
    "            encoder_self_attn_list += [self_attn]\n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src, encoder_self_attn_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "The encoder layers are where all of the \"meat\" of the encoder is contained. We first pass the source sentence and its mask into the *multi-head attention layer*, then perform dropout on it, apply a residual connection and pass it through a [Layer Normalization](https://arxiv.org/abs/1607.06450) layer. We then pass it through a *position-wise feedforward* layer and then, again, apply dropout, a residual connection and then layer normalization to get the output of this layer which is fed into the next layer. The parameters are not shared between layers. \n",
    "\n",
    "The mutli head attention layer is used by the encoder layer to attend to the source sentence, i.e. it is calculating and applying attention over itself instead of another sequence, hence we call it *self attention*.\n",
    "\n",
    "[This](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/) article goes into more detail about layer normalization, but the gist is that it normalizes the values of the features, i.e. across the hidden dimension, so each feature has a mean of 0 and a standard deviation of 1. This allows neural networks with a larger number of layers, like the Transformer, to be trained easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, src len]\n",
    "                \n",
    "        #self attention\n",
    "        _src, self_attn = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src, self_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "One of the key, novel concepts introduced by the Transformer paper is the *multi-head attention layer*. \n",
    "\n",
    "![](assets/transformer-attention.png)\n",
    "\n",
    "Attention can be though of as *queries*, *keys* and *values* - where the query is used with the key to get an attention vector (usually the output of a *softmax* operation and has all values between 0 and 1 which sum to 1) which is then used to get a weighted sum of the values.\n",
    "\n",
    "The Transformer uses *scaled dot-product attention*, where the query and key are combined by taking the dot product between them, then applying the softmax operation and scaling by $d_k$ before finally then multiplying by the value. $d_k$ is the *head dimension*, `head_dim`, which we will shortly explain further.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ \n",
    "\n",
    "This is similar to standard *dot product attention* but is scaled by $d_k$, which the paper states is used to stop the results of the dot products growing large, causing gradients to become too small.\n",
    "\n",
    "However, the scaled dot-product attention isn't simply applied to the queries, keys and values. Instead of doing a single attention application the queries, keys and values have their `hid_dim` split into $h$ *heads* and the scaled dot-product attention is calculated over all heads in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$. We then re-combine the heads into their `hid_dim` shape, thus each `hid_dim` is potentially paying attention to $h$ different concepts.\n",
    "\n",
    "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
    "\n",
    "$W^O$ is the linear layer applied at the end of the multi-head attention layer, `fc`. $W^Q, W^K, W^V$ are the linear layers `fc_q`, `fc_k` and `fc_v`.\n",
    "\n",
    "Walking through the module, first we calculate $QW^Q$, $KW^K$ and $VW^V$ with the linear layers, `fc_q`, `fc_k` and `fc_v`, to give us `Q`, `K` and `V`. Next, we split the `hid_dim` of the query, key and value into `n_heads` using `.view` and correctly permute them so they can be multiplied together. We then calculate the `energy` (the un-normalized attention) by multiplying `Q` and `K` together and scaling it by the square root of `head_dim`, which is calulated as `hid_dim // n_heads`. We then mask the energy so we do not pay attention over any elements of the sequeuence we shouldn't, then apply the softmax and dropout. We then apply the attention to the value heads, `V`, before combining the `n_heads` together. Finally, we multiply this $W^O$, represented by `fc_o`. \n",
    "\n",
    "Note that in our implementation the lengths of the keys and values are always the same, thus when matrix multiplying the output of the softmax, `attention`, with `V` we will always have valid dimension sizes for matrix multiplication. This multiplication is carried out using `torch.matmul` which, when both tensors are >2-dimensional, does a batched matrix multiplication over the last two dimensions of each tensor. This will be a **[query len, key len] x [value len, head dim]** batched matrix multiplication over the batch size and each head which provides the **[batch size, n heads, query len, head dim]** result.\n",
    "\n",
    "One thing that looks strange at first is that dropout is applied directly to the attention. This means that our attention vector will most probably not sum to 1 and we may pay full attention to a token but the attention over that token is set to 0 by dropout. This is never explained, or even mentioned, in the paper however is used by the [official implementation](https://github.com/tensorflow/tensor2tensor/) and every Transformer implementation since, [including BERT](https://github.com/google-research/bert/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer\n",
    "\n",
    "The other main block inside the encoder layer is the *position-wise feedforward layer* This is relatively simple compared to the multi-head attention layer. The input is transformed from `hid_dim` to `pf_dim`, where `pf_dim` is usually a lot larger than `hid_dim`. The original Transformer used a `hid_dim` of 512 and a `pf_dim` of 2048. The ReLU activation function and dropout are applied before it is transformed back into a `hid_dim` representation. \n",
    "\n",
    "Why is this used? Unfortunately, it is never explained in the paper.\n",
    "\n",
    "BERT uses the [GELU](https://arxiv.org/abs/1606.08415) activation function, which can be used by simply switching `torch.relu` for `F.gelu`. Why did they use GELU? Again, it is never explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class PositionwiseFeedForwardWithConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feedforward Layer Implementation with Convolution class\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, dim_hidden, dropout=0.1):\n",
    "        super(PositionwiseFeedForwardWithConv, self).__init__()\n",
    "        self.conv_1 = nn.Conv1d(dim_model, dim_hidden, 1)\n",
    "        self.conv_2 = nn.Conv1d(dim_hidden, dim_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.conv_2(F.relu(self.conv_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The objective of the decoder is to take the encoded representation of the source sentence, $Z$, and convert it into predicted tokens in the target sentence, $\\hat{Y}$. We then compare $\\hat{Y}$ with the actual tokens in the target sentence, $Y$, to calculate our loss, which will be used to calculate the gradients of our parameters and then use our optimizer to update our weights in order to improve our predictions. \n",
    "\n",
    "![](assets/transformer-decoder.png)\n",
    "\n",
    "The decoder is similar to encoder, however it now has two multi-head attention layers. A *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value.\n",
    "\n",
    "The decoder uses positional embeddings and combines - via an elementwise sum - them with the scaled embedded target tokens, followed by dropout. Again, our positional encodings have a \"vocabulary\" of 100, which means they can accept sequences up to 100 tokens long. This can be increased if desired.\n",
    "\n",
    "The combined embeddings are then passed through the $N$ decoder layers, along with the encoded source, `enc_src`, and the source and target masks. Note that the number of layers in the encoder does not have to be equal to the number of layers in the decoder, even though they are both denoted by $N$.\n",
    "\n",
    "The decoder representation after the $N^{th}$ layer is then passed through a linear layer, `fc_out`. In PyTorch, the softmax operation is contained within our loss function, so we do not explicitly need to use a softmax layer here.\n",
    "\n",
    "As well as using the source mask, as we did in the encoder to prevent our model attending to `<pad>` tokens, we also use a target mask. This will be explained further in the `Seq2Seq` model which encapsulates both the encoder and decoder, but the gist of it is that it performs a similar operation as the decoder padding in the convolutional sequence-to-sequence model. As we are processing all of the target tokens at once in parallel we need a method of stopping the decoder from \"cheating\" by simply \"looking\" at what the next token in the target sequence is and outputting it. \n",
    "\n",
    "Our decoder layer also outputs the normalized attention values so we can later plot them to see what our model is actually paying attention to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "                            \n",
    "        #pos = [batch size, trg len]\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "                \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "As mentioned previously, the decoder layer is similar to the encoder layer except that it now has two multi-head attention layers, `self_attention` and `encoder_attention`. \n",
    "\n",
    "The first performs self-attention, as in the encoder, by using the decoder representation so far as the query, key and value. This is followed by dropout, residual connection and layer normalization. This `self_attention` layer uses the target sequence mask, `trg_mask`, in order to prevent the decoder from \"cheating\" by paying attention to tokens that are \"ahead\" of the one it is currently processing as it processes all tokens in the target sentence in parallel.\n",
    "\n",
    "The second is how we actually feed the encoded source sentence, `enc_src`, into our decoder. In this multi-head attention layer the queries are the decoder representations and the keys and values are the decoder representations. Here, the source mask, `src_mask` is used to prevent the multi-head attention layer from attending to `<pad>` tokens within the source sentence. This is then followed by the dropout, residual connection and layer normalization layers. \n",
    "\n",
    "Finally, we pass this through the position-wise feedforward layer and yet another sequence of dropout, residual connection and layer normalization.\n",
    "\n",
    "The decoder layer isn't introducing any new concepts, just using the same set of layers as the encoder in a slightly different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "Finally, we have the `Seq2Seq` module which encapsulates the encoder and decoder, as well as handling the creation of the masks.\n",
    "\n",
    "The source mask is created by checking where the source sequence is not equal to a `<pad>` token. It is 1 where the token is not a `<pad>` token and 0 when it is. It is then unsqueezed so it can be correctly broadcast when applying the mask to the `energy`, which of shape **_[batch size, n heads, seq len, seq len]_**.\n",
    "\n",
    "The target mask is slightly more complicated. First, we create a mask for the `<pad>` tokens, as we did for the source mask. Next, we create a \"subsequent\" mask, `trg_sub_mask`, using `torch.tril`. This creates a diagonal matrix where the elements above the diagonal will be zero and the elements below the diagonal will be set to whatever the input tensor is. In this case, the input tensor will be a tensor filled with ones. So this means our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "This shows what each target token (row) is allowed to look at (column). The first target token has a mask of **_[1, 0, 0, 0, 0]_** which means it can only look at the first target token. The second target token has a mask of **_[1, 1, 0, 0, 0]_** which it means it can look at both the first and second target tokens. \n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "After the masks are created, they used with the encoder and decoder along with the source and target sentences to get our predicted target sentence, `output`, along with the decoder's attention over the source sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Seq2Seq Model\n",
    "\n",
    "\n",
    "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but is able to be run on a single GPU quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-42-35e1237280e2>\u001b[0m(28)\u001b[0;36mcalculate_wer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     27 \u001b[0;31m    \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 28 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mLev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  print(w1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x03', '\\x00']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ''.join(w1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\x03\\x00'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  Lev.distance(''.join(w1), ''.join(w2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-35e1237280e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mcalculate_wer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello everyone\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Hello hope you are\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-35e1237280e2>\u001b[0m in \u001b[0;36mcalculate_wer\u001b[0;34m(s1, s2)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-35e1237280e2>\u001b[0m in \u001b[0;36mcalculate_wer\u001b[0;34m(s1, s2)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import Levenshtein as Lev\n",
    "import ipdb\n",
    "\n",
    "def calculate_wer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    \n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    \n",
    "    # word2char get an integer representation to each word in \n",
    "    # the dictionnary b \n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "\n",
    "    \n",
    "    ipdb.set_trace()\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "\n",
    "calculate_wer(\"Hello everyone\", \"Hello hope you are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 1, 2, 2, 3]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab = {'H':5, 'e':1, 'l':2, 'o':3}\n",
    "\n",
    "list(filter(None, [lab.get(x) for x in list(\"Hello\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SRC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a146880cd59b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mINPUT_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mHID_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mENC_LAYERS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDEC_LAYERS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SRC' is not defined"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab: 41240\n",
      "Target vocab: 41768\n"
     ]
    }
   ],
   "source": [
    "print(f\"Source vocab: {INPUT_DIM}\")\n",
    "print(f\"Target vocab: {OUTPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use them to define our whole sequence-to-sequence encapsulating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of parameters, noticing it is significantly less than the 37M for the convolutional sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 35,989,288 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper does not mention which weight initialization scheme was used, however Xavier uniform seems to be common amongst Transformer models, so we use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer used in the original Transformer paper uses Adam with a learning rate that has a \"warm-up\" and then a \"cool-down\" period. BERT and other Transformer models use Adam with a fixed learning rate, so we will implement that. Check [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#optimizer) link for more details about the original Transformer's learning rate schedule.\n",
    "\n",
    "Note that the learning rate needs to be lower than the default used by Adam or else learning is unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function, making sure to ignore losses calculated over `<pad>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "melfbanks = torchaudio.transforms.MelSpectrogram(n_mels=80)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        x_batch_len = x.shape[-1]\n",
    "        x = torch.log(1. + melfbanks(x))\n",
    "        x = x.squeeze()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "#         if output.shape[0] != trg.shape[0] or output.shape[1]!=6256:\n",
    "#             ipdb.set_trace()\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation loop is the same as the training loop, just without the gradient calculations and parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a small function that we can use to tell us how long an epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CUDA_LAUNCH_BLOCKING=1\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model ...\n",
      "*****************************************************\n",
      "best loss: [epoch: 0], [valid loss 1.4111332705792259]\n",
      "*****************************************************\n",
      "Epoch: 01 | Time: 19m 28s\n",
      "\tTrain Loss: 3.026 | Train PPL:  20.618\n",
      "\t Val. Loss: 1.411 |  Val. PPL:   4.101\n",
      "Saving Model ...\n",
      "*****************************************************\n",
      "best loss: [epoch: 1], [valid loss 1.1523114689132747]\n",
      "*****************************************************\n",
      "Epoch: 02 | Time: 19m 18s\n",
      "\tTrain Loss: 2.163 | Train PPL:   8.696\n",
      "\t Val. Loss: 1.152 |  Val. PPL:   3.166\n",
      "Saving Model ...\n",
      "*****************************************************\n",
      "best loss: [epoch: 2], [valid loss 1.0556168593028012]\n",
      "*****************************************************\n",
      "Epoch: 03 | Time: 19m 12s\n",
      "\tTrain Loss: 1.938 | Train PPL:   6.947\n",
      "\t Val. Loss: 1.056 |  Val. PPL:   2.874\n",
      "Saving Model ...\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss        \n",
    "        print('Saving Model ...')\n",
    "        torch.save(model.state_dict(), 'model/Model_MT_'+str(best_valid_loss)[:4]+'.pt')\n",
    "        print('*****************************************************')\n",
    "        print(f'best loss: [epoch: {epoch}], [valid loss {best_valid_loss}]')\n",
    "        print('*****************************************************')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our \"best\" parameters and manage to achieve a better test perplexity than all previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Valid Loss: 0.672 | Test PPL:   1.958 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model/Model_MT.pt'))\n",
    "\n",
    "valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "print(f'| Valid Loss: {valid_loss:.3f} | Test PPL: {math.exp(valid_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now we can can translations from our model with the `translate_sentence` function below.\n",
    "\n",
    "The steps taken are:\n",
    "- tokenize the source sentence if it has not been tokenized (is a string)\n",
    "- append the `<sos>` and `<eos>` tokens\n",
    "- numericalize the source sentence\n",
    "- convert it to a tensor and add a batch dimension\n",
    "- create the source sentence mask\n",
    "- feed the source sentence and mask into the encoder\n",
    "- create a list to hold the output sentence, initialized with an `<sos>` token\n",
    "- while we have not hit a maximum length\n",
    "  - convert the current output sentence prediction into a tensor with a batch dimension\n",
    "  - create a target sentence mask\n",
    "  - place the current output, encoder output and both masks into the decoder\n",
    "  - get next output token prediction from decoder along with attention\n",
    "  - add prediction to current output sentence prediction\n",
    "  - break if the prediction was an `<eos>` token\n",
    "- convert the output sentence from indexes to tokens\n",
    "- return the output sentence (with the `<sos>` token removed) and the attention from the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define a function that displays the attention over the source sentence for each step of the decoding. As this model has 8 heads our model we can view the attention for each of the heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
    "    \n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                           rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll get an example from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['I', 'am', 'from', 'ghana']\n",
      "predicted trg = ['nazali', 'moto', 'ya', 'ghana', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 8\n",
    "\n",
    "Sentence = \"I am from ghana\"\n",
    "src = tokenize_ln(Sentence)\n",
    "\n",
    "print(f'src = {src}')\n",
    "\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the attention from each head below. Each is certainly different, but it's difficult (perhaps impossible) to reason about what head has actually learned to pay attention to. Some heads pay full attention to \"eine\" when translating \"a\", some don't at all, and some do a little. They all seem to follow the similar \"downward staircase\" pattern and the attention when outputting the last two tokens is equally spread over the final two tokens in the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get an example the model has not been trained on from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['jehovah', 'is', 'the', 'name', 'of', 'god', 'as', 'revealed', 'in', 'the', 'bible', '.']\n",
      "trg = ['yehova', 'ezali', 'nkombo', 'ya', 'nzambe', 'na', 'biblia', '.']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 6\n",
    "\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model translates it by switching *is running* to just *running*, but it is an acceptable swap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['yehova', 'ezali', 'nkombo', 'ya', 'nzambe', 'oyo', 'biblia', 'elobeli', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, some heads pay full attention to \"ein\" whilst some pay no attention to it. Again, most of the heads seem to spread their attention over both the period and `<eos>` tokens in the source sentence when outputting the period and `<eos>` sentence in the predicted target sentence, though some seem to pay attention to tokens from near the start of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/anaconda3/envs/s2s/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/home/aims/anaconda3/envs/s2s/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1800 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll look at an example from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_idx = 10\n",
    "\n",
    "# src = vars(test_data.examples[example_idx])['src']\n",
    "# trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "# print(f'src = {src}')\n",
    "# print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decent translation with *young* being omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "# print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "Finally we calculate the BLEU score for the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a BLEU score of 35.08, which beats the 33.3 of the convolutional sequence-to-sequence model and 28.2 of the attention based RNN model. All this whilst having the least amount of parameters and the fastest training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 54.74\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(valid_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations for finishing these tutorials! I hope you've found them useful.\n",
    "\n",
    "If you find any mistakes or want to ask any questions about any of the code or explanations used, feel free to submit a GitHub issue and I will try to correct it ASAP. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
