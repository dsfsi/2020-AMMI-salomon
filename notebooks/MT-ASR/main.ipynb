{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchtext\n",
    "import librosa\n",
    "import spacy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import ipdb\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "import string\n",
    "import re\n",
    "import fnmatch\n",
    "import io\n",
    "import IPython\n",
    "import unicodedata\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from torchaudio.datasets.utils import walk_files\n",
    "# from __future__ import print_function\n",
    "from tempfile import NamedTemporaryFile\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torch.distributed import get_rank\n",
    "from torch.distributed import get_world_size\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "# from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check hours of recording (New testament)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aims/big_data/2020-AMMI-salomon/notebooks/MT-ASR\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We have a total of '18.781306666666644' hours in the repository '/home/aims/big_data/2020-AMMI-salomon/data/external/mass-dataset-mod/dataset/wav_verse/'\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_audio_length(repository):\n",
    "    \n",
    "    seconds = []\n",
    "    for count, filename in enumerate(os.listdir(repository)): \n",
    "        seconds.append(float(subprocess.check_output(['soxi -D \\\"%s\\\"' % repository+filename.strip()], shell=True)))\n",
    "    \n",
    "    hours = sum(seconds)/(60*60)\n",
    "    \n",
    "    # ipdb.set_trace()    \n",
    "    \n",
    "    return f\"We have a total of '{hours}' hours in the repository '{repository}'\"\n",
    "\n",
    "\n",
    "get_audio_length(\"/home/aims/big_data/2020-AMMI-salomon/data/external/mass-dataset-mod/dataset/wav_verse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    sound, _ = torchaudio.load(path, normalization=True)\n",
    "    sound = sound.numpy().T\n",
    "    \n",
    "#     print(len(sound.shape))\n",
    "    \n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    return sound\n",
    "\n",
    "\n",
    "def get_audio_length(path):\n",
    "    output = subprocess.check_output(\n",
    "        ['soxi -D \\\"%s\\\"' % path.strip()], shell=True)\n",
    "    return float(output)\n",
    "\n",
    "def audio_with_sox(path, sample_rate, start_time, end_time):\n",
    "    \"\"\"\n",
    "    crop and resample the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as tar_file:\n",
    "        tar_filename = tar_file.name\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} trim {} ={} >/dev/null 2>&1\".format(\n",
    "            path, sample_rate,\n",
    "            tar_filename, start_time,\n",
    "            end_time)\n",
    "        \n",
    "        os.system(sox_params)\n",
    "        y = load_audio(tar_filename)\n",
    "        return y\n",
    "\n",
    "def augment_audio_with_sox(path, sample_rate, tempo, gain):\n",
    "    \"\"\"\n",
    "    Changes tempo and gain of the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as augmented_file:\n",
    "        augmented_filename = augmented_file.name\n",
    "        sox_augment_params = [\"tempo\", \"{:.3f}\".format(\n",
    "            tempo), \"gain\", \"{:.3f}\".format(gain)]\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} {} >/dev/null 2>&1\".format(\n",
    "            path, sample_rate, augmented_filename, \" \".join(sox_augment_params))\n",
    "        os.system(sox_params)\n",
    "        y = load_audio(augmented_filename)\n",
    "        return y\n",
    "\n",
    "\n",
    "def load_randomly_augmented_audio(path, sample_rate=16000, tempo_range=(0.85, 1.15), \n",
    "                                  gain_range=(-6, 8)):\n",
    "    \"\"\"\n",
    "    Picks tempo and gain uniformly, applies it to the utterance by using sox utility.\n",
    "    Returns the augmented utterance.\n",
    "    \"\"\"\n",
    "    low_tempo, high_tempo = tempo_range\n",
    "    tempo_value = np.random.uniform(low=low_tempo, high=high_tempo)\n",
    "    low_gain, high_gain = gain_range\n",
    "    gain_value = np.random.uniform(low=low_gain, high=high_gain)\n",
    "    audio = augment_audio_with_sox(path=path, sample_rate=sample_rate,\n",
    "                                   tempo=tempo_value, gain=gain_value)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = {'hamming': scipy.signal.hamming, 'hann': scipy.signal.hann, \n",
    "           'blackman': scipy.signal.blackman, 'bartlett': scipy.signal.bartlett}\n",
    "\n",
    "\n",
    "class AudioParser(object):\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        \"\"\"\n",
    "        :param transcript_path: Path where transcript is stored from the manifest file\n",
    "        :return: Transcript in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        :param audio_path: Path where audio is stored from the manifest file\n",
    "        :return: Audio in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectrogramParser(AudioParser):\n",
    "    def __init__(self, audio_conf, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Parses audio file into spectrogram with optional normalization and various augmentations\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param normalize(default False):  Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment(default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        super(SpectrogramParser, self).__init__()\n",
    "        self.window_stride = audio_conf['window_stride']\n",
    "        self.window_size = audio_conf['window_size']\n",
    "        self.sample_rate = audio_conf['sample_rate']\n",
    "        self.window = windows.get(audio_conf['window'], windows['hamming'])\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        self.noiseInjector = NoiseInjection(audio_conf['noise_dir'], self.sample_rate,\n",
    "                                            audio_conf['noise_levels']) if audio_conf.get(\n",
    "            'noise_dir') is not None else None\n",
    "        self.noise_prob = audio_conf.get('noise_prob')\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "        if self.augment:\n",
    "            y = load_randomly_augmented_audio(audio_path, self.sample_rate)\n",
    "        else:\n",
    "            y = load_audio(audio_path)\n",
    "\n",
    "        if self.noiseInjector:\n",
    "            logging.info(\"inject noise\")\n",
    "            add_noise = np.random.binomial(1, self.noise_prob)\n",
    "            if add_noise:\n",
    "                y = self.noiseInjector.inject_noise(y)\n",
    "\n",
    "        n_fft = int(self.sample_rate * self.window_size)\n",
    "        win_length = n_fft\n",
    "        hop_length = int(self.sample_rate * self.window_stride)\n",
    "\n",
    "        # Short-time Fourier transform (STFT)\n",
    "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                         win_length=win_length, window=self.window)\n",
    "                \n",
    "        spect, phase = librosa.magphase(D)\n",
    "\n",
    "        # S = log(S+1) = log1p(S)\n",
    "        spect = np.log1p(spect)\n",
    "        spect = torch.FloatTensor(spect)\n",
    "\n",
    "        if self.normalize:\n",
    "            mean = spect.mean()\n",
    "            std = spect.std()\n",
    "            spect.add_(-mean)\n",
    "            spect.div_(std)\n",
    "\n",
    "        return spect\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectrogramDataset(Dataset, SpectrogramParser):\n",
    "    def __init__(self, audio_conf, manifest_filepath_list, \n",
    "                 label2id, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Dataset that loads tensors via a csv containing file paths to audio files and transcripts separated by\n",
    "        a comma. Each new line is a different sample. Example below:\n",
    "        /path/to/audio.wav,/path/to/audio.txt\n",
    "        ...\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param manifest_filepath: Path to manifest csv as describe above\n",
    "        :param labels: String containing all the possible characters to map to\n",
    "        :param normalize (default False): Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment (default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        self.max_size = 0\n",
    "        self.ids_list = []\n",
    "        for i in range(len(manifest_filepath_list)):\n",
    "            manifest_filepath = manifest_filepath_list[i]\n",
    "            with open(manifest_filepath) as f:\n",
    "                ids = f.readlines()\n",
    "\n",
    "            ids = [x.strip().split('\\t') for x in ids]\n",
    "            self.ids_list.append(ids)\n",
    "            self.max_size = max(len(ids), self.max_size)\n",
    "\n",
    "        self.manifest_filepath_list = manifest_filepath_list\n",
    "        self.label2id = label2id\n",
    "        super(SpectrogramDataset, self).__init__(\n",
    "            audio_conf, normalize, augment)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        random_id = random.randint(0, len(self.ids_list)-1)\n",
    "        ids = self.ids_list[random_id]\n",
    "        sample = ids[index % len(ids)]\n",
    "        audio_path, transcript, translation = sample[0], sample[1], sample[2]\n",
    "        \n",
    "        # get the audio using Short-time Fourier transform (STFT)\n",
    "        # librosa.stft up to \"args.src_max_len\"\n",
    "        spect = self.parse_audio(audio_path)[:,:args.src_max_len] \n",
    "        \n",
    "        transcript = self.parse_transcript(transcript)\n",
    "        translation = self.parse_translation(translation)\n",
    "        return spect, transcript, translation\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "#         with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "            # add start of sentense and end of sentence token\n",
    "        transcript = args.SOS_CHAR + transcript_path.lower() +\\\n",
    "                        args.EOS_CHAR\n",
    "            \n",
    "        # return all index exept 0 (false), in this case\n",
    "        # there will be no 0 in the list of index (due to filter)\n",
    "#         transcript = list(\n",
    "#             filter(None, [self.label2id.get(x) for x in list(transcript)]))\n",
    "        return transcript\n",
    "    \n",
    "    def parse_translation(self, translation_path):\n",
    "#         with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "            # add start of sentense and end of sentence token\n",
    "        translation = args.SOS_CHAR + translation_path.lower() +\\\n",
    "                        args.EOS_CHAR\n",
    "            \n",
    "        # return all index exept 0 (false), in this case\n",
    "        # there will be no 0 in the list of index (due to filter)\n",
    "#         transcript = list(\n",
    "#             filter(None, [self.label2id.get(x) for x in list(transcript)]))\n",
    "        return translation\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_size\n",
    "\n",
    "\n",
    "class NoiseInjection(object):\n",
    "    def __init__(self,\n",
    "                 path=None,\n",
    "                 sample_rate=16000,\n",
    "                 noise_levels=(0, 0.5)):\n",
    "        \"\"\"\n",
    "        Adds noise to an input signal with specific SNR. Higher the noise level, the more noise added.\n",
    "        Modified code from https://github.com/willfrey/audio/blob/master/torchaudio/transforms.py\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            print(\"Directory doesn't exist: {}\".format(path))\n",
    "            raise IOError\n",
    "        self.paths = path is not None and librosa.util.find_files(path)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.noise_levels = noise_levels\n",
    "\n",
    "    def inject_noise(self, data):\n",
    "        noise_path = np.random.choice(self.paths)\n",
    "        noise_level = np.random.uniform(*self.noise_levels)\n",
    "        return self.inject_noise_sample(data, noise_path, noise_level)\n",
    "\n",
    "    def inject_noise_sample(self, data, noise_path, noise_level):\n",
    "        noise_len = get_audio_length(noise_path)\n",
    "        data_len = len(data) / self.sample_rate\n",
    "        noise_start = np.random.rand() * (noise_len - data_len)\n",
    "        noise_end = noise_start + data_len\n",
    "        noise_dst = audio_with_sox(\n",
    "            noise_path, self.sample_rate, noise_start, noise_end)\n",
    "        assert len(data) == len(noise_dst)\n",
    "        noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)\n",
    "        data_energy = np.sqrt(data.dot(data) / data.size)\n",
    "        data += noise_level * noise_dst * data_energy / noise_energy\n",
    "        return data\n",
    "\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    def func(p):\n",
    "        return p[0].size(1)\n",
    "\n",
    "    def func_tgt_1(p):\n",
    "        return len(p[1])\n",
    "    \n",
    "    def func_tgt_2(p):\n",
    "        return len(p[2])\n",
    "\n",
    "    # descending sorted\n",
    "    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
    "\n",
    "    max_seq_len = max(batch, key=func)[0].size(1)\n",
    "    freq_size = max(batch, key=func)[0].size(0)\n",
    "    max_tgt_1_len = len(max(batch, key=func_tgt_1)[1])\n",
    "    max_tgt_2_len = len(max(batch, key=func_tgt_2)[1])\n",
    "    \n",
    "    inputs = torch.zeros(len(batch), 1, freq_size, max_seq_len)\n",
    "    input_sizes = torch.IntTensor(len(batch))\n",
    "    input_percentages = torch.FloatTensor(len(batch))\n",
    "\n",
    "    targets_1 = torch.zeros(len(batch), max_tgt_1_len).long()\n",
    "    target_1_sizes = torch.IntTensor(len(batch))\n",
    "    \n",
    "    targets_2 = torch.zeros(len(batch), max_tgt_2_len).long()\n",
    "    target_2_sizes = torch.IntTensor(len(batch))\n",
    "    \n",
    "    for x in range(len(batch)):\n",
    "        sample = batch[x]\n",
    "        input_data = sample[0]\n",
    "        target_1 = sample[1]\n",
    "        target_2 = sample[2]\n",
    "        \n",
    "        seq_length = input_data.size(1)\n",
    "        input_sizes[x] = seq_length\n",
    "        inputs[x][0].narrow(1, 0, seq_length).copy_(input_data)\n",
    "        input_percentages[x] = seq_length / float(max_seq_len)\n",
    "        target_1_sizes[x] = len(target_1)\n",
    "        targets_1[x][:len(target_1)] = torch.IntTensor(target_1)\n",
    "        \n",
    "        target_2_sizes[x] = len(target_2)\n",
    "        targets_2[x][:len(target_2)] = torch.IntTensor(target_2)\n",
    "\n",
    "    return inputs, targets, input_percentages, input_sizes, target_sizes\n",
    "\n",
    "\n",
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn\n",
    "\n",
    "\n",
    "class BucketingSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size=1):\n",
    "        \"\"\"\n",
    "        Samples batches assuming they are in order of size to batch similarly \n",
    "        sized samples together.\n",
    "        \"\"\"\n",
    "        super(BucketingSampler, self).__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        ids = list(range(0, len(data_source)))\n",
    "        self.bins = [ids[i:i + batch_size]\n",
    "                     for i in range(0, len(ids), batch_size)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for ids in self.bins:\n",
    "            np.random.shuffle(ids)\n",
    "            yield ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bins)\n",
    "\n",
    "    def shuffle(self, epoch):\n",
    "        np.random.shuffle(self.bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='ASR training')\n",
    "\n",
    "\n",
    "# Train\n",
    "parser.add_argument('--train-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--valid-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--test-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--lang-list', nargs='+', type=str)\n",
    "\n",
    "parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "parser.add_argument('--batch-size', default=20, type=int, help='Batch size for training') # 20\n",
    "parser.add_argument('--num-workers', default=4, type=int, \n",
    "                    help='Number of workers used in data-loading')\n",
    "\n",
    "parser.add_argument('--labels-path', default='labels.json', \n",
    "                    help='Contains all characters for transcription')\n",
    "parser.add_argument('--label-smoothing', default=0.0, type=float, help='Label smoothing')\n",
    "\n",
    "# Speech\n",
    "parser.add_argument('--window-size', default=.02, type=float, \n",
    "                    help='Window size for spectrogram in seconds')\n",
    "parser.add_argument('--window-stride', default=.01, type=float, \n",
    "                    help='Window stride for spectrogram in seconds')\n",
    "parser.add_argument('--window', default='hamming', \n",
    "                    help='Window type for spectrogram generation')\n",
    "\n",
    "parser.add_argument('--epochs', default=100, type=int, \n",
    "                    help='Number of training epochs') # 1000\n",
    "parser.add_argument('--cuda', dest='cuda', action='store_true', \n",
    "                    help='Use cuda to train model')\n",
    "\n",
    "parser.add_argument('--device-ids', default=None, nargs='+', type=int,\n",
    "                    help='If using cuda, sets the GPU devices for the process')\n",
    "parser.add_argument('--lr', '--learning-rate', default=3e-4, type=float, \n",
    "                    help='initial learning rate')\n",
    "\n",
    "parser.add_argument('--save-every', default=5, type=int, \n",
    "                    help='Save model every certain number of epochs')\n",
    "parser.add_argument('--save-folder', default='models/', \n",
    "                    help='Location to save epoch models')\n",
    "\n",
    "parser.add_argument('--emb_trg_sharing', action='store_true', \n",
    "                    help='Share embedding weight source and target')\n",
    "parser.add_argument('--feat_extractor', default='vgg_cnn', type=str, \n",
    "                    help='emb_cnn or vgg_cnn')\n",
    "\n",
    "parser.add_argument('--verbose', action='store_true', \n",
    "                    help='Verbose')\n",
    "\n",
    "parser.add_argument('--continue-from', default='', \n",
    "                    help='Continue from checkpoint model')\n",
    "parser.add_argument('--augment', dest='augment', action='store_true', \n",
    "                    help='Use random tempo and gain perturbations.')\n",
    "parser.add_argument('--noise-dir', default=None,\n",
    "                    help='Directory to inject noise into audio. If default, noise Inject not added')\n",
    "parser.add_argument('--noise-prob', default=0.4, \n",
    "                    help='Probability of noise being added per sample')\n",
    "parser.add_argument('--noise-min', default=0.0,\n",
    "                    help='Minimum noise level to sample from. (1.0 means all noise, not original signal)', type=float)\n",
    "parser.add_argument('--noise-max', default=0.5,\n",
    "                    help='Maximum noise levels to sample from. Maximum 1.0', type=float)\n",
    "\n",
    "# Transformer\n",
    "parser.add_argument('--num-layers', default=3, type=int, help='Number of layers')\n",
    "parser.add_argument('--num-heads', default=5, type=int, help='Number of heads')\n",
    "parser.add_argument('--dim-model', default=512, type=int, help='Model dimension')\n",
    "parser.add_argument('--dim-key', default=64, type=int, help='Key dimension')\n",
    "parser.add_argument('--dim-value', default=64, type=int, help='Value dimension')\n",
    "parser.add_argument('--dim-input', default=161, type=int, help='Input dimension')\n",
    "parser.add_argument('--dim-inner', default=1024, type=int, help='Inner dimension')\n",
    "parser.add_argument('--dim-emb', default=512, type=int, help='Embedding dimension')\n",
    "\n",
    "parser.add_argument('--src-max-len', default=4000, type=int, help='Source max length')\n",
    "parser.add_argument('--tgt-max-len', default=1000, type=int, help='Target max length')\n",
    "\n",
    "# Noam optimizer\n",
    "parser.add_argument('--warmup', default=4000, type=int, help='Warmup')\n",
    "parser.add_argument('--min-lr', default=1e-5, type=float, help='min lr')\n",
    "parser.add_argument('--k-lr', default=1, type=float, help='factor lr')\n",
    "\n",
    "# SGD optimizer\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--lr-anneal', default=1.1, type=float, help='lr anneal')\n",
    "\n",
    "# Decoder search\n",
    "parser.add_argument('--beam-search', action='store_true', help='Beam search')\n",
    "parser.add_argument('--beam-width', default=3, type=int, help='Beam size')\n",
    "parser.add_argument('--beam-nbest', default=5, type=int, help='Number of best sequences')\n",
    "parser.add_argument('--lm-rescoring', action='store_true', help='Rescore using LM')\n",
    "parser.add_argument('--lm-path', type=str, default=\"lm_model.pt\", help=\"Path to LM model\")\n",
    "parser.add_argument('--lm-weight', default=0.1, type=float, help='LM weight')\n",
    "parser.add_argument('--c-weight', default=0.1, type=float, help='Word count weight')\n",
    "parser.add_argument('--prob-weight', default=1.0, type=float, help='Probability E2E weight')\n",
    "\n",
    "# Loss\n",
    "parser.add_argument('--loss', type=str, default='ce', help='ce or ctc')\n",
    "parser.add_argument('--clip', action='store_true', help=\"clip\")\n",
    "parser.add_argument('--max-norm', default=400, type=float, help=\"max norm for clipping\")\n",
    "\n",
    "parser.add_argument('--dropout', default=0.1, type=float, help='Dropout')\n",
    "\n",
    "# Parallelize model\n",
    "parser.add_argument('--parallel', action='store_true', help='Parallelize the model')\n",
    "\n",
    "# shuffle\n",
    "parser.add_argument('--shuffle', action='store_true', help='Shuffle')\n",
    "\n",
    "# PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "parser.add_argument('--PAD_CHAR', default=\"¶\", type=str, help='PAD_CHAR')\n",
    "parser.add_argument('--SOS_CHAR', default=\"§\", type=str, help='SOS_CHAR')\n",
    "parser.add_argument('--EOS_CHAR', default=\"¤\", type=str, help='EOS_CHAR')\n",
    "parser.add_argument('--PAD_TOKEN', default=0, type=int, help='PAD_TOKEN')\n",
    "parser.add_argument('--SOS_TOKEN', default=1, type=int, help='SOS_TOKEN')\n",
    "parser.add_argument('--EOS_TOKEN', default=2, type=int, help='EOS_TOKEN')\n",
    "\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "torch.cuda.manual_seed_all(123456)\n",
    "\n",
    "# https://github.com/spyder-ide/spyder/issues/3883\n",
    "import sys\n",
    "sys.argv=['']; del sys \n",
    "\n",
    "args = parser.parse_args()\n",
    "USE_CUDA = args.cuda\n",
    "\n",
    "# PAD_TOKEN = 0\n",
    "# SOS_TOKEN = 1\n",
    "# EOS_TOKEN = 2\n",
    "\n",
    "# PAD_CHAR = \"¶\"\n",
    "# SOS_CHAR = \"§\"\n",
    "# EOS_CHAR = \"¤\"\n",
    "\n",
    "args.train_manifest_list = ['data/manifests/libri_train_manifest.csv']\n",
    "args.valid_manifest_list = ['data/manifests/libri_val_manifest.csv']\n",
    "args.test_manifest_list = ['data/manifests/libri_test_manifest.csv']\n",
    "\n",
    "args.batch_size = 6\n",
    "args.labels_path = 'data/labels/labels.json'\n",
    "args.lr = 1e-4\n",
    "args.name = 'libri_drop0.1_cnn_batch12_6_vgg_layer_notebook'\n",
    "args.save_folder = 'save/'\n",
    "args.save_every = 10\n",
    "args.feat_extractor = 'vgg_cnn'\n",
    "args.dropout = 0.1\n",
    "args.num_layers = 2\n",
    "args.num_heads = 6\n",
    "args.dim_model = 512\n",
    "args.dim_key = 64\n",
    "args.dim_value = 64\n",
    "args.dim_input = 161\n",
    "args.dim_inner = 2048\n",
    "args.dim_emb = 512\n",
    "args.shuffle = True\n",
    "args.min_lr = 1e-6\n",
    "args.k_lr = 1\n",
    "args.target_dir = '../../../../big_data/end2end-asr-pytorch/LibriSpeech_dataset/'\n",
    "args.sample_rate = 1600 #16000\n",
    "\n",
    "args.window_size = 0.02\n",
    "args.window_stride = 0.01 #0.01\n",
    "args.window = 'hamming'\n",
    "# args.noise_dir = \n",
    "args.noise_prob = 0.4\n",
    "args.noise_min = 0.0\n",
    "args.noise_max = 0.5\n",
    "args.cuda = torch.cuda.is_available()\n",
    "args.epochs = 1000\n",
    "args.continue_from = 'save/libri_drop0.1_cnn_batch12_6_vgg_layer_notebook/best_model.th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SpectrogramDataset(\n",
    "    audio_conf, manifest_filepath_list=args.train_manifest_list,\n",
    "    label2id=label2id, normalize=True, augment=args.augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# from torchaudio.datasets.utils import (\n",
    "#   download_url,\n",
    "#   extract_archive,\n",
    "#   walk_files,\n",
    "# )\n",
    "\n",
    "# URL = \"train-clean-100\"\n",
    "# FOLDER_IN_ARCHIVE = \"LibriSpeech\"\n",
    "# BASE_URL = \"https://dl.fbaipublicfiles.com/librispeech_100h_mp3/\"\n",
    "# _CHECKSUMS = {\n",
    "#   BASE_URL + \"dev-clean.tar.gz\":\n",
    "#   \"076916a8f9c61951c5d2e6efaa8d2188232fcf860eec8c074e46edf4fac9623e\",\n",
    "#   BASE_URL + \"test-clean.tar.gz\":\n",
    "#   \"3c171e2f1e377e4993c2dbe6bff3f01cd324c0ed462f4de6c78737402a7dbedd\",\n",
    "#   BASE_URL + \"train-clean-100.tar.gz\":\n",
    "#   \"7bfbefc680d25ba3a82798ce32c287ea0e82932af1b1f864fae71fb52d2f41f0\",\n",
    "# }\n",
    "\n",
    "# \"nameid_start_end\" format\n",
    "\n",
    "# 308660 number of audio file in train-segment\n",
    "\n",
    "# big_data/2020-AMMI-salomon/data/external/TED_Speech_Translation/train.en-fr\n",
    "\n",
    "def load_data_item(fileid: str, \n",
    "                   path: str, \n",
    "                   ext_audio: str, \n",
    "                   ext_txt: str) -> Tuple[Tensor, int, str, str, int, int, int]:\n",
    "    \n",
    "    temp, end = fileid.split(\"-\")\n",
    "    nameid, start = temp.split(\"_\")\n",
    "    \n",
    "#     file_text = speaker_id + \"_\" + chapter_id + ext_txt\n",
    "    file_text = os.path.join(path.split(\"train\")[0], \"train\"+ext_txt)\n",
    "    \n",
    "    fileid_audio = nameid + \"_\" + start + \"-\" + end\n",
    "    file_audio = fileid_audio + ext_audio\n",
    "    file_audio = os.path.join(path, file_audio)\n",
    "    # Load audio\n",
    "    waveform, sample_rate = torchaudio.load(file_audio)\n",
    "    \n",
    "    # Load text\n",
    "    with open(file_text) as ft:\n",
    "        for line in ft:\n",
    "            \n",
    "            fileid_text, trascription, translation = line.strip().split(\"\\t\")\n",
    "            fileid_text = fileid_text.split('.')[0]\n",
    "            ipdb.set_trace()\n",
    "            if fileid_audio == fileid_text:\n",
    "                break\n",
    "        else:\n",
    "          # Transcription or/and Translation not found\n",
    "          raise FileNotFoundError(\"Transcription or/and Translation not found for \" + fileid_audio)\n",
    "                \n",
    "    return (\n",
    "        waveform,\n",
    "        sample_rate,\n",
    "        trascription,\n",
    "        translation,\n",
    "        int(nameid),\n",
    "        int(start),\n",
    "        int(end)\n",
    "        )\n",
    "\n",
    "\n",
    "class TED_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Create a Dataset for TED dataset. Each item is a tuple of the form:\n",
    "    waveform, sample_rate, trascription, translation, nameid, start, end\n",
    "    \"\"\"\n",
    "    \n",
    "    _ext_txt = \".en-fr\"\n",
    "    _ext_audio = \".wav\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 folder: str) -> None:\n",
    "        \n",
    "        self._path = os.path.join(root, folder)\n",
    "        \n",
    "        walker = walk_files(\n",
    "          self._path, suffix=self._ext_audio, prefix=False, remove_suffix=True\n",
    "        )\n",
    "        \n",
    "        self._walker = list(walker)\n",
    "        \n",
    "    def __getitem__(self, n: int) -> Tuple[Tensor, int, str, int, int, int]:\n",
    "        fileid = self._walker[n]\n",
    "#         ipdb.set_trace()\n",
    "        return load_data_item(fileid, self._path, self._ext_audio, self._ext_txt)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._walker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TED_Dataset(\"../../data/external/TED_Speech_Translation/\", \"train-segment\")\n",
    "# train = TED_Dataset(\"big_data/2020-AMMI-salomon/data/external/TED_Speech_Translation/\",\n",
    "#                     \"train-segment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-97-83eb5df18d46>\u001b[0m(50)\u001b[0;36mload_data_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     49 \u001b[0;31m            \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 50 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mfileid_audio\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfileid_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m                \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  fileid_audio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'93268_0139110-0140457'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  fileid_text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'100230_0000442-0022222'\n",
      "--KeyboardInterrupt--\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-74b9cb5a2ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-97-83eb5df18d46>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mfileid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_walker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m#         ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_data_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ext_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ext_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-83eb5df18d46>\u001b[0m in \u001b[0;36mload_data_item\u001b[0;34m(fileid, path, ext_audio, ext_txt)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mfileid_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileid_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mfileid_audio\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfileid_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-83eb5df18d46>\u001b[0m in \u001b[0;36mload_data_item\u001b[0;34m(fileid, path, ext_audio, ext_txt)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mfileid_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileid_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mfileid_audio\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfileid_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(next(iter(train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,  b = torchaudio.load('../../data/external/TED_Speech_Translation/train-segment/93268_0139110-0140457.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims",
   "language": "python",
   "name": "aims"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
