2020-09-16 00:26:34,499 - {'sample_rate': 1600, 'window_size': 0.02, 'window_stride': 0.01, 'window': 'hamming', 'noise_dir': None, 'noise_prob': 0.4, 'noise_levels': (0.0, 0.5)}
2020-09-16 00:26:34,786 - Continue from checkpoint: save/libri_drop0.1_cnn_batch12_5_emb_cnn_layer_notebook/best_model.th
2020-09-16 00:26:41,547 - Transformer(
  (encoder): Encoder(
    (dropout): Dropout(p=0.1, inplace=False)
    (input_linear): Linear(in_features=512, out_features=512, bias=True)
    (layer_norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (positional_encoding): PositionalEncoding()
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForwardWithConv(
          (conv_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
          (conv_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForwardWithConv(
          (conv_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
          (conv_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): Decoder(
    (trg_embedding): Embedding(89117, 512, padding_idx=0)
    (positional_encoding): PositionalEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForwardWithConv(
          (conv_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
          (conv_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForwardWithConv(
          (conv_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
          (conv_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_linear): Linear(in_features=512, out_features=89117, bias=False)
  )
  (conv): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU()
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
)
2020-09-16 00:26:41,554 - Trainer is initialized
2020-09-16 00:26:41,555 - name libri_drop0.1_cnn_batch12_5_emb_cnn_layer_notebook
2020-09-16 00:26:41,555 - TRAIN
2020-09-16 00:49:24,618 - Epoch 5 || TRAIN LOSS: 2.3641 || CER: 70.09% || WER: 100.00% || LR: 0.0002772
2020-09-16 00:49:24,619 - VALID
2020-09-16 00:52:22,914 - VALID SET 5||LOSS: 2.4498 || CER: 69.20% || WER: 100.00%
2020-09-16 00:52:30,118 - SHUFFLE
2020-09-16 00:52:30,119 - TRAIN
