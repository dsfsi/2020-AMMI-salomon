2020-11-10 11:32:49,646 - {'sample_rate': 1600, 'window_size': 0.02, 'window_stride': 0.01, 'window': 'hamming', 'noise_dir': None, 'noise_prob': 0.4, 'noise_levels': (0.0, 0.5)}
2020-11-10 11:32:52,405 - Transformer(
  (encoder): Encoder(
    (dropout): Dropout(p=0.1, inplace=False)
    (input_linear): Linear(in_features=512, out_features=512, bias=True)
    (layer_norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (positional_encoding): PositionalEncoding()
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForwardWithConv(
          (conv_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
          (conv_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForwardWithConv(
          (conv_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
          (conv_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): Decoder(
    (trg_embedding): Embedding(43, 512, padding_idx=0)
    (positional_encoding): PositionalEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForwardWithConv(
          (conv_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
          (conv_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiHeadAttention(
          (query_linear): Linear(in_features=512, out_features=384, bias=True)
          (key_linear): Linear(in_features=512, out_features=384, bias=True)
          (value_linear): Linear(in_features=512, out_features=384, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (output_linear): Linear(in_features=384, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForwardWithConv(
          (conv_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
          (conv_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_linear): Linear(in_features=512, out_features=43, bias=False)
  )
  (conv): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU()
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
)
2020-11-10 11:32:52,418 - Trainer is initialized
2020-11-10 11:32:52,456 - name libri_drop0.1_cnn_batch12_6_vgg_layer_notebook
2020-11-10 11:32:52,457 - TRAIN
