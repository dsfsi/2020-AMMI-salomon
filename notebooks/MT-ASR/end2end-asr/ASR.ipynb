{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from trainer.asr.trainer import Trainer\n",
    "\n",
    "# from utils import constant\n",
    "# from utils.data_loader import SpectrogramDataset, AudioDataLoader, BucketingSampler\n",
    "# from utils.functions import save_model, load_model, init_transformer_model, init_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='ASR training')\n",
    "\n",
    "parser.add_argument('--model', default='TRFS', type=str, help=\"TRFS:transformer\")\n",
    "parser.add_argument('--name', default='model', help=\"Name of the model for saving\")\n",
    "\n",
    "# Train\n",
    "parser.add_argument('--train-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--valid-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--test-manifest-list', nargs='+', type=str)\n",
    "parser.add_argument('--lang-list', nargs='+', type=str)\n",
    "\n",
    "parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "parser.add_argument('--batch-size', default=20, type=int, help='Batch size for training') # 20\n",
    "parser.add_argument('--num-workers', default=4, type=int, \n",
    "                    help='Number of workers used in data-loading')\n",
    "\n",
    "parser.add_argument('--labels-path', default='labels.json', \n",
    "                    help='Contains all characters for transcription')\n",
    "parser.add_argument('--label-smoothing', default=0.0, type=float, help='Label smoothing')\n",
    "\n",
    "# Speech\n",
    "parser.add_argument('--window-size', default=.02, type=float, \n",
    "                    help='Window size for spectrogram in seconds')\n",
    "parser.add_argument('--window-stride', default=.01, type=float, \n",
    "                    help='Window stride for spectrogram in seconds')\n",
    "parser.add_argument('--window', default='hamming', \n",
    "                    help='Window type for spectrogram generation')\n",
    "\n",
    "parser.add_argument('--epochs', default=100, type=int, \n",
    "                    help='Number of training epochs') # 1000\n",
    "parser.add_argument('--cuda', dest='cuda', action='store_true', \n",
    "                    help='Use cuda to train model')\n",
    "\n",
    "parser.add_argument('--device-ids', default=None, nargs='+', type=int,\n",
    "                    help='If using cuda, sets the GPU devices for the process')\n",
    "parser.add_argument('--lr', '--learning-rate', default=3e-4, type=float, \n",
    "                    help='initial learning rate')\n",
    "\n",
    "parser.add_argument('--save-every', default=5, type=int, \n",
    "                    help='Save model every certain number of epochs')\n",
    "parser.add_argument('--save-folder', default='models/', \n",
    "                    help='Location to save epoch models')\n",
    "\n",
    "parser.add_argument('--emb_trg_sharing', action='store_true', \n",
    "                    help='Share embedding weight source and target')\n",
    "parser.add_argument('--feat_extractor', default='vgg_cnn', type=str, \n",
    "                    help='emb_cnn or vgg_cnn')\n",
    "\n",
    "parser.add_argument('--verbose', action='store_true', \n",
    "                    help='Verbose')\n",
    "\n",
    "parser.add_argument('--continue-from', default='', \n",
    "                    help='Continue from checkpoint model')\n",
    "parser.add_argument('--augment', dest='augment', action='store_true', \n",
    "                    help='Use random tempo and gain perturbations.')\n",
    "parser.add_argument('--noise-dir', default=None,\n",
    "                    help='Directory to inject noise into audio. If default, noise Inject not added')\n",
    "parser.add_argument('--noise-prob', default=0.4, \n",
    "                    help='Probability of noise being added per sample')\n",
    "parser.add_argument('--noise-min', default=0.0,\n",
    "                    help='Minimum noise level to sample from. (1.0 means all noise, not original signal)', type=float)\n",
    "parser.add_argument('--noise-max', default=0.5,\n",
    "                    help='Maximum noise levels to sample from. Maximum 1.0', type=float)\n",
    "\n",
    "# Transformer\n",
    "parser.add_argument('--num-layers', default=3, type=int, help='Number of layers')\n",
    "parser.add_argument('--num-heads', default=5, type=int, help='Number of heads')\n",
    "parser.add_argument('--dim-model', default=512, type=int, help='Model dimension')\n",
    "parser.add_argument('--dim-key', default=64, type=int, help='Key dimension')\n",
    "parser.add_argument('--dim-value', default=64, type=int, help='Value dimension')\n",
    "parser.add_argument('--dim-input', default=161, type=int, help='Input dimension')\n",
    "parser.add_argument('--dim-inner', default=1024, type=int, help='Inner dimension')\n",
    "parser.add_argument('--dim-emb', default=512, type=int, help='Embedding dimension')\n",
    "\n",
    "parser.add_argument('--src-max-len', default=4000, type=int, help='Source max length')\n",
    "parser.add_argument('--tgt-max-len', default=1000, type=int, help='Target max length')\n",
    "\n",
    "# Noam optimizer\n",
    "parser.add_argument('--warmup', default=4000, type=int, help='Warmup')\n",
    "parser.add_argument('--min-lr', default=1e-5, type=float, help='min lr')\n",
    "parser.add_argument('--k-lr', default=1, type=float, help='factor lr')\n",
    "\n",
    "# SGD optimizer\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--lr-anneal', default=1.1, type=float, help='lr anneal')\n",
    "\n",
    "# Decoder search\n",
    "parser.add_argument('--beam-search', action='store_true', help='Beam search')\n",
    "parser.add_argument('--beam-width', default=3, type=int, help='Beam size')\n",
    "parser.add_argument('--beam-nbest', default=5, type=int, help='Number of best sequences')\n",
    "parser.add_argument('--lm-rescoring', action='store_true', help='Rescore using LM')\n",
    "parser.add_argument('--lm-path', type=str, default=\"lm_model.pt\", help=\"Path to LM model\")\n",
    "parser.add_argument('--lm-weight', default=0.1, type=float, help='LM weight')\n",
    "parser.add_argument('--c-weight', default=0.1, type=float, help='Word count weight')\n",
    "parser.add_argument('--prob-weight', default=1.0, type=float, help='Probability E2E weight')\n",
    "\n",
    "# Loss\n",
    "parser.add_argument('--loss', type=str, default='ce', help='ce or ctc')\n",
    "parser.add_argument('--clip', action='store_true', help=\"clip\")\n",
    "parser.add_argument('--max-norm', default=400, type=float, help=\"max norm for clipping\")\n",
    "\n",
    "parser.add_argument('--dropout', default=0.1, type=float, help='Dropout')\n",
    "\n",
    "# Parallelize model\n",
    "parser.add_argument('--parallel', action='store_true', help='Parallelize the model')\n",
    "\n",
    "# shuffle\n",
    "parser.add_argument('--shuffle', action='store_true', help='Shuffle')\n",
    "\n",
    "# PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "parser.add_argument('--PAD_CHAR', default=\"¶\", type=str, help='PAD_CHAR')\n",
    "parser.add_argument('--SOS_CHAR', default=\"§\", type=str, help='SOS_CHAR')\n",
    "parser.add_argument('--EOS_CHAR', default=\"¤\", type=str, help='EOS_CHAR')\n",
    "parser.add_argument('--PAD_TOKEN', default=0, type=int, help='PAD_TOKEN')\n",
    "parser.add_argument('--SOS_TOKEN', default=1, type=int, help='SOS_TOKEN')\n",
    "parser.add_argument('--EOS_TOKEN', default=2, type=int, help='EOS_TOKEN')\n",
    "\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "torch.cuda.manual_seed_all(123456)\n",
    "\n",
    "# https://github.com/spyder-ide/spyder/issues/3883\n",
    "import sys; \n",
    "sys.argv=['']; del sys \n",
    "\n",
    "args = parser.parse_args()\n",
    "USE_CUDA = args.cuda\n",
    "\n",
    "# PAD_TOKEN = 0\n",
    "# SOS_TOKEN = 1\n",
    "# EOS_TOKEN = 2\n",
    "\n",
    "# PAD_CHAR = \"¶\"\n",
    "# SOS_CHAR = \"§\"\n",
    "# EOS_CHAR = \"¤\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant = args\n",
    "# constant.SOS_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torchaudio\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "def load_audio(path):\n",
    "    sound, _ = torchaudio.load(path, normalization=True)\n",
    "    sound = sound.numpy().T\n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    return sound\n",
    "\n",
    "\n",
    "def get_audio_length(path):\n",
    "    output = subprocess.check_output(\n",
    "        ['soxi -D \\\"%s\\\"' % path.strip()], shell=True)\n",
    "    return float(output)\n",
    "\n",
    "def audio_with_sox(path, sample_rate, start_time, end_time):\n",
    "    \"\"\"\n",
    "    crop and resample the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as tar_file:\n",
    "        tar_filename = tar_file.name\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} trim {} ={} >/dev/null 2>&1\".format(\n",
    "            path, sample_rate,\n",
    "            tar_filename, start_time,\n",
    "            end_time)\n",
    "        \n",
    "        os.system(sox_params)\n",
    "        y = load_audio(tar_filename)\n",
    "        return y\n",
    "\n",
    "def augment_audio_with_sox(path, sample_rate, tempo, gain):\n",
    "    \"\"\"\n",
    "    Changes tempo and gain of the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as augmented_file:\n",
    "        augmented_filename = augmented_file.name\n",
    "        sox_augment_params = [\"tempo\", \"{:.3f}\".format(\n",
    "            tempo), \"gain\", \"{:.3f}\".format(gain)]\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} {} >/dev/null 2>&1\".format(\n",
    "            path, sample_rate, augmented_filename, \" \".join(sox_augment_params))\n",
    "        os.system(sox_params)\n",
    "        y = load_audio(augmented_filename)\n",
    "        return y\n",
    "\n",
    "\n",
    "def load_randomly_augmented_audio(path, sample_rate=16000, tempo_range=(0.85, 1.15), \n",
    "                                  gain_range=(-6, 8)):\n",
    "    \"\"\"\n",
    "    Picks tempo and gain uniformly, applies it to the utterance by using sox utility.\n",
    "    Returns the augmented utterance.\n",
    "    \"\"\"\n",
    "    low_tempo, high_tempo = tempo_range\n",
    "    tempo_value = np.random.uniform(low=low_tempo, high=high_tempo)\n",
    "    low_gain, high_gain = gain_range\n",
    "    gain_value = np.random.uniform(low=low_gain, high=high_gain)\n",
    "    audio = augment_audio_with_sox(path=path, sample_rate=sample_rate,\n",
    "                                   tempo=tempo_value, gain=gain_value)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AWD LSTM \n",
    "\n",
    "# from awd_lstm_lm.splitcross import SplitCrossEntropyLoss\n",
    "\n",
    "\n",
    "# def calculate_lm_score(seq, lm, id2label):\n",
    "#     \"\"\"\n",
    "#     seq: (1, seq_len)\n",
    "#     id2label: map\n",
    "#     \"\"\"\n",
    "#     # print(seq)\n",
    "#     # for char in seq[0]:\n",
    "#     #     print(char.item())\n",
    "#     seq_str = \"\".join(id2label[char.item()] for char in seq[0]).replace(constant.PAD_CHAR,\"\").replace(constant.SOS_CHAR,\"\").replace(constant.EOS_CHAR,\"\")\n",
    "#     seq_str = seq_str.replace(\"  \", \" \")\n",
    "#     if seq_str == \"\":\n",
    "#         return -999\n",
    "        \n",
    "#     score = lm.evaluate(seq_str)\n",
    "#     # print(seq_str)\n",
    "#     return -1 * (score / len(seq_str.split())), len(seq_str.split())\n",
    "\n",
    "# class LM(object):\n",
    "#     def __init__(self, model_path):\n",
    "#         self.model_path = model_path\n",
    "#         print(\"load model path:\", self.model_path)\n",
    "\n",
    "#         checkpoint = torch.load(model_path)\n",
    "#         self.word2idx = checkpoint[\"word2idx\"]\n",
    "#         self.idx2word = checkpoint[\"idx2word\"]\n",
    "#         ntokens = checkpoint[\"ntoken\"]\n",
    "#         ninp = checkpoint[\"ninp\"]\n",
    "#         nhid = checkpoint[\"nhid\"]\n",
    "#         nlayers = checkpoint[\"nlayers\"]\n",
    "#         dropout = checkpoint[\"dropout\"]\n",
    "#         dropouth = checkpoint[\"dropouth\"]\n",
    "#         dropouti = checkpoint[\"dropouti\"]\n",
    "#         dropoute = checkpoint[\"dropoute\"]\n",
    "#         tie_weights = checkpoint[\"tie_weights\"]\n",
    "\n",
    "#         self.model = RNNModel(\"LSTM\", ntoken=ntokens, ninp=ninp, nhid=nhid, nlayers=nlayers, dropout=dropout, dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=1, tie_weights=tie_weights)\n",
    "#         self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "#         if constant.args.cuda:\n",
    "#             self.model = self.model.cuda()\n",
    "\n",
    "#         splits = []\n",
    "#         if ntokens > 500000:\n",
    "#             # One Billion\n",
    "#             # This produces fairly even matrix mults for the buckets:\n",
    "#             # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n",
    "#             splits = [4200, 35000, 180000]\n",
    "#         elif ntokens > 75000:\n",
    "#             # WikiText-103\n",
    "#             splits = [2800, 20000, 76000]\n",
    "#         print('Using', splits)\n",
    "#         self.criterion = SplitCrossEntropyLoss(ninp, splits=splits, verbose=False)\n",
    "\n",
    "#     def batchify(self, data, bsz, cuda):\n",
    "#         # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "#         nbatch = data.size(0) // bsz\n",
    "#         # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "#         data = data.narrow(0, 0, nbatch * bsz)\n",
    "#         # Evenly divide the data across the bsz batches.\n",
    "#         data = data.view(bsz, -1).t().contiguous()\n",
    "#         if cuda:\n",
    "#             data = data.cuda()\n",
    "#         return data\n",
    "\n",
    "#     def seq_to_tensor(self, seq):\n",
    "#         words = seq.split() + ['<eos>']\n",
    "\n",
    "#         ids = torch.LongTensor(len(words))\n",
    "#         token = 0\n",
    "#         for word in words:\n",
    "#             if word in self.word2idx:\n",
    "#                 ids[token] = self.word2idx[word]\n",
    "#             else:\n",
    "#                 ids[token] = self.word2idx['<oov>']\n",
    "#             token += 1\n",
    "#         return ids\n",
    "\n",
    "#     def get_batch(self, source, i, bptt, seq_len=None, evaluation=False):\n",
    "#         seq_len = min(seq_len if seq_len else bptt, len(source) - 1 - i)\n",
    "#         data = source[i:i+seq_len]\n",
    "#         target = source[i+1:i+1+seq_len].view(-1)\n",
    "#         return data, target\n",
    "\n",
    "#     def evaluate(self, seq):\n",
    "#         \"\"\"\n",
    "#         batch_size = 1\n",
    "#         \"\"\"\n",
    "#         data_source = self.batchify(self.seq_to_tensor(seq), 1, constant.args.cuda)\n",
    "#         self.model.eval()\n",
    "\n",
    "#         total_loss = 0\n",
    "#         ntokens = len(self.word2idx)\n",
    "#         hidden = self.model.init_hidden(1)\n",
    "#         data, targets = self.get_batch(\n",
    "#             data_source, 0, data_source.size(0), evaluation=True)\n",
    "#         output, hidden = self.model(data, hidden)\n",
    "\n",
    "#         # calculate probability\n",
    "#         # print(output.size()) # seq_len, vocab\n",
    "\n",
    "#         total_loss = len(data) * self.criterion(self.model.decoder.weight, self.model.decoder.bias, output, targets).data\n",
    "#         hidden = self.repackage_hidden(hidden)\n",
    "#         return total_loss\n",
    "\n",
    "#     def repackage_hidden(self, h):\n",
    "#         \"\"\"Wraps hidden states in new Tensors,\n",
    "#         to detach them from their history.\"\"\"\n",
    "#         if isinstance(h, torch.Tensor):\n",
    "#             return h.detach()\n",
    "#         else:\n",
    "#             return tuple(self.repackage_hidden(v) for v in h)\n",
    "\n",
    "# from awd_lstm_lm.embed_regularize import embedded_dropout\n",
    "# from awd_lstm_lm.locked_dropout import LockedDropout\n",
    "# from awd_lstm_lm.weight_drop import WeightDrop\n",
    "\n",
    "# class RNNModel(nn.Module):\n",
    "#     \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "#     def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, dropouth=0.5, dropouti=0.5, dropoute=0.1, wdrop=0, tie_weights=False):\n",
    "#         super(RNNModel, self).__init__()\n",
    "#         self.lockdrop = LockedDropout()\n",
    "#         self.idrop = nn.Dropout(dropouti)\n",
    "#         self.hdrop = nn.Dropout(dropouth)\n",
    "#         self.drop = nn.Dropout(dropout)\n",
    "#         self.encoder = nn.Embedding(ntoken, ninp)\n",
    "#         assert rnn_type in ['LSTM', 'QRNN', 'GRU'], 'RNN type is not supported'\n",
    "#         if rnn_type == 'LSTM':\n",
    "#             self.rnns = [torch.nn.LSTM(ninp if l == 0 else nhid, nhid if l != nlayers - 1 else (ninp if tie_weights else nhid), 1, dropout=0) for l in range(nlayers)]\n",
    "#             if wdrop:\n",
    "#                 self.rnns = [WeightDrop(rnn, ['weight_hh_l0'], dropout=wdrop) for rnn in self.rnns]\n",
    "#         if rnn_type == 'GRU':\n",
    "#             self.rnns = [torch.nn.GRU(ninp if l == 0 else nhid, nhid if l != nlayers - 1 else ninp, 1, dropout=0) for l in range(nlayers)]\n",
    "#             if wdrop:\n",
    "#                 self.rnns = [WeightDrop(rnn, ['weight_hh_l0'], dropout=wdrop) for rnn in self.rnns]\n",
    "#         elif rnn_type == 'QRNN':\n",
    "#             from torchqrnn import QRNNLayer\n",
    "#             self.rnns = [QRNNLayer(input_size=ninp if l == 0 else nhid, hidden_size=nhid if l != nlayers - 1 else (ninp if tie_weights else nhid), save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(nlayers)]\n",
    "#             for rnn in self.rnns:\n",
    "#                 rnn.linear = WeightDrop(rnn.linear, ['weight'], dropout=wdrop)\n",
    "#         print(self.rnns)\n",
    "#         self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "#         self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "#         # Optionally tie weights as in:\n",
    "#         # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "#         # https://arxiv.org/abs/1608.05859\n",
    "#         # and\n",
    "#         # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "#         # https://arxiv.org/abs/1611.01462\n",
    "#         if tie_weights:\n",
    "#             #if nhid != ninp:\n",
    "#             #    raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "#             self.decoder.weight = self.encoder.weight\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "#         self.rnn_type = rnn_type\n",
    "#         self.ninp = ninp\n",
    "#         self.nhid = nhid\n",
    "#         self.nlayers = nlayers\n",
    "#         self.dropout = dropout\n",
    "#         self.dropouti = dropouti\n",
    "#         self.dropouth = dropouth\n",
    "#         self.dropoute = dropoute\n",
    "#         self.tie_weights = tie_weights\n",
    "\n",
    "#     def reset(self):\n",
    "#         if self.rnn_type == 'QRNN': [r.reset() for r in self.rnns]\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.1\n",
    "#         self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.decoder.bias.data.fill_(0)\n",
    "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "#     def forward(self, input, hidden, return_h=False):\n",
    "#         emb = embedded_dropout(self.encoder, input, dropout=self.dropoute if self.training else 0)\n",
    "#         #emb = self.idrop(emb)\n",
    "\n",
    "#         emb = self.lockdrop(emb, self.dropouti)\n",
    "\n",
    "#         raw_output = emb\n",
    "#         new_hidden = []\n",
    "#         #raw_output, hidden = self.rnn(emb, hidden)\n",
    "#         raw_outputs = []\n",
    "#         outputs = []\n",
    "#         for l, rnn in enumerate(self.rnns):\n",
    "#             current_input = raw_output\n",
    "#             raw_output, new_h = rnn(raw_output, hidden[l])\n",
    "#             new_hidden.append(new_h)\n",
    "#             raw_outputs.append(raw_output)\n",
    "#             if l != self.nlayers - 1:\n",
    "#                 #self.hdrop(raw_output)\n",
    "#                 raw_output = self.lockdrop(raw_output, self.dropouth)\n",
    "#                 outputs.append(raw_output)\n",
    "#         hidden = new_hidden\n",
    "\n",
    "#         output = self.lockdrop(raw_output, self.dropout)\n",
    "#         outputs.append(output)\n",
    "\n",
    "#         result = output.view(output.size(0)*output.size(1), output.size(2))\n",
    "#         if return_h:\n",
    "#             return result, hidden, raw_outputs, outputs\n",
    "#         return result, hidden\n",
    "\n",
    "#     def init_hidden(self, bsz):\n",
    "#         weight = next(self.parameters()).data\n",
    "#         if self.rnn_type == 'LSTM':\n",
    "#             return [(weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_(),\n",
    "#                     weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_())\n",
    "#                     for l in range(self.nlayers)]\n",
    "#         elif self.rnn_type == 'QRNN' or self.rnn_type == 'GRU':\n",
    "#             return [weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_()\n",
    "#                     for l in range(self.nlayers)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.signal\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from torch.distributed import get_rank\n",
    "from torch.distributed import get_world_size\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "import logging\n",
    "\n",
    "windows = {'hamming': scipy.signal.hamming, 'hann': scipy.signal.hann, \n",
    "           'blackman': scipy.signal.blackman, 'bartlett': scipy.signal.bartlett}\n",
    "\n",
    "\n",
    "class AudioParser(object):\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        \"\"\"\n",
    "        :param transcript_path: Path where transcript is stored from the manifest file\n",
    "        :return: Transcript in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        :param audio_path: Path where audio is stored from the manifest file\n",
    "        :return: Audio in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectrogramParser(AudioParser):\n",
    "    def __init__(self, audio_conf, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Parses audio file into spectrogram with optional normalization and various augmentations\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param normalize(default False):  Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment(default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        super(SpectrogramParser, self).__init__()\n",
    "        self.window_stride = audio_conf['window_stride']\n",
    "        self.window_size = audio_conf['window_size']\n",
    "        self.sample_rate = audio_conf['sample_rate']\n",
    "        self.window = windows.get(audio_conf['window'], windows['hamming'])\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        self.noiseInjector = NoiseInjection(audio_conf['noise_dir'], self.sample_rate,\n",
    "                                            audio_conf['noise_levels']) if audio_conf.get(\n",
    "            'noise_dir') is not None else None\n",
    "        self.noise_prob = audio_conf.get('noise_prob')\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "        if self.augment:\n",
    "            y = load_randomly_augmented_audio(audio_path, self.sample_rate)\n",
    "        else:\n",
    "            y = load_audio(audio_path)\n",
    "\n",
    "        if self.noiseInjector:\n",
    "            logging.info(\"inject noise\")\n",
    "            add_noise = np.random.binomial(1, self.noise_prob)\n",
    "            if add_noise:\n",
    "                y = self.noiseInjector.inject_noise(y)\n",
    "\n",
    "        n_fft = int(self.sample_rate * self.window_size)\n",
    "        win_length = n_fft\n",
    "        hop_length = int(self.sample_rate * self.window_stride)\n",
    "\n",
    "        # Short-time Fourier transform (STFT)\n",
    "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                         win_length=win_length, window=self.window)\n",
    "        \n",
    "#         print(D)\n",
    "        \n",
    "        spect, phase = librosa.magphase(D)\n",
    "\n",
    "        # S = log(S+1)\n",
    "        spect = np.log1p(spect)\n",
    "        spect = torch.FloatTensor(spect)\n",
    "\n",
    "        if self.normalize:\n",
    "            mean = spect.mean()\n",
    "            std = spect.std()\n",
    "            spect.add_(-mean)\n",
    "            spect.div_(std)\n",
    "\n",
    "        return spect\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectrogramDataset(Dataset, SpectrogramParser):\n",
    "    def __init__(self, audio_conf, manifest_filepath_list, \n",
    "                 label2id, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Dataset that loads tensors via a csv containing file paths to audio files and transcripts separated by\n",
    "        a comma. Each new line is a different sample. Example below:\n",
    "        /path/to/audio.wav,/path/to/audio.txt\n",
    "        ...\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param manifest_filepath: Path to manifest csv as describe above\n",
    "        :param labels: String containing all the possible characters to map to\n",
    "        :param normalize: Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment(default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        self.max_size = 0\n",
    "        self.ids_list = []\n",
    "        for i in range(len(manifest_filepath_list)):\n",
    "            manifest_filepath = manifest_filepath_list[i]\n",
    "            with open(manifest_filepath) as f:\n",
    "                ids = f.readlines()\n",
    "\n",
    "            ids = [x.strip().split(',') for x in ids]\n",
    "            self.ids_list.append(ids)\n",
    "            self.max_size = max(len(ids), self.max_size)\n",
    "\n",
    "        self.manifest_filepath_list = manifest_filepath_list\n",
    "        self.label2id = label2id\n",
    "        super(SpectrogramDataset, self).__init__(\n",
    "            audio_conf, normalize, augment)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        random_id = random.randint(0, len(self.ids_list)-1)\n",
    "        ids = self.ids_list[random_id]\n",
    "        sample = ids[index % len(ids)]\n",
    "        audio_path, transcript_path = sample[0], sample[1]\n",
    "        spect = self.parse_audio(audio_path)[:,:constant.args.src_max_len]\n",
    "        transcript = self.parse_transcript(transcript_path)\n",
    "        return spect, transcript\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "            transcript = constant.SOS_CHAR + transcript_file.read().replace('\\n', '').lower() + constant.EOS_CHAR\n",
    "\n",
    "        transcript = list(\n",
    "            filter(None, [self.label2id.get(x) for x in list(transcript)]))\n",
    "        return transcript\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_size\n",
    "\n",
    "\n",
    "class NoiseInjection(object):\n",
    "    def __init__(self,\n",
    "                 path=None,\n",
    "                 sample_rate=16000,\n",
    "                 noise_levels=(0, 0.5)):\n",
    "        \"\"\"\n",
    "        Adds noise to an input signal with specific SNR. Higher the noise level, the more noise added.\n",
    "        Modified code from https://github.com/willfrey/audio/blob/master/torchaudio/transforms.py\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            print(\"Directory doesn't exist: {}\".format(path))\n",
    "            raise IOError\n",
    "        self.paths = path is not None and librosa.util.find_files(path)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.noise_levels = noise_levels\n",
    "\n",
    "    def inject_noise(self, data):\n",
    "        noise_path = np.random.choice(self.paths)\n",
    "        noise_level = np.random.uniform(*self.noise_levels)\n",
    "        return self.inject_noise_sample(data, noise_path, noise_level)\n",
    "\n",
    "    def inject_noise_sample(self, data, noise_path, noise_level):\n",
    "        noise_len = get_audio_length(noise_path)\n",
    "        data_len = len(data) / self.sample_rate\n",
    "        noise_start = np.random.rand() * (noise_len - data_len)\n",
    "        noise_end = noise_start + data_len\n",
    "        noise_dst = audio_with_sox(\n",
    "            noise_path, self.sample_rate, noise_start, noise_end)\n",
    "        assert len(data) == len(noise_dst)\n",
    "        noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)\n",
    "        data_energy = np.sqrt(data.dot(data) / data.size)\n",
    "        data += noise_level * noise_dst * data_energy / noise_energy\n",
    "        return data\n",
    "\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    def func(p):\n",
    "        return p[0].size(1)\n",
    "\n",
    "    def func_tgt(p):\n",
    "        return len(p[1])\n",
    "\n",
    "    # descending sorted\n",
    "    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
    "\n",
    "    max_seq_len = max(batch, key=func)[0].size(1)\n",
    "    freq_size = max(batch, key=func)[0].size(0)\n",
    "    max_tgt_len = len(max(batch, key=func_tgt)[1])\n",
    "\n",
    "    inputs = torch.zeros(len(batch), 1, freq_size, max_seq_len)\n",
    "    input_sizes = torch.IntTensor(len(batch))\n",
    "    input_percentages = torch.FloatTensor(len(batch))\n",
    "\n",
    "    targets = torch.zeros(len(batch), max_tgt_len).long()\n",
    "    target_sizes = torch.IntTensor(len(batch))\n",
    "\n",
    "    for x in range(len(batch)):\n",
    "        sample = batch[x]\n",
    "        input_data = sample[0]\n",
    "        target = sample[1]\n",
    "        seq_length = input_data.size(1)\n",
    "        input_sizes[x] = seq_length\n",
    "        inputs[x][0].narrow(1, 0, seq_length).copy_(input_data)\n",
    "        input_percentages[x] = seq_length / float(max_seq_len)\n",
    "        target_sizes[x] = len(target)\n",
    "        targets[x][:len(target)] = torch.IntTensor(target)\n",
    "\n",
    "    return inputs, targets, input_percentages, input_sizes, target_sizes\n",
    "\n",
    "\n",
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn\n",
    "\n",
    "\n",
    "class BucketingSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size=1):\n",
    "        \"\"\"\n",
    "        Samples batches assuming they are in order of size to batch similarly sized samples together.\n",
    "        \"\"\"\n",
    "        super(BucketingSampler, self).__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        ids = list(range(0, len(data_source)))\n",
    "        self.bins = [ids[i:i + batch_size]\n",
    "                     for i in range(0, len(ids), batch_size)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for ids in self.bins:\n",
    "            np.random.shuffle(ids)\n",
    "            yield ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bins)\n",
    "\n",
    "    def shuffle(self, epoch):\n",
    "        np.random.shuffle(self.bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, path, label2id, id2label):\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.texts, self.ids = self.read_manifest(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ids[index]\n",
    "        \n",
    "    def read_manifest(self, path):\n",
    "        \"\"\"Read manifest\"\"\"\n",
    "        texts, ids = [], []\n",
    "        with open(path, \"r\") as f:\n",
    "            for line in f:\n",
    "                _, text_path = line.replace(\"\\n\", \"\").split(\",\")\n",
    "                with open(text_path, \"r\") as text_file:\n",
    "                    for l in text_file:\n",
    "                        texts.append(l.lower().replace(\"\\n\", \"\"))\n",
    "\n",
    "            for text in texts:\n",
    "                for char in text:\n",
    "                    if char not in self.label2id:\n",
    "                        print(\">\", char)\n",
    "                ids.append(list(filter(None, [self.label2id.get(x) for x in list(text)])))\n",
    "        return texts, ids    \n",
    "\n",
    "def _collate_fn(batch):\n",
    "    def func(p):\n",
    "        return len(p)\n",
    "\n",
    "    # print(\">\", batch)\n",
    "    batch = sorted(batch, key=lambda x: len(x), reverse=True)\n",
    "    # print(\">>\", batch)\n",
    "\n",
    "    max_seq_len = len(max(batch, key=func))\n",
    "    # print(\"max_seq_len:\", max_seq_len)\n",
    "    inputs = torch.zeros(len(batch), max_seq_len).long()\n",
    "    input_sizes = torch.IntTensor(len(batch))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        sample = batch[i]\n",
    "        inputs[i][:len(sample)] = torch.IntTensor(sample)\n",
    "\n",
    "        seq_length = len(sample)\n",
    "        input_sizes[i] = seq_length\n",
    "\n",
    "    return inputs, input_sizes\n",
    "\n",
    "class LMDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LMDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, opt, metrics, label2id, id2label, best_model=False):\n",
    "    \"\"\"\n",
    "    Saving model, TODO adding history\n",
    "    \"\"\"\n",
    "    if best_model:\n",
    "        save_path = \"{}/{}/best_model.th\".format(\n",
    "            constant.args.save_folder, constant.args.name)\n",
    "    else:\n",
    "        save_path = \"{}/{}/epoch_{}.th\".format(constant.args.save_folder,\n",
    "                                               constant.args.name, epoch)\n",
    "\n",
    "    if not os.path.exists(constant.args.save_folder + \"/\" + constant.args.name):\n",
    "        os.makedirs(constant.args.save_folder + \"/\" + constant.args.name)\n",
    "\n",
    "    print(\"SAVE MODEL to\", save_path)\n",
    "    if constant.args.loss == \"ce\":\n",
    "        args = {\n",
    "            'label2id': label2id,\n",
    "            'id2label': id2label,\n",
    "            'args': constant.args,\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.optimizer.state_dict(),\n",
    "            'optimizer_params': {\n",
    "                '_step': opt._step,\n",
    "                '_rate': opt._rate,\n",
    "                'warmup': opt.warmup,\n",
    "                'factor': opt.factor,\n",
    "                'model_size': opt.model_size\n",
    "            },\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    elif constant.args.loss == \"ctc\":\n",
    "        args = {\n",
    "            'label2id': label2id,\n",
    "            'id2label': id2label,\n",
    "            'args': constant.args,\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.optimizer.state_dict(),\n",
    "            'optimizer_params': {\n",
    "                'lr': opt.lr,\n",
    "                'lr_anneal': opt.lr_anneal\n",
    "            },\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    else:\n",
    "        print(\"Loss is not defined\")\n",
    "    torch.save(args, save_path)\n",
    "\n",
    "\n",
    "def load_model(load_path):\n",
    "    \"\"\"\n",
    "    Loading model\n",
    "    args:\n",
    "        load_path: string\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(load_path)\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metrics']\n",
    "    if 'args' in checkpoint:\n",
    "        args = checkpoint['args']\n",
    "\n",
    "    label2id = checkpoint['label2id']\n",
    "    id2label = checkpoint['id2label']\n",
    "\n",
    "    model = init_transformer_model(args, label2id, id2label)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if args.cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    opt = init_optimizer(args, model)\n",
    "    if opt is not None:\n",
    "        opt.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if constant.args.loss == \"ce\":\n",
    "            opt._step = checkpoint['optimizer_params']['_step']\n",
    "            opt._rate = checkpoint['optimizer_params']['_rate']\n",
    "            opt.warmup = checkpoint['optimizer_params']['warmup']\n",
    "            opt.factor = checkpoint['optimizer_params']['factor']\n",
    "            opt.model_size = checkpoint['optimizer_params']['model_size']\n",
    "        elif constant.args.loss == \"ctc\":\n",
    "            opt.lr = checkpoint['optimizer_params']['lr']\n",
    "            opt.lr_anneal = checkpoint['optimizer_params']['lr_anneal']\n",
    "        else:\n",
    "            print(\"Need to define loss type\")\n",
    "\n",
    "    return model, opt, epoch, metrics, args, label2id, id2label\n",
    "\n",
    "\n",
    "def init_optimizer(args, model, opt_type=\"noam\"):\n",
    "    dim_input = args.dim_input\n",
    "    warmup = args.warmup\n",
    "    lr = args.lr\n",
    "\n",
    "    if opt_type == \"noam\":\n",
    "        opt = NoamOpt(dim_input, args.k_lr, warmup, torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9), min_lr=args.min_lr)\n",
    "    elif opt_type == \"sgd\":\n",
    "        opt = AnnealingOpt(lr, args.lr_anneal, torch.optim.SGD(model.parameters(), lr=lr, momentum=args.momentum, nesterov=True))\n",
    "    else:\n",
    "        opt = None\n",
    "        print(\"Optimizer is not defined\")\n",
    "\n",
    "    return opt\n",
    "\n",
    "def init_transformer_model(args, label2id, id2label):\n",
    "    \"\"\"\n",
    "    Initiate a new transformer object\n",
    "    \"\"\"\n",
    "    if args.feat_extractor == 'emb_cnn':\n",
    "        hidden_size = int(math.floor(\n",
    "            (args.sample_rate * args.window_size) / 2) + 1)\n",
    "        hidden_size = int(math.floor(hidden_size - 41) / 2 + 1)\n",
    "        hidden_size = int(math.floor(hidden_size - 21) / 2 + 1)\n",
    "        hidden_size *= 32\n",
    "        args.dim_input = hidden_size\n",
    "    elif args.feat_extractor == 'vgg_cnn':\n",
    "        hidden_size = int(math.floor((args.sample_rate * args.window_size) / 2) + 1) # 161\n",
    "        hidden_size = int(math.floor(int(math.floor(hidden_size)/2)/2)) * 128 # divide by 2 for maxpooling\n",
    "        args.dim_input = hidden_size\n",
    "    else:\n",
    "        print(\"the model is initialized without feature extractor\")\n",
    "\n",
    "    num_layers = args.num_layers\n",
    "    num_heads = args.num_heads\n",
    "    dim_model = args.dim_model\n",
    "    dim_key = args.dim_key\n",
    "    dim_value = args.dim_value\n",
    "    dim_input = args.dim_input\n",
    "    dim_inner = args.dim_inner\n",
    "    dim_emb = args.dim_emb\n",
    "    src_max_len = args.src_max_len\n",
    "    tgt_max_len = args.tgt_max_len\n",
    "    dropout = args.dropout\n",
    "    emb_trg_sharing = args.emb_trg_sharing\n",
    "    feat_extractor = args.feat_extractor\n",
    "\n",
    "    encoder = Encoder(num_layers, num_heads=num_heads, dim_model=dim_model, dim_key=dim_key,\n",
    "                      dim_value=dim_value, dim_input=dim_input, dim_inner=dim_inner, src_max_length=src_max_len, dropout=dropout)\n",
    "    decoder = Decoder(id2label, num_src_vocab=len(label2id), num_trg_vocab=len(label2id), num_layers=num_layers, num_heads=num_heads,\n",
    "                      dim_emb=dim_emb, dim_model=dim_model, dim_inner=dim_inner, dim_key=dim_key, dim_value=dim_value, trg_max_length=tgt_max_len, dropout=dropout, emb_trg_sharing=emb_trg_sharing)\n",
    "    model = Transformer(encoder, decoder, feat_extractor=feat_extractor)\n",
    "\n",
    "    if args.parallel:\n",
    "        device_ids = args.device_ids\n",
    "        if constant.args.device_ids:\n",
    "            print(\"load with device_ids\", constant.args.device_ids)\n",
    "            model = nn.DataParallel(model, device_ids=constant.args.device_ids)\n",
    "        else:\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.lm.transformer_lm import TransformerLM\n",
    "# from utils.optimizer import NoamOpt\n",
    "\n",
    "\n",
    "# def save_model(model, epoch, opt, metrics, label2id, id2label, best_model=False):\n",
    "#     \"\"\"\n",
    "#     Saving model, TODO adding history\n",
    "#     \"\"\"\n",
    "#     if best_model:\n",
    "#         save_path = \"{}/{}/best_model.th\".format(\n",
    "#             constant.args.save_folder, constant.args.name)\n",
    "#     else:\n",
    "#         save_path = \"{}/{}/epoch_{}.th\".format(constant.args.save_folder,\n",
    "#                                                constant.args.name, epoch)\n",
    "\n",
    "#     if not os.path.exists(constant.args.save_folder + \"/\" + constant.args.name):\n",
    "#         os.makedirs(constant.args.save_folder + \"/\" + constant.args.name)\n",
    "\n",
    "#     print(\"SAVE MODEL to\", save_path)\n",
    "#     args = {\n",
    "#         'label2id': label2id,\n",
    "#         'id2label': id2label,\n",
    "#         'args': constant.args,\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': opt.optimizer.state_dict(),\n",
    "#         'optimizer_params': {\n",
    "#             '_step': opt._step,\n",
    "#             '_rate': opt._rate,\n",
    "#             'warmup': opt.warmup,\n",
    "#             'factor': opt.factor,\n",
    "#             'model_size': opt.model_size\n",
    "#         },\n",
    "#         'metrics': metrics\n",
    "#     }\n",
    "#     torch.save(args, save_path)\n",
    "\n",
    "\n",
    "# def load_model(load_path):\n",
    "#     \"\"\"\n",
    "#     Loading model\n",
    "#     args:\n",
    "#         load_path: string\n",
    "#     \"\"\"\n",
    "#     checkpoint = torch.load(load_path)\n",
    "\n",
    "#     epoch = checkpoint['epoch']\n",
    "#     metrics = checkpoint['metrics']\n",
    "#     if 'args' in checkpoint:\n",
    "#         args = checkpoint['args']\n",
    "\n",
    "#     label2id = checkpoint['label2id']\n",
    "#     id2label = checkpoint['id2label']\n",
    "\n",
    "#     model = init_transformer_model(args, label2id, id2label)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     if args.cuda:\n",
    "#         model = model.cuda()\n",
    "\n",
    "#     opt = init_optimizer(args, model)\n",
    "#     if opt is not None:\n",
    "#         opt.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         opt._step = checkpoint['optimizer_params']['_step']\n",
    "#         opt._rate = checkpoint['optimizer_params']['_rate']\n",
    "#         opt.warmup = checkpoint['optimizer_params']['warmup']\n",
    "#         opt.factor = checkpoint['optimizer_params']['factor']\n",
    "#         opt.model_size = checkpoint['optimizer_params']['model_size']\n",
    "\n",
    "#     return model, opt, epoch, metrics, args, label2id, id2label\n",
    "\n",
    "\n",
    "# def init_optimizer(args, model):\n",
    "#     dim_input = args.dim_input\n",
    "#     warmup = args.warmup\n",
    "#     lr = args.lr\n",
    "\n",
    "#     opt = NoamOpt(dim_input, 1, warmup, torch.optim.Adam(\n",
    "#         model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "#     return opt\n",
    "\n",
    "\n",
    "def init_transformer_model(args, label2id, id2label):\n",
    "    \"\"\"\n",
    "    Initiate a new transformer object\n",
    "    \"\"\"\n",
    "\n",
    "    if args.emb_cnn:\n",
    "        hidden_size = int(math.floor(\n",
    "            (args.sample_rate * args.window_size) / 2) + 1)\n",
    "        hidden_size = int(math.floor(hidden_size - 41) / 2 + 1)\n",
    "        hidden_size = int(math.floor(hidden_size - 21) / 2 + 1)\n",
    "        hidden_size *= 32\n",
    "        args.dim_input = hidden_size\n",
    "\n",
    "    num_layers = args.num_layers\n",
    "    num_heads = args.num_heads\n",
    "    dim_model = args.dim_model\n",
    "    dim_key = args.dim_key\n",
    "    dim_value = args.dim_value\n",
    "    dim_input = args.dim_input\n",
    "    dim_inner = args.dim_inner\n",
    "    dim_emb = args.dim_emb\n",
    "    src_max_len = args.src_max_len\n",
    "    tgt_max_len = args.tgt_max_len\n",
    "    dropout = args.dropout\n",
    "    emb_trg_sharing = args.emb_trg_sharing\n",
    "\n",
    "    model = TransformerLM(id2label, num_src_vocab=len(label2id), num_trg_vocab=len(label2id), num_layers=num_layers, dim_emb=dim_emb, dim_model=dim_model, dim_inner=dim_inner, num_heads=num_heads, dim_key=dim_key, dim_value=dim_value, dropout=dropout)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.helper import get_word_segments_per_language, is_contain_chinese_word\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def calculate_lm_score(seq, lm, id2label):\n",
    "    \"\"\"\n",
    "    seq: (1, seq_len)\n",
    "    id2label: map\n",
    "    \"\"\"\n",
    "    # print(\"hello\")\n",
    "    seq_str = \"\".join(id2label[char.item()] for char in seq[0]).replace(\n",
    "        constant.PAD_CHAR, \"\").replace(constant.SOS_CHAR, \"\").replace(constant.EOS_CHAR, \"\")\n",
    "    seq_str = seq_str.replace(\"  \", \" \")\n",
    "\n",
    "    seq_arr = get_word_segments_per_language(seq_str)\n",
    "    seq_str = \"\"\n",
    "    for i in range(len(seq_arr)):\n",
    "        if is_contain_chinese_word(seq_arr[i]):\n",
    "            for char in seq_arr[i]:\n",
    "                if seq_str != \"\":\n",
    "                    seq_str += \" \"\n",
    "                seq_str += char\n",
    "        else:\n",
    "            if seq_str != \"\":\n",
    "                seq_str += \" \"\n",
    "            seq_str += seq_arr[i]\n",
    "\n",
    "    # print(\"seq_str:\", seq_str)\n",
    "    seq_str = seq_str.replace(\"  \", \" \").replace(\"  \", \" \")\n",
    "    # print(\"seq str:\", seq_str)\n",
    "\n",
    "    if seq_str == \"\":\n",
    "        return -999, 0, 0\n",
    "\n",
    "    score, oov_token = lm.evaluate(seq_str)    \n",
    "    \n",
    "    # a, b = lm.evaluate(\"除非 的 不会 improve 什么 东西 的 这些 esperience\")\n",
    "    # a2, b2 = lm.evaluate(\"除非 的 不会 improve 什么 东西 的 这些 experience\")\n",
    "    # print(a, a2)\n",
    "    return -1 * score / len(seq_str.split()) + 1, len(seq_str.split()) + 1, oov_token\n",
    "\n",
    "\n",
    "class LM(object):\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        print(\"load model path:\", self.model_path)\n",
    "\n",
    "        checkpoint = torch.load(model_path)\n",
    "        self.word2idx = checkpoint[\"word2idx\"]\n",
    "        self.idx2word = checkpoint[\"idx2word\"]\n",
    "        ntokens = checkpoint[\"ntoken\"]\n",
    "        ninp = checkpoint[\"ninp\"]\n",
    "        nhid = checkpoint[\"nhid\"]\n",
    "        nlayers = checkpoint[\"nlayers\"]\n",
    "        dropout = checkpoint[\"dropout\"]\n",
    "        tie_weights = checkpoint[\"tie_weights\"]\n",
    "\n",
    "        self.model = RNNModel(\"LSTM\", ntoken=ntokens, ninp=ninp, nhid=nhid,\n",
    "                              nlayers=nlayers, dropout=dropout, tie_weights=tie_weights)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        if constant.args.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def batchify(self, data, bsz, cuda):\n",
    "        # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "        nbatch = data.size(0) // bsz\n",
    "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "        data = data.narrow(0, 0, nbatch * bsz)\n",
    "        # Evenly divide the data across the bsz batches.\n",
    "        data = data.view(bsz, -1).t().contiguous()\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        return data\n",
    "\n",
    "    def seq_to_tensor(self, seq):\n",
    "        words = seq.split() + ['<eos>']\n",
    "\n",
    "        ids = torch.LongTensor(len(words))\n",
    "        token = 0\n",
    "        oov_token = 0\n",
    "        for word in words:\n",
    "            if word in self.word2idx:\n",
    "                ids[token] = self.word2idx[word]\n",
    "            else:\n",
    "                ids[token] = self.word2idx['<oov>']\n",
    "                oov_token += 1\n",
    "            # print(\">\", word, ids[token])\n",
    "            token += 1\n",
    "        # print(\"ids\", ids)\n",
    "        return ids, oov_token\n",
    "\n",
    "    def get_batch(self, source, i, bptt, seq_len=None, evaluation=False):\n",
    "        seq_len = min(seq_len if seq_len else bptt, len(source) - 1 - i)\n",
    "        data = source[i:i+seq_len]\n",
    "        target = source[i+1:i+1+seq_len].view(-1)\n",
    "        return data, target\n",
    "\n",
    "    def evaluate(self, seq):\n",
    "        \"\"\"\n",
    "        batch_size = 1\n",
    "        \"\"\"\n",
    "        tensor, oov_token = self.seq_to_tensor(seq)\n",
    "        data_source = self.batchify(tensor\n",
    "            , 1, constant.args.cuda)\n",
    "        self.model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        ntokens = len(self.word2idx)\n",
    "        hidden = self.model.init_hidden(1)\n",
    "        data, targets = self.get_batch(\n",
    "            data_source, 0, data_source.size(0), evaluation=True)\n",
    "        output, hidden = self.model(data, hidden)\n",
    "\n",
    "        # calculate probability\n",
    "        # print(output.size()) # seq_len, vocab\n",
    "\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * self.criterion(output_flat, targets).data\n",
    "        hidden = self.repackage_hidden(hidden)\n",
    "        return total_loss, oov_token\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        \"\"\"Wraps hidden states in new Tensors,\n",
    "        to detach them from their history.\"\"\"\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(\n",
    "                ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh',\n",
    "                                'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError(\"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers,\n",
    "                              nonlinearity=nonlinearity, dropout=dropout)\n",
    "\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError(\n",
    "                    'When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "\n",
    "        decoded = self.decoder(output.view(\n",
    "            output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as Lev\n",
    "\n",
    "from data.helper import get_word_segments_per_language, is_contain_chinese_word\n",
    "\n",
    "def calculate_cer_en_zh(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence (hyp)\n",
    "        s2 (string): space-separated sentence (gold)\n",
    "    \"\"\"\n",
    "    s1_segments = get_word_segments_per_language(s1)\n",
    "    s2_segments = get_word_segments_per_language(s2)\n",
    "\n",
    "    en_s1_seq, en_s2_seq = \"\", \"\"\n",
    "    zh_s1_seq, zh_s2_seq = \"\", \"\"\n",
    "\n",
    "    for segment in s1_segments:\n",
    "        if is_contain_chinese_word(segment):\n",
    "            if zh_s1_seq != \"\":\n",
    "                zh_s1_seq += \" \"\n",
    "            zh_s1_seq += segment\n",
    "        else:\n",
    "            if en_s1_seq != \"\":\n",
    "                en_s1_seq += \" \"\n",
    "            en_s1_seq += segment\n",
    "    \n",
    "    for segment in s2_segments:\n",
    "        if is_contain_chinese_word(segment):\n",
    "            if zh_s2_seq != \"\":\n",
    "                zh_s2_seq += \" \"\n",
    "            zh_s2_seq += segment\n",
    "        else:\n",
    "            if en_s2_seq != \"\":\n",
    "                en_s2_seq += \" \"\n",
    "            en_s2_seq += segment\n",
    "\n",
    "    # print(\">\", en_s1_seq, \"||\", en_s2_seq, len(en_s2_seq), \"||\", calculate_cer(en_s1_seq, en_s2_seq) / max(1, len(en_s2_seq.replace(' ', ''))))\n",
    "    # print(\">>\", zh_s1_seq, \"||\", zh_s2_seq, len(zh_s2_seq), \"||\", calculate_cer(zh_s1_seq, zh_s2_seq) /  max(1, len(zh_s2_seq.replace(' ', ''))))\n",
    "\n",
    "    return calculate_cer(en_s1_seq, en_s2_seq), calculate_cer(zh_s1_seq, zh_s2_seq), len(en_s2_seq), len(zh_s2_seq)\n",
    "\n",
    "def calculate_cer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence (hyp)\n",
    "        s2 (string): space-separated sentence (gold)\n",
    "    \"\"\"\n",
    "    return Lev.distance(s1, s2)\n",
    "\n",
    "def calculate_wer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "def calculate_metrics(pred, gold, input_lengths=None, target_lengths=None, smoothing=0.0, loss_type=\"ce\"):\n",
    "    \"\"\"\n",
    "    Calculate metrics\n",
    "    args:\n",
    "        pred: B x T x C\n",
    "        gold: B x T\n",
    "        input_lengths: B (for CTC)\n",
    "        target_lengths: B (for CTC)\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(pred, gold, input_lengths, target_lengths, smoothing, loss_type)\n",
    "    if loss_type == \"ce\":\n",
    "        pred = pred.view(-1, pred.size(2)) # (B*T) x C\n",
    "        gold = gold.contiguous().view(-1) # (B*T)\n",
    "        pred = pred.max(1)[1]\n",
    "        non_pad_mask = gold.ne(constant.PAD_TOKEN)\n",
    "        num_correct = pred.eq(gold)\n",
    "        num_correct = num_correct.masked_select(non_pad_mask).sum().item()\n",
    "        return loss, num_correct\n",
    "    elif loss_type == \"ctc\":\n",
    "        return loss, None\n",
    "    else:\n",
    "        print(\"loss is not defined\")\n",
    "        return None, None\n",
    "\n",
    "def calculate_loss(pred, gold, input_lengths=None, target_lengths=None, smoothing=0.0, loss_type=\"ce\"):\n",
    "    \"\"\"\n",
    "    Calculate loss\n",
    "    args:\n",
    "        pred: B x T x C\n",
    "        gold: B x T\n",
    "        input_lengths: B (for CTC)\n",
    "        target_lengths: B (for CTC)\n",
    "        smoothing:\n",
    "        type: ce|ctc (ctc => pytorch 1.0.0 or later)\n",
    "        input_lengths: B (only for ctc)\n",
    "        target_lengths: B (only for ctc)\n",
    "    \"\"\"\n",
    "    if loss_type == \"ce\":\n",
    "        pred = pred.view(-1, pred.size(2)) # (B*T) x C\n",
    "        gold = gold.contiguous().view(-1) # (B*T)\n",
    "        if smoothing > 0.0:\n",
    "            eps = smoothing\n",
    "            num_class = pred.size(1)\n",
    "\n",
    "            gold_for_scatter = gold.ne(constant.PAD_TOKEN).long() * gold\n",
    "            one_hot = torch.zeros_like(pred).scatter(1, gold_for_scatter.view(-1, 1), 1)\n",
    "            one_hot = one_hot * (1-eps) + (1-one_hot) * eps / num_class\n",
    "            log_prob = F.log_softmax(pred, dim=1)\n",
    "\n",
    "            non_pad_mask = gold.ne(constant.PAD_TOKEN)\n",
    "            num_word = non_pad_mask.sum().item()\n",
    "            loss = -(one_hot * log_prob).sum(dim=1)\n",
    "            loss = loss.masked_select(non_pad_mask).sum() / num_word\n",
    "        else:\n",
    "            loss = F.cross_entropy(pred, gold, ignore_index=constant.PAD_TOKEN, reduction=\"mean\")\n",
    "    elif loss_type == \"ctc\":\n",
    "        log_probs = pred.transpose(0, 1) # T x B x C\n",
    "        # print(gold.size())\n",
    "        targets = gold\n",
    "        # targets = gold.contiguous().view(-1) # (B*T)\n",
    "\n",
    "        \"\"\"\n",
    "        log_probs: torch.Size([209, 8, 3793])\n",
    "        targets: torch.Size([8, 46])\n",
    "        input_lengths: torch.Size([8])\n",
    "        target_lengths: torch.Size([8])\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"log_probs:\", log_probs.size())\n",
    "        # print(\"targets:\", targets.size())\n",
    "        # print(\"input_lengths:\", input_lengths.size())\n",
    "        # print(\"target_lengths:\", target_lengths.size())\n",
    "        # print(input_lengths)\n",
    "        # print(target_lengths)\n",
    "\n",
    "        log_probs = F.log_softmax(log_probs, dim=2)\n",
    "        loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction=\"mean\")\n",
    "        # mask = loss.clone() # mask Inf loss\n",
    "        # # mask[mask != float(\"Inf\")] = 1\n",
    "        # mask[mask == float(\"Inf\")] = 0\n",
    "\n",
    "        # loss = mask\n",
    "        # print(loss)\n",
    "\n",
    "        # loss_size = len(loss)\n",
    "        # loss = loss.sum() / loss_size\n",
    "        # print(loss)\n",
    "    else:\n",
    "        print(\"loss is not defined\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer, min_lr=1e-5):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"Implement `lrate` above\"\n",
    "        step = self._step\n",
    "        return max(self.min_lr, self.factor * \\\n",
    "            (self.model_size ** (-0.5) * min(step **\n",
    "                                             (-0.5), step * self.warmup ** (-1.5))))\n",
    "\n",
    "class AnnealingOpt:\n",
    "    \"Optim wrapper for annealing opt\"\n",
    "\n",
    "    def __init__(self, lr, lr_anneal, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.lr_anneal = lr_anneal\n",
    "    \n",
    "    def step(self):\n",
    "        optim_state = self.optimizer.state_dict()\n",
    "        optim_state['param_groups'][0]['lr'] = optim_state['param_groups'][0]['lr'] / self.lr_anneal\n",
    "        self.optimizer.load_state_dict(optim_state)\n",
    "\n",
    "# class SGDOpt:\n",
    "#     \"Optim wrapper that implements SGD\"\n",
    "\n",
    "#     def __init__(self, parameters, lr, momentum, nesterov=True):\n",
    "#         self.optimizer = torch.optim.SGD(parameters, lr=lr, momentum=momentum, nesterov=nesterov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "\n",
    "from scipy import spatial\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import numpy\n",
    "import subprocess\n",
    "\n",
    "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "def load_stanford_core_nlp(path):\n",
    "    from stanfordcorenlp import StanfordCoreNLP\n",
    "    \n",
    "    \"\"\"\n",
    "    Load stanford core NLP toolkit object\n",
    "    args:\n",
    "        path: String\n",
    "    output:\n",
    "        Stanford core NLP objects\n",
    "    \"\"\"\n",
    "    zh_nlp = StanfordCoreNLP(path, lang='zh')\n",
    "    en_nlp = StanfordCoreNLP(path, lang='en')\n",
    "    return zh_nlp, en_nlp\n",
    "\n",
    "\"\"\"\n",
    "################################################\n",
    "TEXT PREPROCESSING\n",
    "################################################\n",
    "\"\"\"\n",
    "\n",
    "def is_chinese_char(cc):\n",
    "    \"\"\"\n",
    "    Check if the character is Chinese\n",
    "    args:\n",
    "        cc: char\n",
    "    output:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    return unicodedata.category(cc) == 'Lo'\n",
    "\n",
    "def is_contain_chinese_word(seq):\n",
    "    \"\"\"\n",
    "    Check if the sequence has chinese character(s)\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    for i in range(len(seq)):\n",
    "        if is_chinese_char(seq[i]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_word_segments_per_language(seq):\n",
    "    \"\"\"\n",
    "    Get word segments \n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        word_segments: list of String\n",
    "    \"\"\"\n",
    "    cur_lang = -1 # cur_lang = 0 (english), 1 (chinese)\n",
    "    words = seq.split(\" \")\n",
    "    temp_words = \"\"\n",
    "    word_segments = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "\n",
    "        if is_contain_chinese_word(word):\n",
    "            if cur_lang == -1:\n",
    "                cur_lang = 1\n",
    "                temp_words = word\n",
    "            elif cur_lang == 0: # english\n",
    "                cur_lang = 1\n",
    "                word_segments.append(temp_words)\n",
    "                temp_words = word\n",
    "            else:\n",
    "                if temp_words != \"\":\n",
    "                    temp_words += \" \"\n",
    "                temp_words += word\n",
    "        else:\n",
    "            if cur_lang == -1:\n",
    "                cur_lang = 0\n",
    "                temp_words = word\n",
    "            elif cur_lang == 1: # chinese\n",
    "                cur_lang = 0\n",
    "                word_segments.append(temp_words)\n",
    "                temp_words = word\n",
    "            else:\n",
    "                if temp_words != \"\":\n",
    "                    temp_words += \" \"\n",
    "                temp_words += word\n",
    "\n",
    "    word_segments.append(temp_words)\n",
    "\n",
    "    return word_segments\n",
    "\n",
    "def get_word_segments_per_language_with_tokenization(seq, tokenize_lang=-1, zh_nlp=None, en_nlp=None):\n",
    "    \"\"\"\n",
    "    Get word segments and tokenize the sequence for selected language\n",
    "    We cannot run two different languages on stanford core nlp, will be very slow\n",
    "    so instead we do it as many times as the number of languages we want to tokenize\n",
    "    args:\n",
    "        seq: String\n",
    "        tokenize_lang: int (-1 means no language is selected, 0 (english), 1 (chinese))\n",
    "    \"\"\"\n",
    "    cur_lang = -1\n",
    "    words = seq.split(\" \")\n",
    "    temp_words = \"\"\n",
    "    word_segments = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "\n",
    "        if is_contain_chinese_word(word):\n",
    "            if cur_lang == -1:\n",
    "                cur_lang = 1\n",
    "                temp_words = word\n",
    "            elif cur_lang == 0: # english\n",
    "                cur_lang = 1\n",
    "\n",
    "                if tokenize_lang == 0:\n",
    "                    word_list = en_nlp.word_tokenize(temp_words)\n",
    "                    temp_words = ' '.join(word for word in word_list)\n",
    "\n",
    "                word_segments.append(temp_words)\n",
    "                temp_words = word\n",
    "            else:\n",
    "                if temp_words != \"\":\n",
    "                    temp_words += \" \"\n",
    "                temp_words += word\n",
    "        else:\n",
    "            if cur_lang == -1:\n",
    "                cur_lang = 0\n",
    "                temp_words = word\n",
    "            elif cur_lang == 1: # chinese\n",
    "                cur_lang = 0\n",
    "\n",
    "                if tokenize_lang == 1:\n",
    "                    word_list = zh_nlp.word_tokenize(temp_words.replace(\" \",\"\"))\n",
    "                    temp_words = ' '.join(word for word in word_list)\n",
    "\n",
    "                word_segments.append(temp_words)\n",
    "                temp_words = word\n",
    "            else:\n",
    "                if temp_words != \"\":\n",
    "                    temp_words += \" \"\n",
    "                temp_words += word\n",
    "\n",
    "    if tokenize_lang == 0 and cur_lang == 0:\n",
    "        word_list = en_nlp.word_tokenize(temp_words)\n",
    "        temp_words = ' '.join(word for word in word_list)\n",
    "    elif tokenize_lang == 1 and cur_lang == 1:\n",
    "        word_list = zh_nlp.word_tokenize(temp_words)\n",
    "        temp_words = ' '.join(word for word in word_list)\n",
    "\n",
    "    word_segments.append(temp_words)\n",
    "\n",
    "    # word_seq = \"\"\n",
    "    # for i in range(len(word_segments)):\n",
    "    #     if word_seq != \"\":\n",
    "    #         word_seq += \" \"\n",
    "    #     else:\n",
    "    #         word_seq = word_segments[i]\n",
    "\n",
    "    return word_segments\n",
    "\n",
    "def remove_emojis(seq):\n",
    "    \"\"\"\n",
    "    Remove emojis\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    seq = emoji_pattern.sub(r'', seq).strip()\n",
    "    return seq\n",
    "\n",
    "def merge_abbreviation(seq):\n",
    "    seq = seq.replace(\"  \", \" \")\n",
    "    words = seq.split(\" \")\n",
    "    final_seq = \"\"\n",
    "    temp = \"\"\n",
    "    for i in range(len(words)):\n",
    "        word_length = len(words[i])\n",
    "        if word_length == 0: # unknown character case\n",
    "            continue\n",
    "\n",
    "        if words[i][word_length-1] == \".\":\n",
    "            temp += words[i]\n",
    "        else:\n",
    "            if temp != \"\":\n",
    "                if final_seq != \"\":\n",
    "                    final_seq += \" \"\n",
    "                final_seq += temp\n",
    "                temp = \"\"\n",
    "            if final_seq != \"\":\n",
    "                final_seq += \" \"\n",
    "            final_seq += words[i]\n",
    "    if temp != \"\":\n",
    "        if final_seq != \"\":\n",
    "            final_seq += \" \"\n",
    "        final_seq += temp\n",
    "    return final_seq\n",
    "\n",
    "def remove_punctuation(seq):\n",
    "    \"\"\"\n",
    "    Remove english and chinese punctuation except hypen/dash, and full stop.\n",
    "    Also fix some typos and encoding issues\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    seq = re.sub(\"[\\s+\\\\!\\/_,$%=^*?:@&^~`(+\\\"]+|[+！，。？、~@#￥%……&*（）:;：；《）《》“”()»〔〕]+\", \" \", seq)\n",
    "    seq = seq.replace(\" ' \", \" \")\n",
    "    seq = seq.replace(\" ’ \", \" \")\n",
    "    seq = seq.replace(\" ＇ \", \" \")\n",
    "    seq = seq.replace(\" ` \", \" \")\n",
    "\n",
    "    seq = seq.replace(\" '\", \"'\")\n",
    "    seq = seq.replace(\" ’\", \"’\")\n",
    "    seq = seq.replace(\" ＇\", \"＇\")\n",
    "\n",
    "    seq = seq.replace(\"' \", \" \")\n",
    "    seq = seq.replace(\"’ \", \" \")\n",
    "    seq = seq.replace(\"＇ \", \" \")\n",
    "    seq = seq.replace(\"` \", \" \")\n",
    "    seq = seq.replace(\".\", \"\")\n",
    "\n",
    "    seq = seq.replace(\"`\", \"\")\n",
    "    seq = seq.replace(\"-\", \" \")\n",
    "    seq = seq.replace(\"?\", \" \")\n",
    "    seq = seq.replace(\":\", \" \")\n",
    "    seq = seq.replace(\";\", \" \")\n",
    "    seq = seq.replace(\"]\", \" \")\n",
    "    seq = seq.replace(\"[\", \" \")\n",
    "    seq = seq.replace(\"}\", \" \")\n",
    "    seq = seq.replace(\"{\", \" \")\n",
    "    seq = seq.replace(\"|\", \" \")\n",
    "    seq = seq.replace(\"_\", \" \")\n",
    "    seq = seq.replace(\"(\", \" \")\n",
    "    seq = seq.replace(\")\", \" \")\n",
    "    seq = seq.replace(\"=\", \" \")\n",
    "\n",
    "    seq = seq.replace(\" dont \", \" don't \")\n",
    "    seq = seq.replace(\"welcome外星人\", \"welcome 外星人\")\n",
    "    seq = seq.replace(\"doens't\", \"doesn't\")\n",
    "    seq = seq.replace(\"o' clock\", \"o'clock\")\n",
    "    seq = seq.replace(\"因为it's\", \"因为 it's\")\n",
    "    seq = seq.replace(\"it' s\", \"it's\")\n",
    "    seq = seq.replace(\"it ' s\", \"it's\")\n",
    "    seq = seq.replace(\"it' s\", \"it's\")\n",
    "    seq = seq.replace(\"y'\", \"y\")\n",
    "    seq = seq.replace(\"y ' \", \"y\")\n",
    "    seq = seq.replace(\"看different\", \"看 different\")\n",
    "    seq = seq.replace(\"it'self\", \"itself\")\n",
    "    seq = seq.replace(\"it'ss\", \"it's\")\n",
    "    seq = seq.replace(\"don'r\", \"don't\")\n",
    "    seq = seq.replace(\"has't\", \"hasn't\")\n",
    "    seq = seq.replace(\"don'know\", \"don't know\")\n",
    "    seq = seq.replace(\"i'll\", \"i will\")\n",
    "    seq = seq.replace(\"you're\", \"you are\")\n",
    "    seq = seq.replace(\"'re \", \" are \")\n",
    "    seq = seq.replace(\"'ll \", \" will \")\n",
    "    seq = seq.replace(\"'ve \", \" have \")\n",
    "    seq = seq.replace(\"'re\\n\", \" are\\n\")\n",
    "    seq = seq.replace(\"'ll\\n\", \" will\\n\")\n",
    "    seq = seq.replace(\"'ve\\n\", \" have\\n\")\n",
    "\n",
    "    seq = remove_space_in_between_words(seq)\n",
    "    return seq\n",
    "\n",
    "def remove_special_char(seq):\n",
    "    \"\"\"\n",
    "    Remove special characters from the corpus\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    seq = re.sub(\"[【】·．％°℃×→①ぃγ￣σς＝～•＋δ≤∶／⊥＿ñãíå\u0001∈△β［］±]+\", \" \", seq)\n",
    "    return seq\n",
    "\n",
    "def remove_space_in_between_words(seq):\n",
    "    \"\"\"\n",
    "    Remove space between words\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    return seq.replace(\"  \", \" \").replace(\"  \", \" \").replace(\"  \", \" \").replace(\"  \", \" \").strip().lstrip()\n",
    "\n",
    "def remove_return(seq):\n",
    "    \"\"\"\n",
    "    Remove return characters\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    return seq.replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"\\t\", \"\")\n",
    "\n",
    "def preprocess_mixed_language_sentence(seq, tokenize=False, en_nlp=None, zh_nlp=None, tokenize_lang=-1):\n",
    "    \"\"\"\n",
    "    Preprocess function\n",
    "    args:\n",
    "        seq: String\n",
    "    output:\n",
    "        seq: String\n",
    "    \"\"\"\n",
    "    if len(seq) == 0:\n",
    "        return \"\"\n",
    "        \n",
    "    seq = seq.lower()\n",
    "    seq = merge_abbreviation(seq)\n",
    "    seq = seq.replace(\"\\x7f\", \"\")\n",
    "    seq = seq.replace(\"\\x80\", \"\")\n",
    "    seq = seq.replace(\"\\u3000\", \" \")\n",
    "    seq = seq.replace(\"\\xa0\", \"\")\n",
    "    seq = seq.replace(\"[\", \" [\")\n",
    "    seq = seq.replace(\"]\", \"] \")\n",
    "    seq = seq.replace(\"#\", \"\")\n",
    "    seq = seq.replace(\",\", \"\")\n",
    "    seq = seq.replace(\"*\", \"\")\n",
    "    seq = seq.replace(\"\\n\", \"\")\n",
    "    seq = seq.replace(\"\\r\", \"\")\n",
    "    seq = seq.replace(\"\\t\", \"\")\n",
    "    seq = seq.replace(\"~\", \"\")\n",
    "    seq = seq.replace(\"—\", \"\")\n",
    "    seq = seq.replace(\"  \", \" \").replace(\"  \", \" \")\n",
    "    seq = re.sub('\\<.*?\\>','', seq) # REMOVE < >\n",
    "    seq = re.sub('\\【.*?\\】','', seq) # REMOVE 【 】\n",
    "    seq = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", seq) # REMOVE ALL WORDS WITH BRACKETS (HESITATION)\n",
    "    seq = re.sub(\"[\\{\\[].*?[\\}\\]]\", \"\", seq) # REMOVE ALL WORDS WITH BRACKETS (HESITATION)\n",
    "    seq = remove_special_char(seq)\n",
    "    seq = remove_space_in_between_words(seq)\n",
    "    seq = seq.strip()\n",
    "    seq = seq.lstrip()\n",
    "    \n",
    "    seq = remove_punctuation(seq)\n",
    "\n",
    "    temp_words =  \"\"\n",
    "    if not tokenize:\n",
    "        segments = get_word_segments_per_language(seq)\n",
    "    else:\n",
    "        segments = get_word_segments_per_language_with_tokenization(seq, en_nlp=en_nlp, zh_nlp=zh_nlp, tokenize_lang=tokenize_lang)\n",
    "\n",
    "    for j in range(len(segments)):\n",
    "        if not is_contain_chinese_word(segments[j]):\n",
    "            segments[j] = re.sub(r'[^\\x00-\\x7f]',r' ',segments[j])\n",
    "\n",
    "        if temp_words != \"\":\n",
    "            temp_words += \" \"\n",
    "        temp_words += segments[j].replace(\"\\n\", \"\")\n",
    "    seq = temp_words\n",
    "\n",
    "    seq = remove_space_in_between_words(seq)\n",
    "    seq = seq.strip()\n",
    "    seq = seq.lstrip()\n",
    "\n",
    "    # Tokenize chinese characters\n",
    "    if len(seq) <= 1:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return seq\n",
    "\n",
    "\"\"\"\n",
    "################################################\n",
    "AUDIO PREPROCESSING\n",
    "################################################\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_wav(root, dirc, filename):\n",
    "\tsource_audio = root + \"/\" + dirc + \"/audio/\" + filename + \".flac\"\n",
    "\n",
    "\twith open(root + \"/\" + dirc + \"/proc_transcript/phaseII/\" + filename + \".txt\", \"r\", encoding=\"utf-8\") as transcript_file:\n",
    "\t\tpart_num = 0\n",
    "\t\tfor line in transcript_file:\n",
    "\t\t\tdata = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\t\t\tstart_time = float(data[1]) / 1000\n",
    "\t\t\tend_time = float(data[2]) / 1000\n",
    "\t\t\tdif_time = end_time-start_time\n",
    "\t\t\ttext = data[4]\n",
    "\t\t\ttarget_flac_audio = root + \"/parts/\" + dirc + \"/flac/\" + filename + \"_\" + str(part_num) + \".flac\"\n",
    "\t\t\ttarget_wav_audio = root + \"/parts/\" + dirc + \"/wav/\" + filename + \"_\" + str(part_num) + \".wav\"\n",
    "\t\t\t# print(\"sox \" + source_audio + \" \" + target_flac_audio + \" trim \" + str(start_time) + \" \" + str(dif_time))\n",
    "\n",
    "\t\t\tpipe = subprocess.check_output(\"sox \" + source_audio + \" \" + target_flac_audio + \" trim \" + str(start_time) + \" \" + str(dif_time), shell=True)\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# print(\"sox \" + target_flac_audio + \" \" + target_wav_audio)\n",
    "\t\t\t\tout2 = os.popen(\"sox \" + target_flac_audio + \" \" + target_wav_audio).read()\n",
    "\t\t\t\tsound, _ = torchaudio.load(target_wav_audio)\n",
    "\n",
    "\t\t\t\t# print(\"Write transcript\")\n",
    "\t\t\t\twith open(root + \"/parts/\" + dirc + \"/proc_transcript/\" + filename + \"_\" + str(part_num) + \".txt\", \"w+\", encoding=\"utf-8\") as text_file:\n",
    "\t\t\t\t\ttext_file.write(text + \"\\n\")\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(\"Error reading audio file: unknown length, the audio is not with proper length, skip, target_flac_audio {}\", target_flac_audio)\n",
    "\n",
    "\t\t\tpart_num += 1\n",
    "\n",
    "\"\"\"\n",
    "################################################\n",
    "COMMON FUNCTIONS\n",
    "################################################\n",
    "\"\"\"\n",
    "\n",
    "def traverse(root, path, dev_conversation_phase2, test_conversation_phase2, dev_interview_phase2, test_interview_phase2, search_fix=\".txt\"):\n",
    "    f_train_list = []\n",
    "    f_dev_list = []\n",
    "    f_test_list = []\n",
    "\n",
    "    p = root + path\n",
    "    for sub_p in sorted(os.listdir(p)):\n",
    "        if sub_p[len(sub_p)-len(search_fix):] == search_fix:\n",
    "            if \"conversation\" in path:\n",
    "                print(\">\", path, sub_p)\n",
    "                if sub_p[2:6] in dev_conversation_phase2:\n",
    "                    f_dev_list.append(p + \"/\" + sub_p)\n",
    "                elif sub_p[2:6] in test_conversation_phase2:\n",
    "                    f_test_list.append(p + \"/\" + sub_p)\n",
    "                else:\n",
    "                    f_train_list.append(p + \"/\" + sub_p)\n",
    "            elif \"interview\" in path:\n",
    "                print(\">\", path, sub_p)\n",
    "                if sub_p[:4] in dev_interview_phase2:\n",
    "                    f_dev_list.append(p + \"/\" + sub_p)\n",
    "                elif sub_p[:4] in test_interview_phase2:\n",
    "                    f_test_list.append(p + \"/\" + sub_p)\n",
    "                else:\n",
    "                    f_train_list.append(p + \"/\" + sub_p)\n",
    "            else:\n",
    "                print(\"hoho\")\n",
    "\n",
    "    return f_train_list, f_dev_list, f_test_list\n",
    "\n",
    "def traverse_all(root, path):\n",
    "    f_list = []\n",
    "\n",
    "    p = root + path\n",
    "    for sub_p in sorted(os.listdir(p)):\n",
    "        f_list.append(p + \"/\" + sub_p)\n",
    "\n",
    "    return f_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import tarfile\n",
    "import argparse\n",
    "import shutil\n",
    "import ipdb\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Processes and downloads LibriSpeech dataset.')\n",
    "parser.add_argument(\"--target-dir\", default='LibriSpeech_dataset/', type=str, help=\"Directory to store the dataset.\")\n",
    "parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "parser.add_argument('--files-to-use', default=\"train-clean-100.tar.gz,\"\n",
    "                                              \"train-clean-360.tar.gz,train-other-500.tar.gz,\"\n",
    "                                              \"dev-clean.tar.gz,dev-other.tar.gz,\"\n",
    "                                              \"test-clean.tar.gz,test-other.tar.gz\", type=str,\n",
    "                    help='list of file names to download')\n",
    "parser.add_argument('--min-duration', default=1, type=int,\n",
    "                    help='Prunes training samples shorter than the min duration (given in seconds, default 1)')\n",
    "parser.add_argument('--max-duration', default=15, type=int,\n",
    "                    help='Prunes training samples longer than the max duration (given in seconds, default 15)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "LIBRI_SPEECH_URLS = {\n",
    "    \"train\": [\"http://www.openslr.org/resources/12/train-clean-100.tar.gz\",\n",
    "              \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\",\n",
    "              \"http://www.openslr.org/resources/12/train-other-500.tar.gz\"],\n",
    "\n",
    "    \"val\": [\"http://www.openslr.org/resources/12/dev-clean.tar.gz\",\n",
    "            \"http://www.openslr.org/resources/12/dev-other.tar.gz\"],\n",
    "\n",
    "    \"test_clean\": [\"http://www.openslr.org/resources/12/test-clean.tar.gz\"],\n",
    "    \"test_other\": [\"http://www.openslr.org/resources/12/test-other.tar.gz\"]\n",
    "}\n",
    "\n",
    "\n",
    "def _preprocess_transcript(phrase):\n",
    "    return phrase.strip().lower()\n",
    "\n",
    "\n",
    "def _process_file(wav_dir, txt_dir, base_filename, root_dir):\n",
    "    full_recording_path = os.path.join(root_dir, base_filename)\n",
    "    assert os.path.exists(full_recording_path) and os.path.exists(root_dir)\n",
    "    wav_recording_path = os.path.join(wav_dir, base_filename.replace(\".flac\", \".wav\"))\n",
    "    \n",
    "#     subprocess.call([\"sox {}  -r {} -b 16 -c 1 {}\".format(full_recording_path, str(args.sample_rate),\n",
    "#                                                           wav_recording_path)], shell=True)\n",
    "    \n",
    "    subprocess.call([f\"sox {full_recording_path}  -r {str(args.sample_rate)} -b 16 -c 1 {wav_recording_path}\"], shell=True)\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    # process transcript\n",
    "    txt_transcript_path = os.path.join(txt_dir, base_filename.replace(\".flac\", \".txt\"))\n",
    "    transcript_file = os.path.join(root_dir, \"-\".join(base_filename.split('-')[:-1]) + \".trans.txt\")\n",
    "    assert os.path.exists(transcript_file), \"Transcript file {} does not exist.\".format(transcript_file)\n",
    "    transcriptions = open(transcript_file).read().strip().split(\"\\n\")\n",
    "    transcriptions = {t.split()[0].split(\"-\")[-1]: \" \".join(t.split()[1:]) for t in transcriptions}\n",
    "    with open(txt_transcript_path, \"w\") as f:\n",
    "        key = base_filename.replace(\".flac\", \"\").split(\"-\")[-1]\n",
    "        assert key in transcriptions, \"{} is not in the transcriptions\".format(key)\n",
    "        f.write(_preprocess_transcript(transcriptions[key]))\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def main():\n",
    "    target_dl_dir = args.target_dir\n",
    "    if not os.path.exists(target_dl_dir):\n",
    "        os.makedirs(target_dl_dir)\n",
    "    files_to_dl = args.files_to_use.strip().split(',')\n",
    "    for split_type, lst_libri_urls in LIBRI_SPEECH_URLS.items():\n",
    "        split_dir = os.path.join(target_dl_dir, split_type)\n",
    "        if not os.path.exists(split_dir):\n",
    "            os.makedirs(split_dir)\n",
    "        split_wav_dir = os.path.join(split_dir, \"wav\")\n",
    "        if not os.path.exists(split_wav_dir):\n",
    "            os.makedirs(split_wav_dir)\n",
    "        split_txt_dir = os.path.join(split_dir, \"txt\")\n",
    "        if not os.path.exists(split_txt_dir):\n",
    "            os.makedirs(split_txt_dir)\n",
    "        extracted_dir = os.path.join(split_dir, \"LibriSpeech\")\n",
    "        if os.path.exists(extracted_dir):\n",
    "            shutil.rmtree(extracted_dir)\n",
    "        for url in lst_libri_urls:\n",
    "            # check if we want to dl this file\n",
    "            dl_flag = False\n",
    "            for f in files_to_dl:\n",
    "                if url.find(f) != -1:\n",
    "                    dl_flag = True\n",
    "            if not dl_flag:\n",
    "                print(\"Skipping url: {}\".format(url))\n",
    "                continue\n",
    "            filename = url.split(\"/\")[-1]\n",
    "            target_filename = os.path.join(split_dir, filename)\n",
    "#             if not os.path.exists(target_filename):\n",
    "#                 wget.download(url, split_dir)\n",
    "            print(\"Unpacking {}...\".format(filename))\n",
    "            tar = tarfile.open(target_filename)\n",
    "            tar.extractall(split_dir)\n",
    "            tar.close()\n",
    "            os.remove(target_filename)\n",
    "            print(\"Converting flac files to wav and extracting transcripts...\")\n",
    "            assert os.path.exists(extracted_dir), \"Archive {} was not properly uncompressed.\".format(filename)\n",
    "            for root, subdirs, files in tqdm(os.walk(extracted_dir)):\n",
    "                for f in files:\n",
    "                    if f.find(\".flac\") != -1:\n",
    "                        _process_file(wav_dir=split_wav_dir, txt_dir=split_txt_dir,\n",
    "                                      base_filename=f, root_dir=root)\n",
    "\n",
    "            print(\"Finished {}\".format(url))\n",
    "            shutil.rmtree(extracted_dir)\n",
    "        if split_type == 'train':  # Prune to min/max duration\n",
    "            create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv', args.min_duration, args.max_duration)\n",
    "        else:\n",
    "            create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv')\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Processes and downloads LibriSpeech dataset.')\n",
    "parser.add_argument(\"--target-dir\", default='LibriSpeech_dataset/', type=str, help=\"Directory to store the dataset.\")\n",
    "parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "parser.add_argument('--files-to-use', default=\"train-clean-100.tar.gz,\"\n",
    "                                              \"train-clean-360.tar.gz,train-other-500.tar.gz,\"\n",
    "                                              \"dev-clean.tar.gz,dev-other.tar.gz,\"\n",
    "                                              \"test-clean.tar.gz,test-other.tar.gz\", type=str,\n",
    "                    help='list of file names to download')\n",
    "parser.add_argument('--min-duration', default=1, type=int,\n",
    "                    help='Prunes training samples shorter than the min duration (given in seconds, default 1)')\n",
    "parser.add_argument('--max-duration', default=15, type=int,\n",
    "                    help='Prunes training samples longer than the max duration (given in seconds, default 15)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "LIBRI_SPEECH_URLS = {\n",
    "    \"train\": [\"http://www.openslr.org/resources/12/train-clean-100.tar.gz\",\n",
    "              \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\",\n",
    "              \"http://www.openslr.org/resources/12/train-other-500.tar.gz\"],\n",
    "\n",
    "    \"val\": [\"http://www.openslr.org/resources/12/dev-clean.tar.gz\",\n",
    "            \"http://www.openslr.org/resources/12/dev-other.tar.gz\"],\n",
    "\n",
    "    \"test_clean\": [\"http://www.openslr.org/resources/12/test-clean.tar.gz\"],\n",
    "    \"test_other\": [\"http://www.openslr.org/resources/12/test-other.tar.gz\"]\n",
    "}\n",
    "\n",
    "\n",
    "def _preprocess_transcript(phrase):\n",
    "    return phrase.strip().lower()\n",
    "\n",
    "\n",
    "def _process_file(wav_dir, txt_dir, base_filename, root_dir):\n",
    "    full_recording_path = os.path.join(root_dir, base_filename)\n",
    "    assert os.path.exists(full_recording_path) and os.path.exists(root_dir)\n",
    "    wav_recording_path = os.path.join(wav_dir, base_filename.replace(\".flac\", \".wav\"))\n",
    "#     subprocess.call([\"sox {}  -r {} -b 16 -c 1 {}\".format(full_recording_path, str(args.sample_rate),\n",
    "#                                                           wav_recording_path)], shell=True)\n",
    "    \n",
    "    subprocess.call([f\"sox {full_recording_path}  -r {str(args.sample_rate)} -b 16 -c 1 {wav_recording_path}\"], shell=True)\n",
    "    \n",
    "    # process transcript\n",
    "    txt_transcript_path = os.path.join(txt_dir, base_filename.replace(\".flac\", \".txt\"))\n",
    "    transcript_file = os.path.join(root_dir, \"-\".join(base_filename.split('-')[:-1]) + \".trans.txt\")\n",
    "    assert os.path.exists(transcript_file), \"Transcript file {} does not exist.\".format(transcript_file)\n",
    "    transcriptions = open(transcript_file).read().strip().split(\"\\n\")\n",
    "    transcriptions = {t.split()[0].split(\"-\")[-1]: \" \".join(t.split()[1:]) for t in transcriptions}\n",
    "    with open(txt_transcript_path, \"w\") as f:\n",
    "        key = base_filename.replace(\".flac\", \"\").split(\"-\")[-1]\n",
    "        assert key in transcriptions, \"{} is not in the transcriptions\".format(key)\n",
    "        f.write(_preprocess_transcript(transcriptions[key]))\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def main():\n",
    "    target_dl_dir = args.target_dir\n",
    "    if not os.path.exists(target_dl_dir):\n",
    "        os.makedirs(target_dl_dir)\n",
    "    files_to_dl = args.files_to_use.strip().split(',')\n",
    "    for split_type, lst_libri_urls in LIBRI_SPEECH_URLS.items():\n",
    "        split_dir = os.path.join(target_dl_dir, split_type)\n",
    "        if not os.path.exists(split_dir):\n",
    "            os.makedirs(split_dir)\n",
    "        split_wav_dir = os.path.join(split_dir, \"wav\")\n",
    "        if not os.path.exists(split_wav_dir):\n",
    "            os.makedirs(split_wav_dir)\n",
    "        split_txt_dir = os.path.join(split_dir, \"txt\")\n",
    "        if not os.path.exists(split_txt_dir):\n",
    "            os.makedirs(split_txt_dir)\n",
    "        extracted_dir = os.path.join(split_dir, \"LibriSpeech\")\n",
    "        if os.path.exists(extracted_dir):\n",
    "            shutil.rmtree(extracted_dir)\n",
    "        for url in lst_libri_urls:\n",
    "            # check if we want to dl this file\n",
    "            dl_flag = False\n",
    "            for f in files_to_dl:\n",
    "                if url.find(f) != -1:\n",
    "                    dl_flag = True\n",
    "            if not dl_flag:\n",
    "                print(\"Skipping url: {}\".format(url))\n",
    "                continue\n",
    "            filename = url.split(\"/\")[-1]\n",
    "            target_filename = os.path.join(split_dir, filename)\n",
    "            if not os.path.exists(target_filename):\n",
    "                wget.download(url, split_dir)\n",
    "            print(\"Unpacking {}...\".format(filename))\n",
    "            tar = tarfile.open(target_filename)\n",
    "            tar.extractall(split_dir)\n",
    "            tar.close()\n",
    "            os.remove(target_filename)\n",
    "            print(\"Converting flac files to wav and extracting transcripts...\")\n",
    "            assert os.path.exists(extracted_dir), \"Archive {} was not properly uncompressed.\".format(filename)\n",
    "            for root, subdirs, files in tqdm(os.walk(extracted_dir)):\n",
    "                for f in files:\n",
    "                    if f.find(\".flac\") != -1:\n",
    "                        _process_file(wav_dir=split_wav_dir, txt_dir=split_txt_dir,\n",
    "                                      base_filename=f, root_dir=root)\n",
    "\n",
    "            print(\"Finished {}\".format(url))\n",
    "            shutil.rmtree(extracted_dir)\n",
    "        if split_type == 'train':  # Prune to min/max duration\n",
    "            create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv', args.min_duration, args.max_duration)\n",
    "        else:\n",
    "            create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv')\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import fnmatch\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "SPECIAL_SPACE_CHARACTERS = ['\\n', '\\t', '\\r']\n",
    "\n",
    "def generate_label_from_corpora(corpus_paths, output_path=None, lower_case=True):\n",
    "    \"\"\"Generating label data from a given corpus folder file path(s)\n",
    "\n",
    "    This function will generate label data by performing character level tokenization \n",
    "    over all path in the specified `corpus_paths` and store the result as a json formatted \n",
    "    file in the specified `output_path`. If path is a folder, the label file will be generated \n",
    "    based on all files with `.txt` format inside the given path recursively.\n",
    "\n",
    "    Args:\n",
    "        corpus_paths (list[str]): list of file or folder path of the text corpus\n",
    "        output_path (str): file path for the generated label file\n",
    "        lower_case (bool): flag for performing lower case on the text data\n",
    "\n",
    "    Returns:\n",
    "        list(str): The label list generated from the given `corpus_paths`\n",
    "    \"\"\"\n",
    "    \n",
    "    label_set = set()\n",
    "    for corpus_path in corpus_paths:\n",
    "        label_set |= retrieve_label_from_corpus(corpus_path, lower_case)\n",
    "    label_list = list(label_set)\n",
    "\n",
    "    if output_path:\n",
    "        with open(output_path, 'w') as outfile:\n",
    "            json.dump(label_list, outfile, ensure_ascii=False)\n",
    "\n",
    "    return label_list\n",
    "\n",
    "def retrieve_label_from_corpus(corpus_path, lower_case=True):\n",
    "    \"\"\"Retrieve all unique character labels from a given corpus folder or file path\n",
    "\n",
    "    This function will generate json label file by performing character level \n",
    "    tokenization over `corpus_path`. If `corpus_`path is a folder, the character labels\n",
    "    will be retrieved from  all files with `.txt` format inside the given `corpus_path`.\n",
    "\n",
    "    Args:\n",
    "        corpus_path (str): list of file or folder path of the text corpus\n",
    "        lower_case (bool): flag for performing lower case on the text data\n",
    "\n",
    "    Returns:\n",
    "        set(str): The return character labels\n",
    "    \"\"\"\n",
    "    label_set = set()\n",
    "    if os.path.isdir(corpus_path):\n",
    "        # Recursive search over folder\n",
    "        for f_path in os.listdir(corpus_path):\n",
    "            f_path = '{}/{}'.format(corpus_path, f_path)\n",
    "            if  os.path.isdir(f_path) or f_path[-4:] == '.txt':\n",
    "                label_set |= retrieve_label_from_corpus(f_path)\n",
    "            else:\n",
    "                # Skip non-folder and non-txt file\n",
    "                pass\n",
    "    elif corpus_path[-4:] == '.txt':\n",
    "        # Perform character level tokenization over corpus\n",
    "        with open(corpus_path,'r') as corpus_file:\n",
    "            data = corpus_file.read()\n",
    "\n",
    "            # Turn special character to space\n",
    "            for c in SPECIAL_SPACE_CHARACTERS:\n",
    "                data = data.replace(c,' ')\n",
    "\n",
    "            # Perform lower case if needed\n",
    "            if lower_case:\n",
    "                data = data.lower()\n",
    "\n",
    "            # Add to result set\n",
    "            label_set |= set(data)\n",
    "    else:\n",
    "        # Skip non-folder and non-txt file\n",
    "        pass\n",
    "    return label_set\n",
    "\n",
    "def create_manifest(data_path, output_path, min_duration=None, max_duration=None):\n",
    "    file_paths = [os.path.join(dirpath, f)\n",
    "                  for dirpath, dirnames, files in os.walk(data_path)\n",
    "                  for f in fnmatch.filter(files, '*.wav')]\n",
    "    file_paths = order_and_prune_files(file_paths, min_duration, max_duration)\n",
    "    with io.FileIO(output_path, \"w\") as file:\n",
    "        for wav_path in tqdm(file_paths, total=len(file_paths)):\n",
    "            transcript_path = wav_path.replace('/wav/', '/txt/').replace('.wav', '.txt')\n",
    "            sample = os.path.abspath(wav_path) + ',' + os.path.abspath(transcript_path) + '\\n'\n",
    "            file.write(sample.encode('utf-8'))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def order_and_prune_files(file_paths, min_duration, max_duration):\n",
    "    print(\"Sorting manifests...\")\n",
    "    duration_file_paths = [(path, float(subprocess.check_output(\n",
    "        ['soxi -D \\\"%s\\\"' % path.strip()], shell=True))) for path in file_paths]\n",
    "    if min_duration and max_duration:\n",
    "        print(\"Pruning manifests between %d and %d seconds\" % (min_duration, max_duration))\n",
    "        duration_file_paths = [(path, duration) for path, duration in duration_file_paths if\n",
    "                               min_duration <= duration <= max_duration]\n",
    "\n",
    "    def func(element):\n",
    "        return element[1]\n",
    "\n",
    "    duration_file_paths.sort(key=func)\n",
    "    return [x[0] for x in duration_file_paths]  # Remove durations\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Test for generating label file\n",
    "#     print('Test Gen Label File')\n",
    "#     print(generate_label_from_corpora(['./test.txt'], output_path=None, lower_case=True))\n",
    "\n",
    "#     print('Test Gen Label File No Lower Case')\n",
    "#     print(generate_label_from_corpora(['./test.txt'], output_path=None, lower_case=False))\n",
    "\n",
    "#     print('Test Gen Label File to Json file')\n",
    "#     print(generate_label_from_corpora(['./test.txt'], output_path='./label_file.json', lower_case=True))\n",
    "\n",
    "#     print('Test Gen Label Folder')\n",
    "#     print(generate_label_from_corpora(['./test.txt', './test_folder'], output_path=None, lower_case=True))\n",
    "\n",
    "#     print('Test Gen Label Folder No Lower Case')\n",
    "#     print(generate_label_from_corpora(['./test.txt', './test_folder'], output_path=None, lower_case=False))\n",
    "\n",
    "#     print('Test Gen Label Folder to Json file')\n",
    "#     print(generate_label_from_corpora(['./test.txt', './test_folder'], output_path='./label_folder.json', lower_case=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as I\n",
    "\n",
    "\"\"\"\n",
    "General purpose functions\n",
    "\"\"\"\n",
    "\n",
    "def pad_list(xs, pad_value):\n",
    "    # From: espnet/src/nets/e2e_asr_th.py: pad_list()\n",
    "    n_batch = len(xs)\n",
    "    # max_len = max(x.size(0) for x in xs)\n",
    "    max_len = constant.args.tgt_max_len\n",
    "    pad = xs[0].new(n_batch, max_len, * xs[0].size()[1:]).fill_(pad_value)\n",
    "    for i in range(n_batch):\n",
    "        pad[i, :xs[i].size(0)] = xs[i]\n",
    "    return pad\n",
    "\n",
    "\"\"\" \n",
    "Transformer common layers\n",
    "\"\"\"\n",
    "\n",
    "def get_non_pad_mask(padded_input, input_lengths=None, pad_idx=None):\n",
    "    \"\"\"\n",
    "    padding position is set to 0, either use input_lengths or pad_idx\n",
    "    \"\"\"\n",
    "    assert input_lengths is not None or pad_idx is not None\n",
    "    if input_lengths is not None:\n",
    "        # padded_input: N x T x ..\n",
    "        N = padded_input.size(0)\n",
    "        non_pad_mask = padded_input.new_ones(padded_input.size()[:-1])  # B x T\n",
    "        for i in range(N):\n",
    "            non_pad_mask[i, input_lengths[i]:] = 0\n",
    "    if pad_idx is not None:\n",
    "        # padded_input: N x T\n",
    "        assert padded_input.dim() == 2\n",
    "        non_pad_mask = padded_input.ne(pad_idx).float()\n",
    "    # unsqueeze(-1) for broadcast\n",
    "    return non_pad_mask.unsqueeze(-1)\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q, pad_idx):\n",
    "    \"\"\"\n",
    "    For masking out the padding part of key sequence.\n",
    "    \"\"\"\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = seq_k.eq(pad_idx)\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # B x T_Q x T_K\n",
    "\n",
    "    return padding_mask\n",
    "\n",
    "def get_attn_pad_mask(padded_input, input_lengths, expand_length):\n",
    "    \"\"\"mask position is set to 1\"\"\"\n",
    "    # N x Ti x 1\n",
    "    non_pad_mask = get_non_pad_mask(padded_input, input_lengths=input_lengths)\n",
    "    # N x Ti, lt(1) like not operation\n",
    "    pad_mask = non_pad_mask.squeeze(-1).lt(1)\n",
    "    attn_mask = pad_mask.unsqueeze(1).expand(-1, expand_length, -1)\n",
    "    return attn_mask\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = torch.triu(\n",
    "        torch.ones((len_s, len_s), device=seq.device, dtype=torch.uint8), diagonal=1)\n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n",
    "\n",
    "    return subsequent_mask\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding class\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, max_length=2000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_length, dim_model, requires_grad=False)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "        exp_term = torch.exp(torch.arange(0, dim_model, 2).float() * -(math.log(10000.0) / dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * exp_term) # take the odd (jump by 2)\n",
    "        pe[:, 1::2] = torch.cos(position * exp_term) # take the even (jump by 2)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            input: B x T x D\n",
    "        output:\n",
    "            tensor: B x T\n",
    "        \"\"\"\n",
    "        return self.pe[:, :input.size(1)]\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feedforward Layer class\n",
    "    FFN(x) = max(0, xW1 + b1) W2+ b2\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, dim_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(dim_model, dim_ff)\n",
    "        self.linear_2 = nn.Linear(dim_ff, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: tensor\n",
    "        output:\n",
    "            y: tensor\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        output = self.dropout(self.linear_2(F.relu(self.linear_1(x))))\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output\n",
    "\n",
    "class PositionwiseFeedForwardWithConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feedforward Layer Implementation with Convolution class\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, dim_hidden, dropout=0.1):\n",
    "        super(PositionwiseFeedForwardWithConv, self).__init__()\n",
    "        self.conv_1 = nn.Conv1d(dim_model, dim_hidden, 1)\n",
    "        self.conv_2 = nn.Conv1d(dim_hidden, dim_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.conv_2(F.relu(self.conv_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_model, dim_key, dim_value, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_value = dim_value\n",
    "\n",
    "        self.query_linear = nn.Linear(dim_model, num_heads * dim_key)\n",
    "        self.key_linear = nn.Linear(dim_model, num_heads * dim_key)\n",
    "        self.value_linear = nn.Linear(dim_model, num_heads * dim_value)\n",
    "\n",
    "        nn.init.normal_(self.query_linear.weight, mean=0, std=np.sqrt(2.0 / (self.dim_model + self.dim_key)))\n",
    "        nn.init.normal_(self.key_linear.weight, mean=0, std=np.sqrt(2.0 / (self.dim_model + self.dim_key)))\n",
    "        nn.init.normal_(self.value_linear.weight, mean=0, std=np.sqrt(2.0 / (self.dim_model + self.dim_value)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(dim_key, 0.5), attn_dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "        self.output_linear = nn.Linear(num_heads * dim_value, dim_model)\n",
    "        nn.init.xavier_normal_(self.output_linear.weight)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: B x T_Q x H, key: B x T_K x H, value: B x T_V x H\n",
    "        mask: B x T x T (attention mask)\n",
    "        \"\"\"\n",
    "        batch_size, len_query, _ = query.size()\n",
    "        batch_size, len_key, _ = key.size()\n",
    "        batch_size, len_value, _ = value.size()\n",
    "\n",
    "        residual = query\n",
    "\n",
    "        query = self.query_linear(query).view(batch_size, len_query, self.num_heads, self.dim_key) # B x T_Q x num_heads x H_K\n",
    "        key = self.key_linear(key).view(batch_size, len_key, self.num_heads, self.dim_key) # B x T_K x num_heads x H_K\n",
    "        value = self.value_linear(value).view(batch_size, len_value, self.num_heads, self.dim_value) # B x T_V x num_heads x H_V\n",
    "\n",
    "        query = query.permute(2, 0, 1, 3).contiguous().view(-1, len_query, self.dim_key) # (num_heads * B) x T_Q x H_K\n",
    "        key = key.permute(2, 0, 1, 3).contiguous().view(-1, len_key, self.dim_key) # (num_heads * B) x T_K x H_K\n",
    "        value = value.permute(2, 0, 1, 3).contiguous().view(-1, len_value, self.dim_value) # (num_heads * B) x T_V x H_V\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(self.num_heads, 1, 1) # (B * num_head) x T x T\n",
    "        \n",
    "        output, attn = self.attention(query, key, value, mask=mask)\n",
    "\n",
    "        output = output.view(self.num_heads, batch_size, len_query, self.dim_value) # num_heads x B x T_Q x H_V\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(batch_size, len_query, -1) # B x T_Q x (num_heads * H_V)\n",
    "\n",
    "        output = self.dropout(self.output_linear(output)) # B x T_Q x H_O\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\"\"\"\n",
    "LAS common layers\n",
    "\"\"\"\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Dot product attention.\n",
    "    Given a set of vector values, and a vector query, attention is a technique\n",
    "    to compute a weighted sum of the values, dependent on the query.\n",
    "    NOTE: Here we use the terminology in Stanford cs224n-2018-lecture11.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        # TODO: move this out of this class?\n",
    "        # self.linear_out = nn.Linear(dim*2, dim)\n",
    "\n",
    "    def forward(self, queries, values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries: N x To x H\n",
    "            values : N x Ti x H\n",
    "        Returns:\n",
    "            output: N x To x H\n",
    "            attention_distribution: N x To x Ti\n",
    "        \"\"\"\n",
    "        batch_size = queries.size(0)\n",
    "        hidden_size = queries.size(2)\n",
    "        input_lengths = values.size(1)\n",
    "        # (N, To, H) * (N, H, Ti) -> (N, To, Ti)\n",
    "        attention_scores = torch.bmm(queries, values.transpose(1, 2))\n",
    "        attention_distribution = F.softmax(\n",
    "            attention_scores.view(-1, input_lengths), dim=1).view(batch_size, -1, input_lengths)\n",
    "        # (N, To, Ti) * (N, Ti, H) -> (N, To, H)\n",
    "        attention_output = torch.bmm(attention_distribution, values)\n",
    "        # # concat -> (N, To, 2*H)\n",
    "        # concated = torch.cat((attention_output, queries), dim=2)\n",
    "        # # TODO: Move this out of this class?\n",
    "        # # output -> (N, To, H)\n",
    "        # output = torch.tanh(self.linear_out(\n",
    "        #     concated.view(-1, 2*hidden_size))).view(batch_size, -1, hidden_size)\n",
    "\n",
    "        return attention_output, attention_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer class\n",
    "    args:\n",
    "        encoder: Encoder object\n",
    "        decoder: Decoder object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, feat_extractor='vgg_cnn'):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.id2label = decoder.id2label\n",
    "        self.feat_extractor = feat_extractor\n",
    "\n",
    "        # feature embedding\n",
    "        if feat_extractor == 'emb_cnn':\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(0, 10)),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.Hardtanh(0, 20, inplace=True),\n",
    "                nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), ),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.Hardtanh(0, 20, inplace=True)\n",
    "            )\n",
    "        elif feat_extractor == 'vgg_cnn':\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, stride=2)\n",
    "            )\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, padded_input, input_lengths, padded_target, verbose=False):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            padded_input: B x 1 (channel for spectrogram=1) x (freq) x T\n",
    "            padded_input: B x T x D\n",
    "            input_lengths: B\n",
    "            padded_target: B x T\n",
    "        output:\n",
    "            pred: B x T x vocab\n",
    "            gold: B x T\n",
    "        \"\"\"\n",
    "        if self.feat_extractor == 'emb_cnn' or self.feat_extractor == 'vgg_cnn':\n",
    "            padded_input = self.conv(padded_input)\n",
    "\n",
    "        # Reshaping features\n",
    "        sizes = padded_input.size() # B x H_1 (channel?) x H_2 x T\n",
    "        padded_input = padded_input.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
    "        padded_input = padded_input.transpose(1, 2).contiguous()  # BxTxH\n",
    "\n",
    "        encoder_padded_outputs, _ = self.encoder(padded_input, input_lengths)\n",
    "        pred, gold, *_ = self.decoder(padded_target, encoder_padded_outputs, input_lengths)\n",
    "        hyp_best_scores, hyp_best_ids = torch.topk(pred, 1, dim=2)\n",
    "\n",
    "        hyp_seq = hyp_best_ids.squeeze(2)\n",
    "        gold_seq = gold\n",
    "\n",
    "        return pred, gold, hyp_seq, gold_seq\n",
    "\n",
    "    def evaluate(self, padded_input, input_lengths, padded_target, beam_search=False, beam_width=0, beam_nbest=0, lm=None, lm_rescoring=False, lm_weight=0.1, c_weight=1, verbose=False):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            padded_input: B x T x D\n",
    "            input_lengths: B\n",
    "            padded_target: B x T\n",
    "        output:\n",
    "            batch_ids_nbest_hyps: list of nbest id\n",
    "            batch_strs_nbest_hyps: list of nbest str\n",
    "            batch_strs_gold: list of gold str\n",
    "        \"\"\"\n",
    "        if self.feat_extractor == 'emb_cnn' or self.feat_extractor == 'vgg_cnn':\n",
    "            padded_input = self.conv(padded_input)\n",
    "\n",
    "        # Reshaping features\n",
    "        sizes = padded_input.size() # B x H_1 (channel?) x H_2 x T\n",
    "        padded_input = padded_input.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
    "        padded_input = padded_input.transpose(1, 2).contiguous()  # BxTxH\n",
    "\n",
    "        encoder_padded_outputs, _ = self.encoder(padded_input, input_lengths)\n",
    "        hyp, gold, *_ = self.decoder(padded_target, encoder_padded_outputs, input_lengths)\n",
    "        hyp_best_scores, hyp_best_ids = torch.topk(hyp, 1, dim=2)\n",
    "        \n",
    "        strs_gold = [\"\".join([self.id2label[int(x)] for x in gold_seq]) for gold_seq in gold]\n",
    "\n",
    "        if beam_search:\n",
    "            ids_hyps, strs_hyps = self.decoder.beam_search(encoder_padded_outputs, beam_width=beam_width, nbest=1, lm=lm, lm_rescoring=lm_rescoring, lm_weight=lm_weight, c_weight=c_weight)\n",
    "            if len(strs_hyps) != sizes[0]:\n",
    "                print(\">>>>>>> switch to greedy\")\n",
    "                strs_hyps = self.decoder.greedy_search(encoder_padded_outputs)\n",
    "        else:\n",
    "            strs_hyps = self.decoder.greedy_search(encoder_padded_outputs)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"GOLD\", strs_gold)\n",
    "            print(\"HYP\", strs_hyps)\n",
    "\n",
    "        return _, strs_hyps, strs_gold\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder Transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, num_heads, dim_model, dim_key, dim_value, dim_input, dim_inner, dropout=0.1, src_max_length=2500):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.dim_input = dim_input\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_value = dim_value\n",
    "        self.dim_inner = dim_inner\n",
    "\n",
    "        self.src_max_length = src_max_length\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        self.input_linear = nn.Linear(dim_input, dim_model)\n",
    "        self.layer_norm_input = nn.LayerNorm(dim_model)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            dim_model, src_max_length)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(num_heads, dim_model, dim_inner, dim_key, dim_value, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, padded_input, input_lengths):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            padded_input: B x T x D\n",
    "            input_lengths: B\n",
    "        return:\n",
    "            output: B x T x H\n",
    "        \"\"\"\n",
    "        encoder_self_attn_list = []\n",
    "\n",
    "        # Prepare masks\n",
    "        non_pad_mask = get_non_pad_mask(padded_input, input_lengths=input_lengths)  # B x T x D\n",
    "        seq_len = padded_input.size(1)\n",
    "        self_attn_mask = get_attn_pad_mask(padded_input, input_lengths, seq_len)  # B x T x T\n",
    "\n",
    "        encoder_output = self.layer_norm_input(self.input_linear(\n",
    "            padded_input)) + self.positional_encoding(padded_input)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            encoder_output, self_attn = layer(\n",
    "                encoder_output, non_pad_mask=non_pad_mask, self_attn_mask=self_attn_mask)\n",
    "            encoder_self_attn_list += [self_attn]\n",
    "\n",
    "        return encoder_output, encoder_self_attn_list\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Layer Transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, dim_model, dim_inner, dim_key, dim_value, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            num_heads, dim_model, dim_key, dim_value, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForwardWithConv(\n",
    "            dim_model, dim_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, non_pad_mask=None, self_attn_mask=None):\n",
    "        enc_output, self_attn = self.self_attn(\n",
    "            enc_input, enc_input, enc_input, mask=self_attn_mask)\n",
    "        enc_output *= non_pad_mask\n",
    "\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        enc_output *= non_pad_mask\n",
    "\n",
    "        return enc_output, self_attn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Layer Transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, id2label, num_src_vocab, num_trg_vocab, num_layers, num_heads, dim_emb, dim_model, dim_inner, dim_key, dim_value, dropout=0.1, trg_max_length=1000, emb_trg_sharing=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.sos_id = constant.SOS_TOKEN\n",
    "        self.eos_id = constant.EOS_TOKEN\n",
    "\n",
    "        self.id2label = id2label\n",
    "\n",
    "        self.num_src_vocab = num_src_vocab\n",
    "        self.num_trg_vocab = num_trg_vocab\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_inner = dim_inner\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_value = dim_value\n",
    "\n",
    "        self.dropout_rate = dropout\n",
    "        self.emb_trg_sharing = emb_trg_sharing\n",
    "\n",
    "        self.trg_max_length = trg_max_length\n",
    "\n",
    "        self.trg_embedding = nn.Embedding(num_trg_vocab, dim_emb, padding_idx=constant.PAD_TOKEN)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            dim_model, max_length=trg_max_length)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(dim_model, dim_inner, num_heads,\n",
    "                         dim_key, dim_value, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_linear = nn.Linear(dim_model, num_trg_vocab, bias=False)\n",
    "        nn.init.xavier_normal_(self.output_linear.weight)\n",
    "\n",
    "        if emb_trg_sharing:\n",
    "            self.output_linear.weight = self.trg_embedding.weight\n",
    "            self.x_logit_scale = (dim_model ** -0.5)\n",
    "        else:\n",
    "            self.x_logit_scale = 1.0\n",
    "\n",
    "    def preprocess(self, padded_input):\n",
    "        \"\"\"\n",
    "        Add SOS TOKEN and EOS TOKEN into padded_input\n",
    "        \"\"\"\n",
    "        seq = [y[y != constant.PAD_TOKEN] for y in padded_input]\n",
    "        eos = seq[0].new([self.eos_id])\n",
    "        sos = seq[0].new([self.sos_id])\n",
    "        seq_in = [torch.cat([sos, y], dim=0) for y in seq]\n",
    "        seq_out = [torch.cat([y, eos], dim=0) for y in seq]\n",
    "        seq_in_pad = pad_list(seq_in, self.eos_id)\n",
    "        seq_out_pad = pad_list(seq_out, constant.PAD_TOKEN)\n",
    "        assert seq_in_pad.size() == seq_out_pad.size()\n",
    "        return seq_in_pad, seq_out_pad\n",
    "\n",
    "    def forward(self, padded_input, encoder_padded_outputs, encoder_input_lengths):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            padded_input: B x T\n",
    "            encoder_padded_outputs: B x T x H\n",
    "            encoder_input_lengths: B\n",
    "        returns:\n",
    "            pred: B x T x vocab\n",
    "            gold: B x T\n",
    "        \"\"\"\n",
    "        decoder_self_attn_list, decoder_encoder_attn_list = [], []\n",
    "        seq_in_pad, seq_out_pad = self.preprocess(padded_input)\n",
    "\n",
    "        # Prepare masks\n",
    "        non_pad_mask = get_non_pad_mask(seq_in_pad, pad_idx=constant.EOS_TOKEN)\n",
    "        self_attn_mask_subseq = get_subsequent_mask(seq_in_pad)\n",
    "        self_attn_mask_keypad = get_attn_key_pad_mask(\n",
    "            seq_k=seq_in_pad, seq_q=seq_in_pad, pad_idx=constant.EOS_TOKEN)\n",
    "        self_attn_mask = (self_attn_mask_keypad + self_attn_mask_subseq).gt(0)\n",
    "\n",
    "        output_length = seq_in_pad.size(1)\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(\n",
    "            encoder_padded_outputs, encoder_input_lengths, output_length)\n",
    "\n",
    "        decoder_output = self.dropout(self.trg_embedding(\n",
    "            seq_in_pad) * self.x_logit_scale + self.positional_encoding(seq_in_pad))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            decoder_output, decoder_self_attn, decoder_enc_attn = layer(\n",
    "                decoder_output, encoder_padded_outputs, non_pad_mask=non_pad_mask, self_attn_mask=self_attn_mask, dec_enc_attn_mask=dec_enc_attn_mask)\n",
    "\n",
    "            decoder_self_attn_list += [decoder_self_attn]\n",
    "            decoder_encoder_attn_list += [decoder_enc_attn]\n",
    "\n",
    "        seq_logit = self.output_linear(decoder_output)\n",
    "        pred, gold = seq_logit, seq_out_pad\n",
    "\n",
    "        return pred, gold, decoder_self_attn_list, decoder_encoder_attn_list\n",
    "\n",
    "    def post_process_hyp(self, hyp):\n",
    "        \"\"\"\n",
    "        args: \n",
    "            hyp: list of hypothesis\n",
    "        output:\n",
    "            list of hypothesis (string)>\n",
    "        \"\"\"\n",
    "        return \"\".join([self.id2label[int(x)] for x in hyp['yseq'][1:]])\n",
    "\n",
    "    def greedy_search(self, encoder_padded_outputs, beam_width=2, lm_rescoring=False, lm=None, lm_weight=0.1, c_weight=1):\n",
    "        \"\"\"\n",
    "        Greedy search, decode 1-best utterance\n",
    "        args:\n",
    "            encoder_padded_outputs: B x T x H\n",
    "        output:\n",
    "            batch_ids_nbest_hyps: list of nbest in ids (size B)\n",
    "            batch_strs_nbest_hyps: list of nbest in strings (size B)\n",
    "        \"\"\"\n",
    "        max_seq_len = self.trg_max_length\n",
    "        \n",
    "        ys = torch.ones(encoder_padded_outputs.size(0),1).fill_(constant.SOS_TOKEN).long() # batch_size x 1\n",
    "        if constant.args.cuda:\n",
    "            ys = ys.cuda()\n",
    "\n",
    "        decoded_words = []\n",
    "        for t in range(300):\n",
    "        # for t in range(max_seq_len):\n",
    "            # print(t)\n",
    "            # Prepare masks\n",
    "            non_pad_mask = torch.ones_like(ys).float().unsqueeze(-1) # batch_size x t x 1\n",
    "            self_attn_mask = get_subsequent_mask(ys) # batch_size x t x t\n",
    "\n",
    "            decoder_output = self.dropout(self.trg_embedding(ys) * self.x_logit_scale \n",
    "                                        + self.positional_encoding(ys))\n",
    "\n",
    "            for layer in self.layers:\n",
    "                decoder_output, _, _ = layer(\n",
    "                    decoder_output, encoder_padded_outputs,\n",
    "                    non_pad_mask=non_pad_mask,\n",
    "                    self_attn_mask=self_attn_mask,\n",
    "                    dec_enc_attn_mask=None\n",
    "                )\n",
    "\n",
    "            prob = self.output_linear(decoder_output) # batch_size x t x label_size\n",
    "            # _, next_word = torch.max(prob[:, -1], dim=1)\n",
    "            # decoded_words.append([constant.EOS_CHAR if ni.item() == constant.EOS_TOKEN else self.id2label[ni.item()] for ni in next_word.view(-1)])\n",
    "            # next_word = next_word.unsqueeze(-1)\n",
    "\n",
    "            # local_best_scores, local_best_ids = torch.topk(local_scores, beam_width, dim=1)\n",
    "\n",
    "            if lm_rescoring:\n",
    "                local_scores = F.log_softmax(prob, dim=1)\n",
    "                local_best_scores, local_best_ids = torch.topk(local_scores, beam_width, dim=1)\n",
    "\n",
    "                best_score = -1\n",
    "                best_word = None\n",
    "\n",
    "                # calculate beam scores\n",
    "                for j in range(beam_width):\n",
    "                    cur_seq = \" \".join(word for word in decoded_words)\n",
    "                    lm_score, num_words, oov_token = calculate_lm_score(cur_seq, lm, self.id2label)\n",
    "                    score = local_best_scores[0, j] + lm_score\n",
    "                    if best_score < score:\n",
    "                        best_score = score\n",
    "                        best_word = local_best_ids[0, j]\n",
    "                        next_word = best_word.unsqueeze(-1)\n",
    "                decoded_words.append(self.id2label[int(best_word)])\n",
    "            else:\n",
    "                _, next_word = torch.max(prob[:, -1], dim=1)\n",
    "                decoded_words.append([constant.EOS_CHAR if ni.item() == constant.EOS_TOKEN else self.id2label[ni.item()] for ni in next_word.view(-1)])\n",
    "                next_word = next_word.unsqueeze(-1)\n",
    "\n",
    "            if constant.args.cuda:\n",
    "                ys = torch.cat([ys, next_word.cuda()], dim=1)\n",
    "                ys = ys.cuda()\n",
    "            else:\n",
    "                ys = torch.cat([ys, next_word], dim=1)\n",
    "\n",
    "        sent = []\n",
    "        for _, row in enumerate(np.transpose(decoded_words)):\n",
    "            st = ''\n",
    "            for e in row:\n",
    "                if e == constant.EOS_CHAR: \n",
    "                    break\n",
    "                else: \n",
    "                    st += e\n",
    "            sent.append(st)\n",
    "        return sent\n",
    "\n",
    "    def beam_search(self, encoder_padded_outputs, beam_width=2, nbest=5, lm_rescoring=False, lm=None, lm_weight=0.1, c_weight=1, prob_weight=1.0):\n",
    "        \"\"\"\n",
    "        Beam search, decode nbest utterances\n",
    "        args:\n",
    "            encoder_padded_outputs: B x T x H\n",
    "            beam_size: int\n",
    "            nbest: int\n",
    "        output:\n",
    "            batch_ids_nbest_hyps: list of nbest in ids (size B)\n",
    "            batch_strs_nbest_hyps: list of nbest in strings (size B)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_padded_outputs.size(0)\n",
    "        max_len = encoder_padded_outputs.size(1)\n",
    "\n",
    "        batch_ids_nbest_hyps = []\n",
    "        batch_strs_nbest_hyps = []\n",
    "\n",
    "        for x in range(batch_size):\n",
    "            encoder_output = encoder_padded_outputs[x].unsqueeze(0) # 1 x T x H\n",
    "\n",
    "            # add SOS_TOKEN\n",
    "            ys = torch.ones(1, 1).fill_(constant.SOS_TOKEN).type_as(encoder_output).long()\n",
    "            \n",
    "            hyp = {'score': 0.0, 'yseq':ys}\n",
    "            hyps = [hyp]\n",
    "            ended_hyps = []\n",
    "\n",
    "            for i in range(300):\n",
    "            # for i in range(self.trg_max_length):\n",
    "                hyps_best_kept = []\n",
    "                for hyp in hyps:\n",
    "                    ys = hyp['yseq'] # 1 x i\n",
    "\n",
    "                    # Prepare masks\n",
    "                    non_pad_mask = torch.ones_like(ys).float().unsqueeze(-1) # 1xix1\n",
    "                    self_attn_mask = get_subsequent_mask(ys)\n",
    "\n",
    "                    decoder_output = self.dropout(self.trg_embedding(ys) * self.x_logit_scale \n",
    "                                                + self.positional_encoding(ys))\n",
    "\n",
    "                    for layer in self.layers:\n",
    "                        # print(decoder_output.size(), encoder_output.size())\n",
    "                        decoder_output, _, _ = layer(\n",
    "                            decoder_output, encoder_output,\n",
    "                            non_pad_mask=non_pad_mask,\n",
    "                            self_attn_mask=self_attn_mask,\n",
    "                            dec_enc_attn_mask=None\n",
    "                        )\n",
    "\n",
    "                    seq_logit = self.output_linear(decoder_output[:, -1])\n",
    "                    local_scores = F.log_softmax(seq_logit, dim=1)\n",
    "                    local_best_scores, local_best_ids = torch.topk(local_scores, beam_width, dim=1)\n",
    "\n",
    "                    # calculate beam scores\n",
    "                    for j in range(beam_width):\n",
    "                        new_hyp = {}\n",
    "                        new_hyp[\"score\"] = hyp[\"score\"] + local_best_scores[0, j]\n",
    "\n",
    "                        new_hyp[\"yseq\"] = torch.ones(1, (1+ys.size(1))).type_as(encoder_output).long()\n",
    "                        new_hyp[\"yseq\"][:, :ys.size(1)] = hyp[\"yseq\"].cpu()\n",
    "                        new_hyp[\"yseq\"][:, ys.size(1)] = int(local_best_ids[0, j]) # adding new word\n",
    "                        \n",
    "                        hyps_best_kept.append(new_hyp)\n",
    "\n",
    "                    hyps_best_kept = sorted(hyps_best_kept, key=lambda x:x[\"score\"], reverse=True)[:beam_width]\n",
    "                \n",
    "                hyps = hyps_best_kept\n",
    "\n",
    "                # add EOS_TOKEN\n",
    "                if i == max_len - 1:\n",
    "                    for hyp in hyps:\n",
    "                        hyp[\"yseq\"] = torch.cat([hyp[\"yseq\"], torch.ones(1,1).fill_(constant.EOS_TOKEN).type_as(encoder_output).long()], dim=1)\n",
    "\n",
    "                # add hypothesis that have EOS_TOKEN to ended_hyps list\n",
    "                unended_hyps = []\n",
    "                for hyp in hyps:\n",
    "                    if hyp[\"yseq\"][0, -1] == constant.EOS_TOKEN:\n",
    "                        if lm_rescoring:\n",
    "                            # seq_str = \"\".join(self.id2label[char.item()] for char in hyp[\"yseq\"][0]).replace(constant.PAD_CHAR,\"\").replace(constant.SOS_CHAR,\"\").replace(constant.EOS_CHAR,\"\")\n",
    "                            # seq_str = seq_str.replace(\"  \", \" \")\n",
    "                            # num_words = len(seq_str.split())\n",
    "\n",
    "                            hyp[\"lm_score\"], hyp[\"num_words\"], oov_token = calculate_lm_score(hyp[\"yseq\"], lm, self.id2label)\n",
    "                            num_words = hyp[\"num_words\"]\n",
    "                            hyp[\"lm_score\"] -= oov_token * 2\n",
    "                            hyp[\"final_score\"] = hyp[\"score\"] + lm_weight * hyp[\"lm_score\"] + math.sqrt(num_words) * c_weight\n",
    "                        else:\n",
    "                            seq_str = \"\".join(self.id2label[char.item()] for char in hyp[\"yseq\"][0]).replace(constant.PAD_CHAR,\"\").replace(constant.SOS_CHAR,\"\").replace(constant.EOS_CHAR,\"\")\n",
    "                            seq_str = seq_str.replace(\"  \", \" \")\n",
    "                            num_words = len(seq_str.split())\n",
    "                            hyp[\"final_score\"] = hyp[\"score\"] + math.sqrt(num_words) * c_weight\n",
    "                        \n",
    "                        ended_hyps.append(hyp)\n",
    "                        \n",
    "                    else:\n",
    "                        unended_hyps.append(hyp)\n",
    "                hyps = unended_hyps\n",
    "\n",
    "                if len(hyps) == 0:\n",
    "                    # decoding process is finished\n",
    "                    break\n",
    "                \n",
    "            num_nbest = min(len(ended_hyps), nbest)\n",
    "            nbest_hyps = sorted(ended_hyps, key=lambda x:x[\"final_score\"], reverse=True)[:num_nbest]\n",
    "\n",
    "            a_nbest_hyps = sorted(ended_hyps, key=lambda x:x[\"final_score\"], reverse=True)[:beam_width]\n",
    "\n",
    "            if lm_rescoring:\n",
    "                for hyp in a_nbest_hyps:\n",
    "                    seq_str = \"\".join(self.id2label[char.item()] for char in hyp[\"yseq\"][0]).replace(constant.PAD_CHAR,\"\").replace(constant.SOS_CHAR,\"\").replace(constant.EOS_CHAR,\"\")\n",
    "                    seq_str = seq_str.replace(\"  \", \" \")\n",
    "                    num_words = len(seq_str.split())\n",
    "                    # print(\"{}  || final:{} e2e:{} lm:{} num words:{}\".format(seq_str, hyp[\"final_score\"], hyp[\"score\"], hyp[\"lm_score\"], hyp[\"num_words\"]))\n",
    "\n",
    "            for hyp in nbest_hyps:                \n",
    "                hyp[\"yseq\"] = hyp[\"yseq\"][0].cpu().numpy().tolist()\n",
    "                hyp_strs = self.post_process_hyp(hyp)\n",
    "\n",
    "                batch_ids_nbest_hyps.append(hyp[\"yseq\"])\n",
    "                batch_strs_nbest_hyps.append(hyp_strs)\n",
    "                # print(hyp[\"yseq\"], hyp_strs)\n",
    "        return batch_ids_nbest_hyps, batch_strs_nbest_hyps\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_model, dim_inner, num_heads, dim_key, dim_value, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            num_heads, dim_model, dim_key, dim_value, dropout=dropout)\n",
    "        self.encoder_attn = MultiHeadAttention(\n",
    "            num_heads, dim_model, dim_key, dim_value, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForwardWithConv(\n",
    "            dim_model, dim_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, decoder_input, encoder_output, non_pad_mask=None, self_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        decoder_output, decoder_self_attn = self.self_attn(\n",
    "            decoder_input, decoder_input, decoder_input, mask=self_attn_mask)\n",
    "        decoder_output *= non_pad_mask\n",
    "\n",
    "        decoder_output, decoder_encoder_attn = self.encoder_attn(\n",
    "            decoder_output, encoder_output, encoder_output, mask=dec_enc_attn_mask)\n",
    "        decoder_output *= non_pad_mask\n",
    "\n",
    "        decoder_output = self.pos_ffn(decoder_output)\n",
    "        decoder_output *= non_pad_mask\n",
    "\n",
    "        return decoder_output, decoder_self_attn, decoder_encoder_attn        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\"\n",
    "    Trainer class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        logging.info(\"Trainer is initialized\")\n",
    "\n",
    "    def train(self, model, train_loader, train_sampler, valid_loader_list, opt, loss_type, start_epoch, num_epochs, label2id, id2label, last_metrics=None):\n",
    "        \"\"\"\n",
    "        Training\n",
    "        args:\n",
    "            model: Model object\n",
    "            train_loader: DataLoader object of the training set\n",
    "            valid_loader_list: a list of Validation DataLoader objects\n",
    "            opt: Optimizer object\n",
    "            start_epoch: start epoch (> 0 if you resume the process)\n",
    "            num_epochs: last epoch\n",
    "            last_metrics: (if resume)\n",
    "        \"\"\"\n",
    "        history = []\n",
    "        start_time = time.time()\n",
    "        best_valid_loss = 1000000000 if last_metrics is None else last_metrics['valid_loss']\n",
    "        smoothing = constant.args.label_smoothing\n",
    "\n",
    "        logging.info(\"name \" +  constant.args.name)\n",
    "\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            sys.stdout.flush()\n",
    "            total_loss, total_cer, total_wer, total_char, total_word = 0, 0, 0, 0, 0\n",
    "\n",
    "            start_iter = 0\n",
    "\n",
    "            logging.info(\"TRAIN\")\n",
    "            model.train()\n",
    "            pbar = tqdm(iter(train_loader), leave=True, total=len(train_loader))\n",
    "            for i, (data) in enumerate(pbar, start=start_iter):\n",
    "                src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "                if constant.USE_CUDA:\n",
    "                    src = src.cuda()\n",
    "                    tgt = tgt.cuda()\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "                pred, gold, hyp_seq, gold_seq = model(src, src_lengths, tgt, verbose=False)\n",
    "\n",
    "                try: # handle case for CTC\n",
    "                    strs_gold, strs_hyps = [], []\n",
    "                    for ut_gold in gold_seq:\n",
    "                        str_gold = \"\"\n",
    "                        for x in ut_gold:\n",
    "                            if int(x) == constant.PAD_TOKEN:\n",
    "                                break\n",
    "                            str_gold = str_gold + id2label[int(x)]\n",
    "                        strs_gold.append(str_gold)\n",
    "                    for ut_hyp in hyp_seq:\n",
    "                        str_hyp = \"\"\n",
    "                        for x in ut_hyp:\n",
    "                            if int(x) == constant.PAD_TOKEN:\n",
    "                                break\n",
    "                            str_hyp = str_hyp + id2label[int(x)]\n",
    "                        strs_hyps.append(str_hyp)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    logging.info(\"NaN predictions\")\n",
    "                    continue\n",
    "\n",
    "                seq_length = pred.size(1)\n",
    "                sizes = Variable(src_percentages.mul_(int(seq_length)).int(), requires_grad=False)\n",
    "\n",
    "                loss, num_correct = calculate_metrics(\n",
    "                    pred, gold, input_lengths=sizes, target_lengths=tgt_lengths, smoothing=smoothing, loss_type=loss_type)\n",
    "\n",
    "                if loss.item() == float('Inf'):\n",
    "                    logging.info(\"Found infinity loss, masking\")\n",
    "                    loss = torch.where(loss != loss, torch.zeros_like(loss), loss) # NaN masking\n",
    "                    continue\n",
    "\n",
    "                # if constant.args.verbose:\n",
    "                #     logging.info(\"GOLD\", strs_gold)\n",
    "                #     logging.info(\"HYP\", strs_hyps)\n",
    "\n",
    "                for j in range(len(strs_hyps)):\n",
    "                    strs_hyps[j] = strs_hyps[j].replace(constant.SOS_CHAR, '').replace(constant.EOS_CHAR, '')\n",
    "                    strs_gold[j] = strs_gold[j].replace(constant.SOS_CHAR, '').replace(constant.EOS_CHAR, '')\n",
    "                    cer = calculate_cer(strs_hyps[j].replace(' ', ''), strs_gold[j].replace(' ', ''))\n",
    "                    wer = calculate_wer(strs_hyps[j], strs_gold[j])\n",
    "                    total_cer += cer\n",
    "                    total_wer += wer\n",
    "                    total_char += len(strs_gold[j].replace(' ', ''))\n",
    "                    total_word += len(strs_gold[j].split(\" \"))\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if constant.args.clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), constant.args.max_norm)\n",
    "                \n",
    "                opt.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                non_pad_mask = gold.ne(constant.PAD_TOKEN)\n",
    "                num_word = non_pad_mask.sum().item()\n",
    "\n",
    "                pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} CER:{:.2f}% LR:{:.7f}\".format(\n",
    "                    (epoch+1), total_loss/(i+1), total_cer*100/total_char, opt._rate))\n",
    "            logging.info(\"(Epoch {}) TRAIN LOSS:{:.4f} CER:{:.2f}% LR:{:.7f}\".format(\n",
    "                (epoch+1), total_loss/(len(train_loader)), total_cer*100/total_char, opt._rate))\n",
    "\n",
    "            # evaluate\n",
    "            print(\"\")\n",
    "            logging.info(\"VALID\")\n",
    "            model.eval()\n",
    "\n",
    "            for ind in range(len(valid_loader_list)):\n",
    "                valid_loader = valid_loader_list[ind]\n",
    "\n",
    "                total_valid_loss, total_valid_cer, total_valid_wer, total_valid_char, total_valid_word = 0, 0, 0, 0, 0\n",
    "                valid_pbar = tqdm(iter(valid_loader), leave=True, total=len(valid_loader))\n",
    "                for i, (data) in enumerate(valid_pbar):\n",
    "                    src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "                    if constant.USE_CUDA:\n",
    "                        src = src.cuda()\n",
    "                        tgt = tgt.cuda()\n",
    "\n",
    "                    pred, gold, hyp_seq, gold_seq = model(src, src_lengths, tgt, verbose=False)\n",
    "\n",
    "                    seq_length = pred.size(1)\n",
    "                    sizes = Variable(src_percentages.mul_(int(seq_length)).int(), requires_grad=False)\n",
    "\n",
    "                    loss, num_correct = calculate_metrics(\n",
    "                        pred, gold, input_lengths=sizes, target_lengths=tgt_lengths, smoothing=smoothing, loss_type=loss_type)\n",
    "\n",
    "                    if loss.item() == float('Inf'):\n",
    "                        logging.info(\"Found infinity loss, masking\")\n",
    "                        loss = torch.where(loss != loss, torch.zeros_like(loss), loss) # NaN masking\n",
    "                        continue\n",
    "\n",
    "                    try: # handle case for CTC\n",
    "                        strs_gold, strs_hyps = [], []\n",
    "                        for ut_gold in gold_seq:\n",
    "                            str_gold = \"\"\n",
    "                            for x in ut_gold:\n",
    "                                if int(x) == constant.PAD_TOKEN:\n",
    "                                    break\n",
    "                                str_gold = str_gold + id2label[int(x)]\n",
    "                            strs_gold.append(str_gold)\n",
    "                        for ut_hyp in hyp_seq:\n",
    "                            str_hyp = \"\"\n",
    "                            for x in ut_hyp:\n",
    "                                if int(x) == constant.PAD_TOKEN:\n",
    "                                    break\n",
    "                                str_hyp = str_hyp + id2label[int(x)]\n",
    "                            strs_hyps.append(str_hyp)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        logging.info(\"NaN predictions\")\n",
    "                        continue\n",
    "\n",
    "                    for j in range(len(strs_hyps)):\n",
    "                        strs_hyps[j] = strs_hyps[j].replace(constant.SOS_CHAR, '').replace(constant.EOS_CHAR, '')\n",
    "                        strs_gold[j] = strs_gold[j].replace(constant.SOS_CHAR, '').replace(constant.EOS_CHAR, '')\n",
    "                        cer = calculate_cer(strs_hyps[j].replace(' ', ''), strs_gold[j].replace(' ', ''))\n",
    "                        wer = calculate_wer(strs_hyps[j], strs_gold[j])\n",
    "                        total_valid_cer += cer\n",
    "                        total_valid_wer += wer\n",
    "                        total_valid_char += len(strs_gold[j].replace(' ', ''))\n",
    "                        total_valid_word += len(strs_gold[j].split(\" \"))\n",
    "\n",
    "                    total_valid_loss += loss.item()\n",
    "                    valid_pbar.set_description(\"VALID SET {} LOSS:{:.4f} CER:{:.2f}%\".format(ind,\n",
    "                        total_valid_loss/(i+1), total_valid_cer*100/total_valid_char))\n",
    "                logging.info(\"VALID SET {} LOSS:{:.4f} CER:{:.2f}%\".format(ind,\n",
    "                        total_valid_loss/(len(valid_loader)), total_valid_cer*100/total_valid_char))\n",
    "\n",
    "            metrics = {}\n",
    "            metrics[\"train_loss\"] = total_loss / len(train_loader)\n",
    "            metrics[\"valid_loss\"] = total_valid_loss / (len(valid_loader))\n",
    "            metrics[\"train_cer\"] = total_cer\n",
    "            metrics[\"train_wer\"] = total_wer\n",
    "            metrics[\"valid_cer\"] = total_valid_cer\n",
    "            metrics[\"valid_wer\"] = total_valid_wer\n",
    "            metrics[\"history\"] = history\n",
    "            history.append(metrics)\n",
    "\n",
    "            if epoch % constant.args.save_every == 0:\n",
    "                save_model(model, (epoch+1), opt, metrics,\n",
    "                        label2id, id2label, best_model=False)\n",
    "\n",
    "            # save the best model\n",
    "            if best_valid_loss > total_valid_loss/len(valid_loader):\n",
    "                best_valid_loss = total_valid_loss/len(valid_loader)\n",
    "                save_model(model, (epoch+1), opt, metrics,\n",
    "                        label2id, id2label, best_model=True)\n",
    "\n",
    "            if constant.args.shuffle:\n",
    "                logging.info(\"SHUFFLE\")\n",
    "                print(\"SHUFFLE\")\n",
    "                train_sampler.shuffle(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "THE EXPERIMENT LOG IS SAVED IN: log/model\n",
      "TRAINING MANIFEST:  None\n",
      "VALID MANIFEST:  None\n",
      "TEST MANIFEST:  None\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"THE EXPERIMENT LOG IS SAVED IN: \" + \"log/\" + args.name)\n",
    "print(\"TRAINING MANIFEST: \", args.train_manifest_list)\n",
    "print(\"VALID MANIFEST: \", args.valid_manifest_list)\n",
    "print(\"TEST MANIFEST: \", args.test_manifest_list)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"log/\" + args.name, filemode='w+', \n",
    "                    format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "audio_conf = dict(sample_rate=args.sample_rate,\n",
    "                  window_size=args.window_size,\n",
    "                  window_stride=args.window_stride,\n",
    "                  window=args.window,\n",
    "                  noise_dir=args.noise_dir,\n",
    "                  noise_prob=args.noise_prob,\n",
    "                  noise_levels=(args.noise_min, args.noise_max))\n",
    "\n",
    "logging.info(audio_conf)\n",
    "\n",
    "with open(\"data/labels/\"+args.labels_path) as label_file:\n",
    "        labels = str(''.join(json.load(label_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"¶§¤_'abcdefghijklmnopqrstuvwxyz \""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "\n",
    "labels = args.PAD_CHAR + args.SOS_CHAR + args.EOS_CHAR + labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id, id2label = {}, {}\n",
    "count = 0\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] not in label2id:\n",
    "        label2id[labels[i]] = count\n",
    "        id2label[count] = labels[i]\n",
    "        count += 1\n",
    "    else:\n",
    "        print(\"multiple label: \", labels[i])\n",
    "        \n",
    "# label2id = dict([(labels[i], i) for i in range(len(labels))])\n",
    "# id2label = dict([(i, labels[i]) for i in range(len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'¶': 0,\n",
       " '§': 1,\n",
       " '¤': 2,\n",
       " '_': 3,\n",
       " \"'\": 4,\n",
       " 'a': 5,\n",
       " 'b': 6,\n",
       " 'c': 7,\n",
       " 'd': 8,\n",
       " 'e': 9,\n",
       " 'f': 10,\n",
       " 'g': 11,\n",
       " 'h': 12,\n",
       " 'i': 13,\n",
       " 'j': 14,\n",
       " 'k': 15,\n",
       " 'l': 16,\n",
       " 'm': 17,\n",
       " 'n': 18,\n",
       " 'o': 19,\n",
       " 'p': 20,\n",
       " 'q': 21,\n",
       " 'r': 22,\n",
       " 's': 23,\n",
       " 't': 24,\n",
       " 'u': 25,\n",
       " 'v': 26,\n",
       " 'w': 27,\n",
       " 'x': 28,\n",
       " 'y': 29,\n",
       " 'z': 30,\n",
       " ' ': 31}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, lm=None):\n",
    "    \"\"\"\n",
    "    Evaluation\n",
    "    args:\n",
    "        model: Model object\n",
    "        test_loader: DataLoader object\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_word, total_char, total_cer, total_wer = 0, 0, 0, 0\n",
    "    total_en_cer, total_zh_cer, total_en_char, total_zh_char = 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(iter(test_loader), leave=True, total=len(test_loader))\n",
    "        for i, (data) in enumerate(test_pbar):\n",
    "            src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "            if constant.USE_CUDA:\n",
    "                src = src.cuda()\n",
    "                tgt = tgt.cuda()\n",
    "\n",
    "            batch_ids_hyps, batch_strs_hyps, batch_strs_gold = model.evaluate(\n",
    "                src, src_lengths, tgt, beam_search=constant.args.beam_search, beam_width=constant.args.beam_width, beam_nbest=constant.args.beam_nbest, lm=lm, lm_rescoring=constant.args.lm_rescoring, lm_weight=constant.args.lm_weight, c_weight=constant.args.c_weight, verbose=constant.args.verbose)\n",
    "\n",
    "            for x in range(len(batch_strs_gold)):\n",
    "                hyp = batch_strs_hyps[x].replace(constant.EOS_CHAR, \"\").replace(constant.SOS_CHAR, \"\").replace(constant.PAD_CHAR, \"\")\n",
    "                gold = batch_strs_gold[x].replace(constant.EOS_CHAR, \"\").replace(constant.SOS_CHAR, \"\").replace(constant.PAD_CHAR, \"\")\n",
    "\n",
    "                wer = calculate_wer(hyp, gold)\n",
    "                cer = calculate_cer(hyp.strip(), gold.strip())\n",
    "\n",
    "                en_cer, zh_cer, num_en_char, num_zh_char = calculate_cer_en_zh(hyp, gold)\n",
    "                total_en_cer += en_cer\n",
    "                total_zh_cer += zh_cer\n",
    "                total_en_char += num_en_char\n",
    "                total_zh_char += num_zh_char\n",
    "\n",
    "                total_wer += wer\n",
    "                total_cer += cer\n",
    "                total_word += len(gold.split(\" \"))\n",
    "                total_char += len(gold)\n",
    "\n",
    "            test_pbar.set_description(\"TEST CER:{:.2f}% WER:{:.2f}% CER_EN:{:.2f}% CER_ZH:{:.2f}%\".format(\n",
    "                total_cer*100/total_char, total_wer*100/total_word, total_en_cer*100/max(1, total_en_char), total_zh_cer*100/max(1, total_zh_char)))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = constant.args\n",
    "\n",
    "#     start_iter = 0\n",
    "\n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "#     # Load the model\n",
    "#     load_path = constant.args.continue_from\n",
    "#     model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(\n",
    "#         constant.args.continue_from)\n",
    "    \n",
    "    \n",
    "#     if loaded_args.parallel:\n",
    "#         print(\"unwrap data parallel\")\n",
    "#         model = model.module\n",
    "\n",
    "#     audio_conf = dict(sample_rate=loaded_args.sample_rate,\n",
    "#                       window_size=loaded_args.window_size,\n",
    "#                       window_stride=loaded_args.window_stride,\n",
    "#                       window=loaded_args.window,\n",
    "#                       noise_dir=loaded_args.noise_dir,\n",
    "#                       noise_prob=loaded_args.noise_prob,\n",
    "#                       noise_levels=(loaded_args.noise_min, loaded_args.noise_max))\n",
    "\n",
    "#     test_data = SpectrogramDataset(\n",
    "#         audio_conf=audio_conf, manifest_filepath_list=constant.args.test_manifest_list,\n",
    "#         label2id=label2id, normalize=True, augment=False)\n",
    "#     test_sampler = BucketingSampler(test_data, batch_size=constant.args.batch_size)\n",
    "#     test_loader = AudioDataLoader(test_data, num_workers=args.num_workers, \n",
    "#                                   batch_sampler=test_sampler)\n",
    "\n",
    "#     lm = None\n",
    "#     if constant.args.lm_rescoring:\n",
    "#         lm = LM(constant.args.lm_path)\n",
    "\n",
    "#     print(model)\n",
    "\n",
    "#     evaluate(model, test_loader, lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, lm=None):\n",
    "    \"\"\"\n",
    "    Evaluation\n",
    "    args:\n",
    "        model: Model object\n",
    "        test_loader: DataLoader object\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_word, total_char, total_cer, total_wer = 0, 0, 0, 0\n",
    "    total_en_cer, total_zh_cer, total_en_char, total_zh_char = 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(iter(test_loader), leave=True, total=len(test_loader))\n",
    "        for i, (data) in enumerate(test_pbar):\n",
    "            src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "\n",
    "            if constant.USE_CUDA:\n",
    "                src = src.cuda()\n",
    "                tgt = tgt.cuda()\n",
    "\n",
    "            batch_ids_hyps, batch_strs_hyps, batch_strs_gold = model.evaluate(\n",
    "                src, src_lengths, tgt, beam_search=constant.args.beam_search, beam_width=constant.args.beam_width, beam_nbest=constant.args.beam_nbest, lm=lm, lm_rescoring=constant.args.lm_rescoring, lm_weight=constant.args.lm_weight, c_weight=constant.args.c_weight, verbose=constant.args.verbose)\n",
    "\n",
    "            for x in range(len(batch_strs_gold)):\n",
    "                hyp = batch_strs_hyps[x].replace(constant.EOS_CHAR, \"\").replace(constant.SOS_CHAR, \"\").replace(constant.PAD_CHAR, \"\")\n",
    "                gold = batch_strs_gold[x].replace(constant.EOS_CHAR, \"\").replace(constant.SOS_CHAR, \"\").replace(constant.PAD_CHAR, \"\")\n",
    "\n",
    "                wer = calculate_wer(hyp, gold)\n",
    "                cer = calculate_cer(hyp.strip(), gold.strip())\n",
    "\n",
    "                en_cer, zh_cer, num_en_char, num_zh_char = calculate_cer_en_zh(hyp, gold)\n",
    "                total_en_cer += en_cer\n",
    "                total_zh_cer += zh_cer\n",
    "                total_en_char += num_en_char\n",
    "                total_zh_char += num_zh_char\n",
    "\n",
    "                total_wer += wer\n",
    "                total_cer += cer\n",
    "                total_word += len(gold.split(\" \"))\n",
    "                total_char += len(gold)\n",
    "\n",
    "            test_pbar.set_description(\"TEST CER:{:.2f}% WER:{:.2f}% CER_EN:{:.2f}% CER_ZH:{:.2f}%\".format(\n",
    "                total_cer*100/total_char, total_wer*100/total_word, total_en_cer*100/max(1, total_en_char), total_zh_cer*100/max(1, total_zh_char)))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = constant.args\n",
    "\n",
    "#     start_iter = 0\n",
    "\n",
    "#     # Load the model\n",
    "#     load_path = constant.args.continue_from\n",
    "#     model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(constant.args.continue_from)\n",
    "    \n",
    "#     if loaded_args.parallel:\n",
    "#         print(\"unwrap data parallel\")\n",
    "#         model = model.module\n",
    "\n",
    "#     audio_conf = dict(sample_rate=loaded_args.sample_rate,\n",
    "#                       window_size=loaded_args.window_size,\n",
    "#                       window_stride=loaded_args.window_stride,\n",
    "#                       window=loaded_args.window,\n",
    "#                       noise_dir=loaded_args.noise_dir,\n",
    "#                       noise_prob=loaded_args.noise_prob,\n",
    "#                       noise_levels=(loaded_args.noise_min, loaded_args.noise_max))\n",
    "\n",
    "#     test_data = SpectrogramDataset(audio_conf=audio_conf, manifest_filepath_list=constant.args.test_manifest_list, label2id=label2id,\n",
    "#                                    normalize=True, augment=False)\n",
    "#     test_sampler = BucketingSampler(test_data, batch_size=constant.args.batch_size)\n",
    "#     test_loader = AudioDataLoader(test_data, num_workers=args.num_workers, batch_sampler=test_sampler)\n",
    "\n",
    "#     lm = None\n",
    "#     if constant.args.lm_rescoring:\n",
    "#         lm = LM(constant.args.lm_path)\n",
    "\n",
    "#     print(model)\n",
    "\n",
    "#     evaluate(model, test_loader, lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SpectrogramDataset(audio_conf, manifest_filepath_list=args.train_manifest_list, \n",
    "                                label2id=label2id, normalize=True, augment=args.augment)\n",
    "\n",
    "train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "\n",
    "train_loader = AudioDataLoader(\n",
    "    train_data, num_workers=args.num_workers, batch_sampler=train_sampler)\n",
    "\n",
    "valid_loader_list, test_loader_list = [], []\n",
    "for i in range(len(args.valid_manifest_list)):\n",
    "    valid_data = SpectrogramDataset(\n",
    "        audio_conf, manifest_filepath_list=[args.valid_manifest_list[i]], \n",
    "        label2id=label2id, normalize=True, augment=False)\n",
    "    \n",
    "    valid_loader = AudioDataLoader(valid_data, num_workers=args.num_workers, \n",
    "                                   batch_size=args.batch_size)\n",
    "    valid_loader_list.append(valid_loader)\n",
    "    \n",
    "\n",
    "for i in range(len(args.test_manifest_list)):\n",
    "    test_data = SpectrogramDataset(\n",
    "        audio_conf, manifest_filepath_list=[args.test_manifest_list[i]], \n",
    "        label2id=label2id, normalize=True, augment=False)\n",
    "    \n",
    "    test_loader = AudioDataLoader(test_data, num_workers=args.num_workers)\n",
    "    test_loader_list.append(test_loader)\n",
    "\n",
    "start_epoch = 0\n",
    "metrics = None\n",
    "loaded_args = None\n",
    "print(constant.args.continue_from)\n",
    "if constant.args.continue_from != \"\":\n",
    "    logging.info(\"Continue from checkpoint: \" + constant.args.continue_from)\n",
    "    model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(\n",
    "        constant.args.continue_from)\n",
    "    start_epoch = epoch  # index starts from zero\n",
    "    verbose = constant.args.verbose\n",
    "\n",
    "    if loaded_args != None:\n",
    "        # Unwrap nn.DataParallel\n",
    "        if loaded_args.parallel:\n",
    "            logging.info(\"unwrap from DataParallel\")\n",
    "            model = model.module\n",
    "\n",
    "        # Parallelize the batch\n",
    "        if args.parallel:\n",
    "            model = nn.DataParallel(model, device_ids=args.device_ids)\n",
    "else:\n",
    "    if constant.args.model == \"TRFS\":\n",
    "        model = init_transformer_model(constant.args, label2id, id2label)\n",
    "        opt = init_optimizer(constant.args, model, \"noam\")\n",
    "    else:\n",
    "        logging.info(\"The model is not supported, check args --h\")\n",
    "\n",
    "loss_type = args.loss\n",
    "\n",
    "if constant.USE_CUDA:\n",
    "    model = model.cuda(0)\n",
    "\n",
    "logging.info(model)\n",
    "num_epochs = constant.args.epochs\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.train(model, train_loader, train_sampler, valid_loader_list, opt, loss_type, start_epoch, num_epochs, label2id, id2label, metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
