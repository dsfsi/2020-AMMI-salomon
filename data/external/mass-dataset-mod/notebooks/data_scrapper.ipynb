{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ipdb\n",
    "import wget\n",
    "import os\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP = json.loads(\"./nlp_progress/NLP-progress/structured.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"./data/paperwithcode/\", refresh=False):\n",
    "    \n",
    "    if refresh:\n",
    "        print('Beginning file download')\n",
    "        link_all_paper_abstract = \"https://paperswithcode.com/media/about/papers-with-abstracts.json.gz\"\n",
    "\n",
    "        # check if there is a directory data/paperwithcode\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            \n",
    "        # downloading the file, and saving into our path    \n",
    "        wget.download(link_all_paper_abstract, path+'papers-with-abstracts.json')\n",
    "        print('Download complete')\n",
    "        \n",
    "    # Loading the data from json (deserialization)\n",
    "    with open(path+\"structured.json\", \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(path=\"./nlp_progress/NLP-progress/\", refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonExplorer(data):\n",
    "    for i in range(len(data)):\n",
    "        if 'subtasks' in data[i].keys():\n",
    "            \n",
    "            for j in range(len(data[i]['subtasks'])):\n",
    "                \n",
    "                if 'datasets' in data[i]['subtasks'][j].keys():\n",
    "                    \n",
    "                    for k in range(len(data[i]['subtasks'][j]['datasets'])):\n",
    "                        \n",
    "                        dataset = data[i]['subtasks'][j]['datasets'][k]['dataset']\n",
    "                        \n",
    "                        description = data[i]['subtasks'][j]['datasets'][k]['description']\n",
    "                        \n",
    "                        if 'sota' in data[i]['subtasks'][j]['datasets'][k].keys():\n",
    "                            \n",
    "                            if 'rows' in data[i]['subtasks'][j]['datasets'][k]['sota'].keys():\n",
    "                                for l in range(len(data[i]['subtasks'][j]['datasets'][k]['sota']['rows'])):\n",
    "                                    \n",
    "                                    if 'paper_url' in data[i]['subtasks'][j]['datasets'][k]\\\n",
    "                                            ['sota']['rows'][l].keys():\n",
    "                                        \n",
    "#                                         paper_url = data[i]['subtasks'][j]['datasets'][k]\\\n",
    "#                                                 ['sota']['rows'][l]['paper_url']\n",
    "                                        \n",
    "                                        # to only have the link to the pdf and exclude the code\n",
    "                                        paper_url = data[i]['subtasks'][j]['datasets'][k]\\\n",
    "                                                ['sota']['rows'][l]['paper_url'].split(')')[0]#.split('(')[0].split('(')[0]\n",
    "\n",
    "\n",
    "                                        if 'metrics' in data[i]['subtasks'][j]['datasets'][k]\\\n",
    "                                                ['sota']['rows'][l].keys():\n",
    "\n",
    "                                            for mt in data[i]['subtasks'][j]['datasets'][k]\\\n",
    "                                                ['sota']['rows'][l]['metrics'].keys():\n",
    "\n",
    "                                                metric = mt\n",
    "\n",
    "                                                value = data[i]['subtasks'][j]['datasets'][k]\\\n",
    "                                                ['sota']['rows'][l]['metrics'][mt]\n",
    "\n",
    "\n",
    "                                                with open(\"./data/dataPreTrain.txt\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "#                                                     text_file.write(\\\n",
    "#                                                         paper_url+\"#\"+value+\"\\t\\t\"+dataset+\"\\t\\t\"+metric+\"\\t\\t\"+description)\n",
    "#                                                     text_file.write(\\\n",
    "#                                                         paper_url+\"#\"+value+\"    \"+dataset+\"    \"+metric+\"    \\n\")\n",
    "\n",
    "                                                    text_file.write(\\\n",
    "                                                        paper_url+\"#\"+value+\"\\t\\t\"+dataset+\"\\t\\t\"+metric)\n",
    "                                                    \n",
    "                                                    text_file.write(\"\\n\")\n",
    "                        \n",
    "                        \n",
    "        if 'datasets' in data[i].keys():\n",
    "            for k in range(len(data[i]['datasets'])):\n",
    "                dataset = data[i]['datasets'][k]['dataset']\n",
    "                \n",
    "                description = data[i]['datasets'][k]['description']\n",
    "                        \n",
    "                if 'sota' in data[i]['datasets'][k].keys():\n",
    "\n",
    "                    if 'rows' in data[i]['datasets'][k]['sota'].keys():\n",
    "                        for l in range(len(data[i]['datasets'][k]['sota']['rows'])):\n",
    "\n",
    "                            if 'paper_url' in data[i]['datasets'][k]\\\n",
    "                                    ['sota']['rows'][l].keys():\n",
    "\n",
    "#                                 paper_url = data[i]['datasets'][k]\\\n",
    "#                                         ['sota']['rows'][l]['paper_url']\n",
    "                                \n",
    "                                # to only have the link to the pdf and exclude the code\n",
    "                                paper_url = data[i]['datasets'][k]\\\n",
    "                                        ['sota']['rows'][l]['paper_url'].split(')')[0]#.split('(')[0].split('(')[0]\n",
    "\n",
    "                                if 'metrics' in data[i]['datasets'][k]\\\n",
    "                                        ['sota']['rows'][l].keys():\n",
    "\n",
    "                                    for mt in data[i]['datasets'][k]\\\n",
    "                                        ['sota']['rows'][l]['metrics'].keys():\n",
    "\n",
    "                                        metric = mt\n",
    "\n",
    "                                        value = data[i]['datasets'][k]\\\n",
    "                                        ['sota']['rows'][l]['metrics'][mt]\n",
    "\n",
    "\n",
    "                                        with open(\"./data/dataPreTrain.txt\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "                                            text_file.write(\\\n",
    "                                                paper_url+\"#\"+value+\"\\t\\t\"+dataset+\"\\t\\t\"+metric)\n",
    "\n",
    "                                            text_file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonExplorer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import fileinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingDataCreator(path, source, target):\n",
    "    \n",
    "    ## to make sure we don't append on existing content \n",
    "    if os.path.isfile(path+target) :\n",
    "        os.remove(path+target)\n",
    "        \n",
    "    # replace all occurrences of 'sit' with 'SIT' and insert a line after the 5th\n",
    "    with open(path+source, 'r') as f:  # Open file for read\n",
    "        for numb1, lineSlow in enumerate(f):           # Read line-by-line\n",
    "            with open(path+source, 'r') as f:  # Open file for read\n",
    "                for numb2, lineFast in enumerate(f):           # Read line-by-line\n",
    "                    \n",
    "                     with open(path+target, \"a+\", encoding=\"utf-8\") as text_file:\n",
    "                        \n",
    "                        if numb1 == numb2:\n",
    "                            text_file.write(\\\n",
    "                            \"true\\t\\t\"+lineSlow)\n",
    "#                             text_file.write(\"\\n\")\n",
    "                        else:\n",
    "                            text_file.write(\\\n",
    "                            \"false\\t\\t\"+lineFast)\n",
    "#                             text_file.write(\"\\n\")\n",
    "                            \n",
    "# File closed automatically upon exit of with-statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually splited from the main file 601 papers for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingDataCreator(\"./data/\", \\\n",
    "                    \"dataPreTrainSplit602.txt\", \"dataTrainSplit.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http:', '', 'arxiv.org', 'pdf', '1611.03566v3.pdf']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'http://arxiv.org/pdf/1611.03566v3.pdf'.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/paperwithcode/1611.03566v3.pdf.Construction_Inspection_through_Spatial_Database.pdf'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query for a paper of interest, then download\n",
    "# paper = arxiv.query(id_list=[\"1707.08567\"])[0]\n",
    "# arxiv.download(paper)\n",
    "\n",
    "# You can skip the query step if you have the paper info!\n",
    "paper2 = {\"pdf_url\": \"http://arxiv.org/pdf/1611.03566v3.pdf\",\n",
    "          \"title\": 'Construction Inspection through Spatial Database'}\n",
    "\n",
    "arxiv.download(paper2, dirpath=\"./data/paperwithcode/\")\n",
    "# for arxiv_id : None\n",
    "# wget.download(\"https://www.researchgate.net/profile/Madson_Dias/publication/328819483_Optimally_Selected_Minimal_Learning_Machine/links/5d7a605e4585157fde0fce53/Optimally-Selected-Minimal-Learning-Machine.pdf\",'./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdirpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mslugify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mslugify\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f54ed116950\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefer_source_tarfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslugify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslugify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer_source_tarfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Download the .pdf corresponding to the result object 'obj'. If prefer_source_tarfile==True, download the source .tar.gz instead.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pdf_url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Object has no PDF URL.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mdirpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdirpath\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mprefer_source_tarfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/pdf/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/src/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pdf_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mslugify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pdf_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mslugify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/orkg/lib/python3.6/site-packages/arxiv/arxiv.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arxiv.download??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Construction_Inspection_through_Spatial_Database'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/paperwithcode//N16-1106.pdf'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('https://www.aclweb.org/anthology/N16-1106.pdf', \"./data/paperwithcode/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_url': 'https://paperswithcode.com/paper/dag-structured-long-short-term-memory-for',\n",
       " 'arxiv_id': None,\n",
       " 'title': 'DAG-Structured Long Short-Term Memory for Semantic Compositionality',\n",
       " 'abstract': '',\n",
       " 'url_abs': 'https://www.aclweb.org/anthology/N16-1106/',\n",
       " 'url_pdf': 'https://www.aclweb.org/anthology/N16-1106',\n",
       " 'proceeding': 'NAACL 2016 6',\n",
       " 'authors': ['Xiaodan Zhu', 'Parinaz Sobhani', 'Hongyu Guo'],\n",
       " 'tasks': ['Machine Translation',\n",
       "  'Semantic Composition',\n",
       "  'Sentiment Analysis',\n",
       "  'Speech Recognition'],\n",
       " 'date': '2016-06-01'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download(path + pdf_url, \"./data/paperwithcode/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic Models, ello'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_fom_json(path, data, download_pdf = False):\n",
    "    \"\"\"\n",
    "    This function will help to save in text form, depening to the book and verse argument \n",
    "    given in parameter\n",
    "    \n",
    "    The data is maintly taken from the website https://www.jw.org/\n",
    "    \"\"\"\n",
    "    \n",
    "    for d in data:\n",
    "        \n",
    "        pdf_url = d['url_pdf']\n",
    "        tasks = d['tasks']\n",
    "        \n",
    "        title = d['title']\n",
    "            \n",
    "        if tasks == []:\n",
    "            tasks = \"N/A\"\n",
    "        else:\n",
    "            tasks = ', '.join(tasks)\n",
    "            \n",
    "        if len(pdf_url.split('.pdf')) != 2:\n",
    "            if pdf_url[-1] == '/':\n",
    "                pdf_url = pdf_url[:-1]+'.pdf'                \n",
    "            \n",
    "        # for arxiv_id : None\n",
    "        if download_pdf:\n",
    "            if d['arxiv_id'] != \"\":\n",
    "            \n",
    "                paper = {\"pdf_url\": pdf_url,\n",
    "                    \"title\": title}\n",
    "                \n",
    "                if not os.path.exists(path+\"pdf/\"):\n",
    "                    os.makedirs(path+\"pdf/\")\n",
    "\n",
    "                arxiv.download(pdf_url,\n",
    "                                   dirpath = path+\"pdf/\")\n",
    "            \n",
    "            else:\n",
    "                wget.download(pdf_url, \n",
    "                                  path+\"pdf/\")       \n",
    "        \n",
    "        try :\n",
    "            \n",
    "            if d['arxiv_id'] != \"\":\n",
    "                path_save = path+pdf_url.split('/')[-1]+'.'+'_'.join(title.split())+'.pdf'\n",
    "            else:\n",
    "                path_save = path+pdf_url.split('/')[-1]\n",
    "            \n",
    "            ipdb.set_trace()\n",
    "            with open(path+\"all_data.txt\", \"a\") as text_file:\n",
    "                text_file.write(path_save+\" \"+tasks+\"\\n\")\n",
    "\n",
    "        except Exception as inst:      \n",
    "            traceback.print_exc()\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_data_fom_json(\"./data/paperwithcode/\", data, download_pdf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims",
   "language": "python",
   "name": "aims"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
