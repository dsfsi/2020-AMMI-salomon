{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EL9hWd6d2GGh",
    "outputId": "45b7cf3d-73a9-4e34-c46e-08f8522ec3d2"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.15\n",
    "# !pip install tensorflow-gpu==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 766
    },
    "id": "u-MsOiOeF_Jq",
    "outputId": "bc94e1ff-7761-4afa-c668-b2d96c6e1c63"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.13.0rc1\n",
    "# !pip install tensorflow==1.13.1\n",
    "# !pip install tensorflow-gpu==1.13.1\n",
    "\n",
    "# !pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52-QshW6MnXz",
    "outputId": "0d605f07-1c19-4435-d0f5-8dc0bbbc0c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 13 10:36:30 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bM2B-OVhhn6v"
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import six\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import ipdb\n",
    "import copy\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.training import saver\n",
    "from tensorflow.python.framework import function\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "from six.moves import zip  # pylint: disable=redefined-builtin\n",
    "from importlib import reload # not build in in py3\n",
    "reload(sys)\n",
    "from six import PY2\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# Dependency imports\n",
    "\n",
    "# from utils import tfAudioTools as tfAudio\n",
    "# from utils.tfRecord import RESERVED_TOKENS_TO_INDEX\n",
    "# from utils import text_reader\n",
    "# from models import common_layers\n",
    "# from models import common_attention\n",
    "# from utils.beamsearch import *\n",
    "# from utils import parallel\n",
    "# from utils import datareader\n",
    "# from models import common_hparams\n",
    "# from models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKXDNiL3hZhj"
   },
   "source": [
    "\n",
    "\n",
    "# Colab configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhGhvICkiGEZ",
    "outputId": "c2f83328-4800-4209-8fd1-f09442caac0d"
   },
   "outputs": [],
   "source": [
    "# # link to google drive, click on the given link and choose the google drive account you would like to be available to you \n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKxYCHo-4AVq",
    "outputId": "fd3e72ab-e1f1-4b0b-9412-4364bac8afa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aims/big_data/2020-AMMI-salomon/src\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3D5HkI64iMCH",
    "outputId": "a349dcd2-ce39-4a37-8071-f995d60b3bc9"
   },
   "outputs": [],
   "source": [
    "# %cd ../content/gdrive/MyDrive/colab-ssh/2020-AMMI-salomon/notebooks/MT-ASR/WORKING_Tensorflow/synchronous/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdA-O17qiWQR",
    "outputId": "f54eb701-383c-4726-8302-565e7246b436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue.sh\n",
      "chi_char_segment.pl\n",
      "data\n",
      "datagen.sh\n",
      "events.out.tfevents.1615022775.dsfsi-ammi-salomon-vm\n",
      "features\n",
      "__init__.py\n",
      "listra\n",
      "model.ckpt-61000.data-00000-of-00002\n",
      "model.ckpt-61000.data-00001-of-00002\n",
      "model.ckpt-61000.index\n",
      "model.ckpt-61000.meta\n",
      "model.ckpt-62000.data-00000-of-00002\n",
      "model.ckpt-62000.data-00001-of-00002\n",
      "model.ckpt-62000.index\n",
      "model.ckpt-62000.meta\n",
      "model.ckpt-63000.data-00000-of-00002\n",
      "model.ckpt-63000.data-00001-of-00002\n",
      "model.ckpt-63000.index\n",
      "model.ckpt-63000.meta\n",
      "model.ckpt-64000.data-00000-of-00002\n",
      "model.ckpt-64000.data-00001-of-00002\n",
      "model.ckpt-64000.index\n",
      "model.ckpt-64000.meta\n",
      "model.ckpt-65000.data-00000-of-00002\n",
      "model.ckpt-65000.data-00001-of-00002\n",
      "model.ckpt-65000.index\n",
      "model.ckpt-65000.meta\n",
      "model.ckpt-66000.data-00000-of-00002\n",
      "model.ckpt-66000.data-00001-of-00002\n",
      "model.ckpt-66000.index\n",
      "model.ckpt-66000.meta\n",
      "model.ckpt-67000.data-00000-of-00002\n",
      "model.ckpt-67000.data-00001-of-00002\n",
      "model.ckpt-67000.index\n",
      "model.ckpt-67000.meta\n",
      "model.ckpt-68000.data-00000-of-00002\n",
      "model.ckpt-68000.data-00001-of-00002\n",
      "model.ckpt-68000.index\n",
      "model.ckpt-68000.meta\n",
      "model.ckpt-69000.data-00000-of-00002\n",
      "model.ckpt-69000.data-00001-of-00002\n",
      "model.ckpt-69000.index\n",
      "model.ckpt-69000.meta\n",
      "model.ckpt-70000.data-00000-of-00002\n",
      "model.ckpt-70000.data-00001-of-00002\n",
      "model.ckpt-70000.index\n",
      "model.ckpt-70000.meta\n",
      "model.ckpt-71000.data-00000-of-00002\n",
      "model.ckpt-71000.data-00001-of-00002\n",
      "model.ckpt-71000.index\n",
      "model.ckpt-71000.meta\n",
      "model.ckpt-72000.data-00000-of-00002\n",
      "model.ckpt-72000.data-00001-of-00002\n",
      "model.ckpt-72000.index\n",
      "model.ckpt-72000.meta\n",
      "model.ckpt-73000.data-00000-of-00002\n",
      "model.ckpt-73000.data-00001-of-00002\n",
      "model.ckpt-73000.index\n",
      "model.ckpt-73000.meta\n",
      "model.ckpt-74000.data-00000-of-00002\n",
      "model.ckpt-74000.data-00001-of-00002\n",
      "model.ckpt-74000.index\n",
      "model.ckpt-74000.meta\n",
      "model.ckpt-75000.data-00000-of-00002\n",
      "model.ckpt-75000.data-00001-of-00002\n",
      "model.ckpt-75000.index\n",
      "model.ckpt-75000.meta\n",
      "model.ckpt-76000.data-00000-of-00002\n",
      "model.ckpt-76000.data-00001-of-00002\n",
      "model.ckpt-76000.index\n",
      "model.ckpt-76000.meta\n",
      "model.ckpt-77000.data-00000-of-00002\n",
      "model.ckpt-77000.data-00001-of-00002\n",
      "model.ckpt-77000.index\n",
      "model.ckpt-77000.meta\n",
      "model.ckpt-78000.data-00000-of-00002\n",
      "model.ckpt-78000.data-00001-of-00002\n",
      "model.ckpt-78000.index\n",
      "model.ckpt-78000.meta\n",
      "model.ckpt-79000.data-00000-of-00002\n",
      "model.ckpt-79000.data-00001-of-00002\n",
      "model.ckpt-79000.index\n",
      "model.ckpt-79000.meta\n",
      "models\n",
      "multi-bleu.perl\n",
      "nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
      "python_speech_features\n",
      "README\n",
      "req.txt\n",
      "requirement.txt\n",
      "run.py\n",
      "test.sh\n",
      "Train_Listra\n",
      "Train_Listra.ipynb\n",
      "Train_Listra.py\n",
      "train.sh\n",
      "utils\n",
      "visualization\n",
      "wer.py\n",
      "wer.sh\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCmKHR-cdsZa"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BG6FNleGhw-t"
   },
   "outputs": [],
   "source": [
    "# Reserved tokens for things like padding and EOS symbols.\n",
    "PAD = \"<PAD>\"  # the index of PAD must be 0\n",
    "EOS = \"<EOS>\"\n",
    "L1 = \"<2L1>\"\n",
    "L2 = \"<2L2>\"\n",
    "DELAY = \"<DELAY>\"\n",
    "UNK = \"<UNK>\"\n",
    "SPACE = \" \"\n",
    "RESERVED_TOKENS = [PAD, EOS, L1, L2, DELAY, UNK, SPACE]\n",
    "RESERVED_TOKENS_TO_INDEX = {tok: idx for idx, tok in enumerate(RESERVED_TOKENS)}\n",
    "\n",
    "# Assuming EOS_ID is 1\n",
    "EOS_ID = 1\n",
    "# Default value for INF\n",
    "INF = 1. * 1e7\n",
    "L1_SYMBOL = RESERVED_TOKENS_TO_INDEX[\"<2L1>\"]\n",
    "L2_SYMBOL = RESERVED_TOKENS_TO_INDEX[\"<2L2>\"]\n",
    "DELAY_SYMBOL = RESERVED_TOKENS_TO_INDEX[\"<DELAY>\"]\n",
    "\n",
    "DELAY_SYMBOL = RESERVED_TOKENS_TO_INDEX[\"<DELAY>\"]\n",
    "L2_SYMBOL = RESERVED_TOKENS_TO_INDEX[\"<2L2>\"]\n",
    "\n",
    "\n",
    "\n",
    "PAD_idx = 0\n",
    "EOS_idx = 1\n",
    "L2R = 2\n",
    "R2L = 3\n",
    "\n",
    "# Assuming EOS_ID is 1\n",
    "EOS_ID = 1\n",
    "# Default value for INF\n",
    "INF = 1. * 1e7\n",
    "\n",
    "# import ipdb\n",
    "\n",
    "# Conversion between Unicode and UTF-8, if required (on Python2)\n",
    "native_to_unicode = (lambda s: s.decode(\"utf-8\")) if PY2 else (lambda s: s)\n",
    "unicode_to_native = (lambda s: s.encode(\"utf-8\")) if PY2 else (lambda s: s)\n",
    "\n",
    "\n",
    "# Reserved tokens for things like padding and EOS symbols.\n",
    "# PAD = \"<PAD>\"\n",
    "# EOS = \"<EOS>\"\n",
    "# L2R = \"<2en>\"\n",
    "# R2L = \"<2fr>\"\n",
    "# RESERVED_TOKENS = [PAD, EOS, L2R, R2L]\n",
    "# if six.PY2:\n",
    "#     RESERVED_TOKENS_BYTES = RESERVED_TOKENS\n",
    "# else:\n",
    "#     RESERVED_TOKENS_BYTES = [bytes(PAD, \"ascii\"), bytes(EOS, \"ascii\"), bytes(L2R, \"ascii\"), bytes(R2L, \"ascii\")]\n",
    "RESERVED_TOKENS_BYTES = RESERVED_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JbbSd9skdrF7"
   },
   "outputs": [],
   "source": [
    "\"\"\"Implemetation of beam seach with penalties.\"\"\"\n",
    "\n",
    "def shape_list(x):\n",
    "    if x.get_shape().dims is None:\n",
    "        return tf.shape(x)\n",
    "    static = x.get_shape().as_list()\n",
    "    shape = tf.shape(x)\n",
    "    ret = []\n",
    "    for i in range(len(static)):\n",
    "        dim = static[i]\n",
    "        if dim is None:\n",
    "            dim = shape[i]\n",
    "        ret.append(dim)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _merge_beam_dim(tensor):\n",
    "    \"\"\"Reshapes first two dimensions in to single dimension.\n",
    "        tensor: Tensor to reshape of shape [A, B, ...] --> [A*B, ...]\n",
    "    \"\"\"\n",
    "    shape = shape_list(tensor)\n",
    "    batch_size = shape[0]\n",
    "    beam_size = shape[1]\n",
    "    return tf.reshape(tensor, [batch_size * beam_size] + shape[2:])\n",
    "\n",
    "\n",
    "def _unmerge_beam_dim(tensor, batch_size, beam_size):\n",
    "    \"\"\"Reshapes first dimension back to [batch_size, beam_size].\n",
    "        [batch_size*beam_size, ...] --> [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = shape_list(tensor)\n",
    "    new_shape = [batch_size] + [beam_size] + shape[1:]\n",
    "    return tf.reshape(tensor, new_shape)\n",
    "\n",
    "\n",
    "def _expand_to_beam_size(tensor, beam_size):\n",
    "    \"\"\"Tiles a given tensor by beam_size.\n",
    "        tensor: tensor to tile [batch_size, ...] --> [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    tile_dims = [1] * tensor.shape.ndims\n",
    "    tile_dims[1] = beam_size\n",
    "    return tf.tile(tensor, tile_dims)\n",
    "\n",
    "\n",
    "def log_prob_from_logits(logits):\n",
    "    return logits - tf.reduce_logsumexp(logits, axis=2, keep_dims=True)\n",
    "\n",
    "\n",
    "def compute_batch_indices(batch_size, beam_size):\n",
    "    \"\"\"Computes the i'th coodinate that contains the batch index for gathers.\n",
    "    like [[0,0,0,0,],[1,1,1,1],..]\n",
    "    \"\"\"\n",
    "    batch_pos = tf.range(batch_size * beam_size) // beam_size\n",
    "    batch_pos = tf.reshape(batch_pos, [batch_size, beam_size])\n",
    "    return batch_pos\n",
    "\n",
    "\n",
    "def get_state_shape_invariants(tensor):\n",
    "    \"\"\"Returns the shape of the tensor but sets middle dims to None.\"\"\"\n",
    "    shape = tensor.shape.as_list()\n",
    "    for i in range(1, len(shape) - 1):\n",
    "        shape[i] = None\n",
    "    return tf.TensorShape(shape)\n",
    "\n",
    "\n",
    "def beam_search(predict_next_symbols,\n",
    "                initial_ids,\n",
    "                beam_size,\n",
    "                decode_length,\n",
    "                vocab_size,\n",
    "                alpha,\n",
    "                states=None,\n",
    "                eos_id=EOS_ID,\n",
    "                stop_early=True):\n",
    "    \"\"\"Beam search with length penalties.\n",
    "    \"\"\"\n",
    "    batch_size = shape_list(initial_ids)[0]\n",
    "    initial_log_probs = tf.constant(\n",
    "        [[0.] + (int(beam_size / 2) - 1) * [-float(\"inf\")] + [0.] + (int(beam_size / 2) - 1) * [-float(\"inf\")]])\n",
    "    alive_log_probs = tf.tile(initial_log_probs, [batch_size, 1])  # (batch_size, beam_size)\n",
    "\n",
    "    initial_ids_1 = L1_SYMBOL * tf.ones([batch_size, 1], dtype=tf.int32)  # index 2 == <l1>\n",
    "    initial_ids_2 = DELAY_SYMBOL * tf.ones([batch_size, 1], dtype=tf.int32)  # index 3 == <l2>\n",
    "\n",
    "    initial_ids_1 = tf.concat([initial_ids, initial_ids_1], axis=1)  # [batch, 2]\n",
    "    initial_ids_2 = tf.concat([initial_ids, initial_ids_2], axis=1)\n",
    "\n",
    "    alive_seq_1 = tf.tile(tf.expand_dims(initial_ids_1, 1),\n",
    "                          [1, tf.cast(beam_size / 2, tf.int32), 1])  # [batch, beam/2, 2]\n",
    "    alive_seq_2 = tf.tile(tf.expand_dims(initial_ids_2, 1), [1, tf.cast(beam_size / 2, tf.int32), 1])\n",
    "    alive_seq = tf.concat([alive_seq_1, alive_seq_2], axis=1)  # [batch, beam, 2]\n",
    "\n",
    "    states = nest.map_structure(\n",
    "        lambda state: _expand_to_beam_size(state, beam_size), states)\n",
    "    finished_seq_1 = tf.zeros(shape_list(alive_seq), tf.int32, name=\"fin-seq1\")\n",
    "    finished_seq_2 = tf.zeros(shape_list(alive_seq), tf.int32, name=\"fin-seq2\")\n",
    "    finished_scores_1 = tf.ones([batch_size, beam_size]) * -INF\n",
    "    finished_scores_2 = tf.ones([batch_size, beam_size]) * -INF\n",
    "    finished_flags_1 = tf.zeros([batch_size, beam_size], tf.bool, name=\"fin-flag1\")\n",
    "    finished_flags_2 = tf.zeros([batch_size, beam_size], tf.bool, name=\"fin-flag2\")\n",
    "\n",
    "    def _beam_search_step(i,\n",
    "                          alive_seq,\n",
    "                          alive_log_probs,\n",
    "                          finished_seq_1,\n",
    "                          finished_seq_2,\n",
    "                          finished_scores_1,\n",
    "                          finished_scores_2,\n",
    "                          finished_flags_1,\n",
    "                          finished_flags_2,\n",
    "                          states):\n",
    "        \"\"\"Inner beam seach loop.\n",
    "        \"\"\"\n",
    "        # 1. Get the current topk items.\n",
    "        flat_ids = tf.reshape(alive_seq, [batch_size * beam_size, -1])\n",
    "        flat_states = nest.map_structure(_merge_beam_dim, states)\n",
    "        flat_logits, flat_states = predict_next_symbols(flat_ids, i, batch_size, beam_size, flat_states)  # !!\n",
    "        states = nest.map_structure(\n",
    "            lambda t: _unmerge_beam_dim(t, batch_size, beam_size), flat_states)\n",
    "\n",
    "        logits = tf.reshape(flat_logits, [batch_size, beam_size, -1])  # (batch, beam, vocab)\n",
    "        candidate_log_probs = log_prob_from_logits(logits)  # softmax\n",
    "        log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)\n",
    "        length_penalty = tf.pow(((5. + tf.to_float(i + 1)) / 6.), alpha)\n",
    "        curr_scores = log_probs / length_penalty\n",
    "        # flat_curr_scores = tf.reshape(curr_scores, [-1, beam_size * vocab_size]) # (batch, beam*vocab)\n",
    "        flat_curr_scores = tf.reshape(curr_scores, [-1, 2, tf.cast(beam_size / 2,\n",
    "                                                                   tf.int32) * vocab_size])  # [batch, 2, (beam/2) * vocab]\n",
    "        # topk_scores, topk_ids = tf.nn.top_k(flat_curr_scores, k=beam_size * 2) # (batch, 2*beam)\n",
    "        topk_scores, topk_ids = tf.nn.top_k(flat_curr_scores, k=beam_size)  # [batch, 2, beam]\n",
    "        topk_log_probs = topk_scores * length_penalty\n",
    "        topk_log_probs = tf.reshape(topk_log_probs, [-1, 2 * beam_size])  # add; [batch, 2*beam]\n",
    "        topk_scores = tf.reshape(topk_scores, [-1, 2 * beam_size])\n",
    "        topk_beam_index = topk_ids // vocab_size  # like [[0,1,1,0],[1,1,0,0],[1,0,0,0],...], e.g. beam=2\n",
    "        topk_ids %= vocab_size  # Unflatten the ids\n",
    "\n",
    "        topk_beam_index_1 = tf.concat([tf.expand_dims(topk_beam_index[:, 0, :], 1),\n",
    "                                       tf.expand_dims(topk_beam_index[:, 1, :] + tf.cast(beam_size / 2, tf.int32), 1)],\n",
    "                                      axis=1)\n",
    "        topk_beam_index = tf.reshape(topk_beam_index_1, [-1, beam_size * 2])\n",
    "        topk_ids = tf.reshape(topk_ids, [-1, beam_size * 2])\n",
    "\n",
    "        batch_pos = compute_batch_indices(batch_size,\n",
    "                                          beam_size * 2)\n",
    "        # like [[0,0,0,0,],[1,1,1,1],[2,2,2,2],...] (batch, 2*beam)\n",
    "        topk_coordinates = tf.stack([batch_pos, topk_beam_index],\n",
    "                                    axis=2)\n",
    "        # like [[[0,0],[0,1],[0,1],[0,0]], [[1,1],[1,1],[1,0],[1,0]], [[2,1],[2,0],[2,0],[2,0]],...]  (batch, 2*beam, 2)\n",
    "        topk_seq = tf.gather_nd(alive_seq, topk_coordinates)  # (batch, 2*beam, lenght)\n",
    "        states = nest.map_structure(\n",
    "            lambda state: tf.gather_nd(state, topk_coordinates), states)\n",
    "        topk_seq = tf.concat([topk_seq, tf.expand_dims(topk_ids, axis=2)], axis=2)  # (batch, 2*beam, length+1)\n",
    "        topk_finished = tf.equal(topk_ids, eos_id)  # (batch, 2*beam)\n",
    "\n",
    "        # 2. Extract the ones that have finished and haven't finished\n",
    "        curr_scores = topk_scores + tf.to_float(topk_finished) * -INF  # (batch, 2*beam)\n",
    "        curr_scores = tf.reshape(curr_scores, [batch_size, 2, beam_size])\n",
    "        _, topk_indexes = tf.nn.top_k(curr_scores, k=tf.cast(beam_size / 2, tf.int32))  # [batch, 2, beam/2]\n",
    "        topk_indexes_tmp = topk_indexes[:, 1, :] + beam_size\n",
    "        topk_indexes = tf.concat([tf.expand_dims(topk_indexes[:, 0, :], 1), tf.expand_dims(topk_indexes_tmp, 1)],\n",
    "                                 axis=1)\n",
    "        topk_indexes = tf.reshape(topk_indexes, [batch_size, beam_size])\n",
    "\n",
    "        batch_pos_2 = compute_batch_indices(batch_size, beam_size)\n",
    "        top_coordinates = tf.stack([batch_pos_2, topk_indexes], axis=2)  # (batch, beam, 2)\n",
    "        alive_seq = tf.gather_nd(topk_seq, top_coordinates)\n",
    "        alive_log_probs = tf.gather_nd(topk_log_probs, top_coordinates)\n",
    "        alive_states = nest.map_structure(\n",
    "            lambda state: tf.gather_nd(state, top_coordinates), states)\n",
    "\n",
    "        # 3. Recompute the contents of finished based on scores.\n",
    "        finished_seq_1 = tf.concat(\n",
    "            [finished_seq_1,\n",
    "             tf.zeros([batch_size, beam_size, 1], tf.int32)], axis=2)\n",
    "        finished_seq_2 = tf.concat(\n",
    "            [finished_seq_2,\n",
    "             tf.zeros([batch_size, beam_size, 1], tf.int32)], axis=2)\n",
    "\n",
    "        curr_scores = topk_scores + (1. - tf.to_float(topk_finished)) * -INF\n",
    "\n",
    "        topk_seq_tmp_1 = tf.slice(topk_seq, [0, 0, 0], [batch_size, beam_size, -1])\n",
    "        topk_seq_tmp_2 = tf.slice(topk_seq, [0, beam_size, 0], [batch_size, beam_size, -1])\n",
    "        curr_finished_seq_1 = tf.concat([finished_seq_1, topk_seq_tmp_1], axis=1)\n",
    "        curr_finished_seq_2 = tf.concat([finished_seq_2, topk_seq_tmp_2], axis=1)\n",
    "\n",
    "        curr_scores_tmp_1 = tf.slice(curr_scores, [0, 0], [batch_size, beam_size])\n",
    "        curr_scores_tmp_2 = tf.slice(curr_scores, [0, beam_size], [batch_size, beam_size])\n",
    "        curr_finished_scores_1 = tf.concat([finished_scores_1, curr_scores_tmp_1], axis=1)\n",
    "        curr_finished_scores_2 = tf.concat([finished_scores_2, curr_scores_tmp_2], axis=1)\n",
    "\n",
    "        topk_finished_tmp_1 = tf.slice(topk_finished, [0, 0], [batch_size, beam_size])\n",
    "        topk_finished_tmp_2 = tf.slice(topk_finished, [0, beam_size], [batch_size, beam_size])\n",
    "        curr_finished_flags_1 = tf.concat([finished_flags_1, topk_finished_tmp_1], axis=1)\n",
    "        curr_finished_flags_2 = tf.concat([finished_flags_2, topk_finished_tmp_2], axis=1)\n",
    "\n",
    "        _, topk_indexes_tmp_1 = tf.nn.top_k(curr_finished_scores_1, k=beam_size)\n",
    "        _, topk_indexes_tmp_2 = tf.nn.top_k(curr_finished_scores_2, k=beam_size)\n",
    "\n",
    "        top_coordinates_tmp_1 = tf.stack([batch_pos_2, topk_indexes_tmp_1], axis=2)\n",
    "        top_coordinates_tmp_2 = tf.stack([batch_pos_2, topk_indexes_tmp_2], axis=2)\n",
    "        finished_seq_1 = tf.gather_nd(curr_finished_seq_1, top_coordinates_tmp_1)\n",
    "        finished_seq_2 = tf.gather_nd(curr_finished_seq_2, top_coordinates_tmp_2)\n",
    "        finished_flags_1 = tf.gather_nd(curr_finished_flags_1, top_coordinates_tmp_1)\n",
    "        finished_flags_2 = tf.gather_nd(curr_finished_flags_2, top_coordinates_tmp_2)\n",
    "        finished_scores_1 = tf.gather_nd(curr_finished_scores_1, top_coordinates_tmp_1)\n",
    "        finished_scores_2 = tf.gather_nd(curr_finished_scores_2, top_coordinates_tmp_2)\n",
    "\n",
    "        return (i + 1, alive_seq, alive_log_probs, finished_seq_1, finished_seq_2, finished_scores_1, finished_scores_2,\n",
    "                finished_flags_1, finished_flags_2, alive_states)\n",
    "\n",
    "    def _is_finished(i, unused_alive_seq, alive_log_probs, unused_finished_seq_1, unused_finished_seq_2,\n",
    "                     finished_scores_1, finished_scores_2, finished_flags_1, finished_flags_2, unused_states):\n",
    "        \"\"\"Checking termination condition.\n",
    "        \"\"\"\n",
    "        if not stop_early:\n",
    "            return tf.less(i, decode_length)\n",
    "        max_length_penalty = tf.pow(((5. + tf.to_float(decode_length)) / 6.), alpha)\n",
    "        lower_bound_alive_scores = alive_log_probs[:, 0] / max_length_penalty\n",
    "        lowest_score_of_finished_1 = tf.reduce_min(\n",
    "            finished_scores_1 * tf.to_float(finished_flags_1), axis=1)\n",
    "        lowest_score_of_finished_2 = tf.reduce_min(\n",
    "            finished_scores_2 * tf.to_float(finished_flags_2), axis=1)\n",
    "        lowest_score_of_finished_1 += (\n",
    "                (1. - tf.to_float(tf.reduce_any(finished_flags_1, 1))) * -INF)\n",
    "        lowest_score_of_finished_2 += (\n",
    "                (1. - tf.to_float(tf.reduce_any(finished_flags_2, 1))) * -INF)\n",
    "        bound_is_met = tf.reduce_all(  # return True when lowest_score_of_finished > lower_bound_alive_scores\n",
    "            tf.logical_and(tf.greater(lowest_score_of_finished_1, lower_bound_alive_scores),\n",
    "                           tf.greater(lowest_score_of_finished_2, lower_bound_alive_scores)))\n",
    "\n",
    "        return tf.logical_and(\n",
    "            tf.less(i, decode_length), tf.logical_not(bound_is_met))\n",
    "\n",
    "    (_, alive_seq, alive_log_probs, finished_seq_1, finished_seq_2, finished_scores_1, finished_scores_2,\n",
    "     finished_flags_1, finished_flags_2, _) = tf.while_loop(\n",
    "        _is_finished,  # termination when return False\n",
    "        _beam_search_step, [\n",
    "            tf.constant(0), alive_seq, alive_log_probs, finished_seq_1, finished_seq_2,\n",
    "            finished_scores_1, finished_scores_2, finished_flags_1, finished_flags_2, states],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape([]),\n",
    "            tf.TensorShape([None, None, None]),\n",
    "            alive_log_probs.get_shape(),\n",
    "            tf.TensorShape([None, None, None]),\n",
    "            tf.TensorShape([None, None, None]),\n",
    "            finished_scores_1.get_shape(),\n",
    "            finished_scores_2.get_shape(),\n",
    "            finished_flags_1.get_shape(),\n",
    "            finished_flags_2.get_shape(),\n",
    "            nest.map_structure(\n",
    "                lambda tensor: get_state_shape_invariants(tensor), states)],\n",
    "        parallel_iterations=1,\n",
    "        back_prop=False)\n",
    "\n",
    "    alive_seq.set_shape((None, beam_size, None))  # (batch, beam, length)\n",
    "    finished_seq_1.set_shape((None, beam_size, None))\n",
    "    finished_seq_2.set_shape((None, beam_size, None))\n",
    "\n",
    "    finished_seq_1 = tf.where(\n",
    "        tf.reduce_any(finished_flags_1, 1), finished_seq_1, alive_seq)\n",
    "    finished_seq_2 = tf.where(\n",
    "        tf.reduce_any(finished_flags_2, 1), finished_seq_2, alive_seq)\n",
    "    finished_scores_1 = tf.where(\n",
    "        tf.reduce_any(finished_flags_1, 1), finished_scores_1, alive_log_probs)\n",
    "    finished_scores_2 = tf.where(\n",
    "        tf.reduce_any(finished_flags_2, 1), finished_scores_2, alive_log_probs)\n",
    "    return finished_seq_1, finished_seq_2, finished_scores_1, finished_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l4hTBBEed3Wi"
   },
   "outputs": [],
   "source": [
    "def get_input_fn(mode,\n",
    "                 hparams,\n",
    "                 transform=True):\n",
    "    \"\"\"Provides input to the graph, either from disk or via a placeholder.\n",
    "    \"\"\"\n",
    "\n",
    "    def input_fn():\n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            prefix = os.path.join(hparams.data_dir, \"train.*.record\")\n",
    "        else:\n",
    "#             prefix = os.path.join(hparams.data_dir, \"test2015.0.record\")\n",
    "            prefix = os.path.join(hparams.data_dir, \"test.0.record\")\n",
    "            \n",
    "        data_file_patterns = sorted(glob.glob(prefix))\n",
    "        drop_long_sequences = mode == tf.contrib.learn.ModeKeys.TRAIN\n",
    "\n",
    "        with tf.name_scope(\"input_queues\"):\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "                    filename_queue = tf.train.string_input_producer(\n",
    "                        data_file_patterns, num_epochs=None, shuffle=True)\n",
    "                else:\n",
    "                    filename_queue = tf.train.string_input_producer(\n",
    "                        data_file_patterns, num_epochs=1, shuffle=False)\n",
    "\n",
    "                    \n",
    "\n",
    "                reader_tfRecord = tf.TFRecordReader()\n",
    "                _, serialized_example = reader_tfRecord.read(filename_queue)\n",
    "                features = tf.parse_single_example(\n",
    "                    serialized_example,\n",
    "                    features={'inputs': tf.FixedLenFeature([], tf.string),\n",
    "                              'target_l1': tf.FixedLenFeature([], tf.string),\n",
    "                              'target_l2': tf.FixedLenFeature([], tf.string)}\n",
    "                )\n",
    "\n",
    "                inputs = tf.reshape(tf.decode_raw(features['inputs'], tf.float32),\n",
    "                                    [-1, hparams.dim_feature])\n",
    "                                    \n",
    "                # TODO automatically change 3000\n",
    "                inputs = inputs[:3000, :] if drop_long_sequences else inputs\n",
    "\n",
    "                if transform:\n",
    "                    inputs = process_raw_feature(inputs, hparams.dim_feature, hparams.num_context, hparams.downsample)\n",
    "                target_l1 = tf.decode_raw(features['target_l1'], tf.int32)\n",
    "                target_l2 = tf.decode_raw(features['target_l2'], tf.int32)\n",
    "                \n",
    "                # TODO : I need to make it an hyperparam that can be changed when running the model \n",
    "                # Manually Add delay (for wait-k)\n",
    "#                 delay_target_l2 = tf.concat([tf.constant([DELAY_SYMBOL], tf.int32),\n",
    "#                                              target_l2[1:]], 0)\n",
    "                delay_target_l2 = tf.concat([tf.constant([DELAY_SYMBOL, DELAY_SYMBOL], tf.int32),\n",
    "                                             target_l2[1:]], 0)\n",
    "#                 delay_target_l2 = tf.concat([tf.constant([DELAY_SYMBOL, DELAY_SYMBOL, L2_SYMBOL], tf.int32),\n",
    "#                                              target_l2[1:]], 0)\n",
    "\n",
    "                feature_map = {\"inputs\": inputs, \"targets_l1\": target_l1,\n",
    "                               \"targets_l2\": delay_target_l2}\n",
    "\n",
    "                if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "                    feature_map = fentch_batch_bucket(feature_map)\n",
    "                else:\n",
    "                    feature_map = fentch_batch(feature_map)\n",
    "\n",
    "                targets_l1 = feature_map[\"targets_l1\"]\n",
    "                targets_l2 = feature_map[\"targets_l2\"]\n",
    "\n",
    "                targets_l1_length = tf.shape(targets_l1)[1]\n",
    "                targets_l2_length = tf.shape(targets_l2)[1]\n",
    "\n",
    "                targets_l1_pad, targets_l2_pad = tf.cond(\n",
    "                    tf.less(targets_l1_length, targets_l2_length),\n",
    "                    lambda: (tf.pad(targets_l1, [[0, 0], [0, targets_l2_length - targets_l1_length]]), targets_l2),\n",
    "                    lambda: (targets_l1, tf.pad(targets_l2, [[0, 0], [0, targets_l1_length - targets_l2_length]])))\n",
    "                    \n",
    "                feature_map[\"targets_l1\"] = targets_l1_pad\n",
    "                feature_map[\"targets_l2\"] = targets_l2_pad\n",
    "\n",
    "            # Ensure inputs and targets are proper rank.\n",
    "            while len(feature_map[\"inputs\"].get_shape()) != 3:\n",
    "                feature_map[\"inputs\"] = tf.expand_dims(feature_map[\"inputs\"], axis=-1)\n",
    "            while len(feature_map[\"targets_l1\"].get_shape()) != 4:\n",
    "                feature_map[\"targets_l1\"] = tf.expand_dims(feature_map[\"targets_l1\"], axis=-1)\n",
    "            while len(feature_map[\"targets_l2\"].get_shape()) != 4:\n",
    "                feature_map[\"targets_l2\"] = tf.expand_dims(feature_map[\"targets_l2\"], axis=-1)\n",
    "\n",
    "        rand_inputs, rand_target_l1, rand_target_l2 = \\\n",
    "            feature_map[\"inputs\"], feature_map[\"targets_l1\"], feature_map[\"targets_l2\"]\n",
    "\n",
    "        # Set shapes so the ranks are clear.\n",
    "        rand_inputs.set_shape([None, None, None])\n",
    "        rand_target_l1.set_shape([None, None, None, None])\n",
    "        rand_target_l2.set_shape([None, None, None, None])\n",
    "\n",
    "        # Final feature map.\n",
    "        rand_feature_map = {\"inputs\": rand_inputs, \"targets_l2\": rand_target_l2}\n",
    "        return rand_feature_map, rand_target_l1\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "\n",
    "def fentch_batch(features):\n",
    "    list_inputs = [features[\"inputs\"], features[\"targets_l1\"], features[\"targets_l2\"]]\n",
    "    list_outputs = tf.train.batch(\n",
    "        tensors=list_inputs,\n",
    "        batch_size=8,\n",
    "        num_threads=1,\n",
    "        capacity=2000,\n",
    "        dynamic_pad=True,\n",
    "        allow_smaller_final_batch=True\n",
    "    )\n",
    "    feature_map = {\"inputs\": list_outputs[0], \"targets_l1\": list_outputs[1],\n",
    "                   \"targets_l2\": list_outputs[2]}\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "def fentch_batch_bucket(features):\n",
    "    \"\"\"\n",
    "    the input tensor length is not equal,\n",
    "    so will add the len as a input tensor\n",
    "    list_inputs: [tensor1, tensor2]\n",
    "    added_list_inputs: [tensor1, tensor2, len_tensor1, len_tensor2]\n",
    "    \"\"\"\n",
    "    batch_size_list = [80, 64, 48, 32, 24, 16, 12, 8, 4]\n",
    "    bucket_boundaries_list = [100, 200, 404, 615, 828, 1065, 1360, 1792]\n",
    "    list_inputs = [features[\"inputs\"], features[\"targets_l1\"], features[\"targets_l2\"]]\n",
    "    _, list_outputs = tf.contrib.training.bucket_by_sequence_length(\n",
    "        input_length=tf.shape(features[\"inputs\"])[0],\n",
    "        tensors=list_inputs,\n",
    "        batch_size=batch_size_list,\n",
    "        bucket_boundaries=bucket_boundaries_list,\n",
    "        num_threads=8,\n",
    "        bucket_capacities=[i * 3 for i in batch_size_list],\n",
    "        capacity=2000,\n",
    "        dynamic_pad=True,\n",
    "        allow_smaller_final_batch=True)\n",
    "    feature_map = {\"inputs\": list_outputs[0], \"targets_l1\": list_outputs[1],\n",
    "                   \"targets_l2\": list_outputs[2]}\n",
    "\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "def process_raw_feature(seq_raw_features, dim_feature, num_context, downsample):\n",
    "    # 1-D, 2-D\n",
    "    # if add_delta:\n",
    "    #     seq_raw_features = add_delt(seq_raw_features)\n",
    "\n",
    "    # Splice\n",
    "    feature = splice(seq_raw_features,\n",
    "                             left_num=0,\n",
    "                             right_num=num_context)\n",
    "\n",
    "    # downsample\n",
    "    feature = down_sample(feature,\n",
    "                                  rate=downsample,\n",
    "                                  axis=0)\n",
    "\n",
    "    dim_input = dim_feature * (num_context + 1)\n",
    "    feature.set_shape([None, dim_input])\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zTSSAIsBd9ff"
   },
   "outputs": [],
   "source": [
    "PAD_idx = 0\n",
    "EOS_idx = 1\n",
    "L2R = 2\n",
    "R2L = 3\n",
    "\n",
    "\n",
    "def token_generator_three(source_path, target_path_l2r, target_path_r2l, token_vocab_src, token_vocab_tgt, eos=1, pad=1, l2r=1, r2l=1):\n",
    "    \"\"\"Generator for sequence-to-sequence tasks that uses tokens.\n",
    "    \"\"\"\n",
    "    eos_list = [] if eos is None else [PAD_idx]\n",
    "    pad_list = [] if pad is None else [EOS_idx]\n",
    "    l2r_list = [] if l2r is None else [L2R]\n",
    "    r2l_list = [] if r2l is None else [R2L]\n",
    "    with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "        with tf.gfile.GFile(target_path_l2r, mode=\"r\") as target_file_l2r:\n",
    "            with tf.gfile.GFile(target_path_r2l, mode=\"r\") as target_file_r2l:\n",
    "\n",
    "                source, target_l2r, target_r2l = source_file.readline(), target_file_l2r.readline(), target_file_r2l.readline()\n",
    "                while source and target_l2r and target_r2l:\n",
    "                    source_ints = token_vocab_src.encode(source.strip()) + eos_list \n",
    "        \n",
    "                    t_l2r = token_vocab_tgt.encode(target_l2r.strip())\n",
    "                    t_r2l = token_vocab_tgt.encode(target_r2l.strip())\n",
    "                    print (len(t_l2r))\n",
    "                    print (len(t_r2l))\n",
    "                    t_length_max = max(len(t_l2r),len(t_r2l))\n",
    "                    t_l2r_add_len = t_length_max - len(t_l2r)\n",
    "                    t_r2l_add_len = t_length_max - len(t_r2l)\n",
    "                    # let len(target_ints_l2r)==len(target_ints_r2l)\n",
    "                    target_ints_l2r = l2r_list + t_l2r + t_l2r_add_len*pad_list + eos_list\n",
    "                    target_ints_r2l = r2l_list + t_r2l + t_r2l_add_len*pad_list + eos_list\n",
    "\n",
    "                    yield {\"inputs\": source_ints, \"targets_l2r\": target_ints_l2r, \"targets_r2l\": target_ints_r2l}\n",
    "                    source, target_l2r, target_r2l = source_file.readline(), target_file_l2r.readline(), target_file_r2l.readline()\n",
    "\n",
    "\n",
    "def translation_token_generator(data_dir, tmp_dir, train_src_name, train_tgt_name, vocab_src_name, vocab_tgt_name):\n",
    "  \n",
    "    train_src_path = os.path.join(tmp_dir, train_src_name)\n",
    "    train_tgt_path_l2r = os.path.join(tmp_dir, train_tgt_name + \".l2r\")\n",
    "    train_tgt_path_r2l = os.path.join(tmp_dir, train_tgt_name + \".r2l\")\n",
    "\n",
    "    token_vocab_src_dir = os.path.join(data_dir, vocab_src_name)\n",
    "    token_vocab_tgt_dir = os.path.join(data_dir, vocab_tgt_name)\n",
    "    if not tf.gfile.Exists(token_vocab_src_dir):\n",
    "        tf.gfile.Copy(os.path.join(tmp_dir, vocab_src_name), token_vocab_src_dir)\n",
    "    if not tf.gfile.Exists(token_vocab_tgt_dir):\n",
    "        tf.gfile.Copy(os.path.join(tmp_dir, vocab_tgt_name), token_vocab_tgt_dir)\n",
    "\n",
    "    token_vocab_src = TokenTextEncoder(vocab_filename=token_vocab_src_dir)\n",
    "    token_vocab_tgt = TokenTextEncoder(vocab_filename=token_vocab_tgt_dir)\n",
    "    return token_generator_three(train_src_path, train_tgt_path_l2r, train_tgt_path_r2l, token_vocab_src, token_vocab_tgt, 1,1,1,1)\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def to_example(dictionary):\n",
    "    \"\"\"Helper: build tf.Example from (string -> int/float/str list) dictionary.\"\"\"\n",
    "    features = {}\n",
    "    for (k, v) in six.iteritems(dictionary):\n",
    "        if not v:\n",
    "            raise ValueError(\"Empty generated field: %s\", str((k, v)))\n",
    "        if isinstance(v[0], six.integer_types):\n",
    "            features[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n",
    "        elif isinstance(v[0], float):\n",
    "            features[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v))\n",
    "        elif isinstance(v[0], six.string_types):\n",
    "            if not six.PY2:  # Convert in python 3.\n",
    "                v = [bytes(x, \"utf-8\") for x in v]\n",
    "            features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n",
    "        elif isinstance(v[0], bytes):\n",
    "            features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n",
    "        else:\n",
    "            raise ValueError(\"Value for %s is not a recognized type; v: %s type: %s\" %\n",
    "                    (k, str(v[0]), str(type(v[0]))))\n",
    "    return tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "\n",
    "def generate_files(generator,\n",
    "                   output_name,\n",
    "                   output_dir,\n",
    "                   num_shards=1,\n",
    "                   max_cases=None):\n",
    "    \"\"\"Generate cases from a generator and save as TFRecord files.\n",
    "    \"\"\"\n",
    "    writers = []\n",
    "    output_files = []\n",
    "    for shard in xrange(num_shards):\n",
    "        output_filename = \"%s-%.5d-of-%.5d\" % (output_name, shard, num_shards)\n",
    "        output_file = os.path.join(output_dir, output_filename)\n",
    "        output_files.append(output_file)\n",
    "        writers.append(tf.python_io.TFRecordWriter(output_file))\n",
    "\n",
    "    counter, shard = 0, 0\n",
    "    for case in generator:\n",
    "        if counter > 0 and counter % 100000 == 0:\n",
    "            tf.logging.info(\"Generating case %d for %s.\" % (counter, output_name))\n",
    "        counter += 1\n",
    "        if max_cases and counter > max_cases:\n",
    "            break\n",
    "        sequence_example = to_example(case)\n",
    "        writers[shard].write(sequence_example.SerializeToString())\n",
    "        shard = (shard + 1) % num_shards\n",
    "\n",
    "    for writer in writers:\n",
    "        writer.close()\n",
    "\n",
    "    return output_files\n",
    "\n",
    "\n",
    "def read_records(filename):\n",
    "    reader = tf.python_io.tf_record_iterator(filename)\n",
    "    records = []\n",
    "    for record in reader:\n",
    "        records.append(record)\n",
    "    if len(records) % 100000 == 0:\n",
    "        tf.logging.info(\"read: %d\", len(records))\n",
    "    return records\n",
    "\n",
    "\n",
    "def write_records(records, out_filename):\n",
    "    writer = tf.python_io.TFRecordWriter(out_filename)\n",
    "    for count, record in enumerate(records):\n",
    "        writer.write(record)\n",
    "    if count > 0 and count % 100000 == 0:\n",
    "        tf.logging.info(\"write: %d\", count)\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zzhTg_h0eAUy"
   },
   "outputs": [],
   "source": [
    "\"\"\"Implemetation of beam seach with penalties.\"\"\"\n",
    "\n",
    "def _beam_decode(features, decode_length, beam_size, top_beams, alpha, local_features):\n",
    "\n",
    "    decoded_ids_l1, decoded_ids_l2, scores_l1, scores_l2 = \\\n",
    "        _fast_decode(features, decode_length, beam_size, top_beams, alpha, local_features)\n",
    "    return {\"outputs_l1\": decoded_ids_l1, \"outputs_l2\": decoded_ids_l2,\n",
    "            \"scores_l1\": scores_l1, \"scores_l2\": scores_l2}\n",
    "\n",
    "\n",
    "def _fast_decode(features,\n",
    "                 decode_length,\n",
    "                 beam_size=1,\n",
    "                 top_beams=1,\n",
    "                 alpha=1.0,\n",
    "                 local_features=None):\n",
    "    \"\"\"Fast decoding.\n",
    "    \"\"\"\n",
    "    if local_features[\"_num_datashards\"] != 1:\n",
    "        raise NotImplementedError(\"Fast decoding only supports a single shard.\")\n",
    "    dp = local_features[\"_data_parallelism\"]\n",
    "    hparams = local_features[\"_hparams\"]\n",
    "\n",
    "    inputs = features[\"inputs\"]\n",
    "    batch_size = tf.shape(features[\"inputs\"])[0]\n",
    "    target_modality = local_features[\"_hparams\"].target_modality\n",
    "    decode_length = tf.constant(decode_length)\n",
    "\n",
    "    input_modality = local_features[\"_hparams\"].input_modality\n",
    "    with tf.variable_scope(input_modality.name):\n",
    "        inputs = local_features[\"_shard_features\"]({\"inputs\": inputs})[\"inputs\"]\n",
    "    with tf.variable_scope(\"body\"):\n",
    "        encoder_output, encoder_decoder_attention_bias = dp(\n",
    "                local_features[\"encode\"], inputs, hparams)\n",
    "    encoder_output = encoder_output[0]\n",
    "    encoder_decoder_attention_bias = encoder_decoder_attention_bias[0]\n",
    "\n",
    "    if hparams.pos == \"timing\":\n",
    "        timing_signal = get_timing_signal_1d(\n",
    "                decode_length + 1, hparams.hidden_size)\n",
    "\n",
    "    def preprocess_targets(targets, i):\n",
    "        \"\"\"Performs preprocessing steps on the targets to prepare for the decoder.\n",
    "        Returns: Processed targets [batch_size, 1, hidden_dim]\n",
    "        \"\"\"\n",
    "        # _shard_features called to ensure that the variable names match\n",
    "        targets = local_features[\"_shard_features\"]({\"targets\": targets})[\"targets\"]\n",
    "        with tf.variable_scope(target_modality.name):\n",
    "                targets = target_modality.targets_bottom_sharded(targets, dp)[0]\n",
    "        targets = flatten4d3d(targets)\n",
    "\n",
    "        # TODO(llion): Explain! Is this even needed?\n",
    "        targets = tf.cond(\n",
    "            tf.equal(i, 0), lambda: tf.concat([tf.zeros_like(targets)[:,:1,:],targets[:,1:,:]], axis=1), lambda: targets)\n",
    "        \n",
    "        if hparams.pos == \"timing\":\n",
    "            timing_signal_1 = tf.cond(\n",
    "                        tf.equal(i, 0), lambda: timing_signal[:, i:i + 2], lambda: timing_signal[:, i+1:i + 2])\n",
    "            targets += timing_signal_1\n",
    "        return targets\n",
    "\n",
    "    decoder_self_attention_bias = (\n",
    "            attention_bias_lower_triangle(decode_length+1))\n",
    "\n",
    "    def predict_next_symbols(ids, i, batch_size, beam_size, cache):\n",
    "        \"\"\"Go from ids to logits for next symbol.\"\"\"\n",
    "        ids = tf.cond(\n",
    "                    tf.equal(i, 0), lambda: ids[:, -2:], lambda: ids[:, -1:])\n",
    "        targets = tf.expand_dims(tf.expand_dims(ids, axis=2), axis=3)\n",
    "        targets = preprocess_targets(targets, i)\n",
    "\n",
    "        bias_1 = decoder_self_attention_bias[:, :, i:i + 2, :i + 2]\n",
    "        bias_2 = decoder_self_attention_bias[:, :, i+1:i + 2, :i + 2]\n",
    "        bias = tf.cond(\n",
    "                    tf.equal(i, 0), lambda: bias_1, lambda: bias_2)\n",
    "        \n",
    "        s = tf.shape(cache['encoder_output'])\n",
    "        cache['encoder_output'] = tf.reshape(cache['encoder_output'],[s[0],s[1],hparams.hidden_size])\n",
    "        with tf.variable_scope(\"body\"):\n",
    "            body_outputs = dp(\n",
    "                local_features[\"decode\"], targets, cache[\"encoder_output\"],\n",
    "                    cache[\"encoder_decoder_attention_bias\"], bias, hparams, batch_size, beam_size, cache)\n",
    "\n",
    "        with tf.variable_scope(target_modality.name):\n",
    "            logits = target_modality.top_sharded(body_outputs, None, dp)[0]\n",
    "            \n",
    "        tf.logging.info(\"logits's shape is {0}\".format(logits[0].shape))\n",
    "        return tf.squeeze(logits, axis=[0, 3])[:, -1, :], cache\n",
    "\n",
    "    key_channels = hparams.hidden_size\n",
    "    value_channels = hparams.hidden_size\n",
    "    num_layers = hparams.num_hidden_layers\n",
    "\n",
    "    cache = {\n",
    "        \"layer_%d\" % layer: {\n",
    "                \"k\": tf.zeros([batch_size, 0, key_channels]),\n",
    "                \"v\": tf.zeros([batch_size, 0, value_channels]),\n",
    "        }\n",
    "        for layer in range(num_layers)\n",
    "    }\n",
    "\n",
    "    for layer in cache:\n",
    "        cache[layer][\"k\"].set_shape = tf.TensorShape([None, None, key_channels])\n",
    "        cache[layer][\"v\"].set_shape = tf.TensorShape([None, None, value_channels])\n",
    "    # pylint: enable=protected-access\n",
    "    cache[\"encoder_output\"] = encoder_output\n",
    "\n",
    "    cache[\"encoder_decoder_attention_bias\"] = encoder_decoder_attention_bias\n",
    "\n",
    "    target_modality = (\n",
    "            local_features[\"_hparams\"].target_modality)\n",
    "    vocab_size = target_modality.top_dimensionality\n",
    "    initial_ids = tf.zeros([batch_size, 1], dtype=tf.int32)\n",
    "    decoded_ids_l1, decoded_ids_l2, scores_l1, scores_l2 = beam_search(\n",
    "            predict_next_symbols,\n",
    "            initial_ids,\n",
    "            beam_size,\n",
    "            decode_length,\n",
    "            vocab_size,\n",
    "            alpha,\n",
    "            states=cache,\n",
    "            stop_early=(top_beams == 1))\n",
    "\n",
    "    if top_beams == 1:\n",
    "        decoded_ids_l1 = decoded_ids_l1[:, 0, 1:]\n",
    "        decoded_ids_l2 = decoded_ids_l2[:, 0, 1:]\n",
    "    else:\n",
    "        decoded_ids_l1 = decoded_ids_l1[:, :top_beams, 1:]\n",
    "        decoded_ids_l2 = decoded_ids_l2[:, :top_beams, 1:]\n",
    "\n",
    "    return decoded_ids_l1, decoded_ids_l2, scores_l1, scores_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "r7wbBUJweARL"
   },
   "outputs": [],
   "source": [
    "\"\"\"Utilities for creating Sparsely-Gated Mixture-of-Experts Layers.\n",
    "\n",
    "See the most recent draft of our ICLR paper:\n",
    "https://openreview.net/pdf?id=B1ckMDqlg\n",
    "\"\"\"\n",
    "\n",
    "@function.Defun(\n",
    "    python_grad_func=lambda x, dy: tf.convert_to_tensor(dy),\n",
    "    shape_func=lambda op: [op.inputs[0].get_shape()])\n",
    "def ConvertGradientToTensor(x):\n",
    "  \"\"\"Identity operation whose gradient is converted to a `Tensor`.\n",
    "\n",
    "  Currently, the gradient to `tf.concat` is particularly expensive to\n",
    "  compute if dy is an `IndexedSlices` (a lack of GPU implementation\n",
    "  forces the gradient operation onto CPU).  This situation occurs when\n",
    "  the output of the `tf.concat` is eventually passed to `tf.gather`.\n",
    "  It is sometimes faster to convert the gradient to a `Tensor`, so as\n",
    "  to get the cheaper gradient for `tf.concat`.  To do this, replace\n",
    "  `tf.concat(x)` with `ConvertGradientToTensor(tf.concat(x))`.\n",
    "\n",
    "  Args:\n",
    "    x: A `Tensor`.\n",
    "\n",
    "  Returns:\n",
    "    The input `Tensor`.\n",
    "  \"\"\"\n",
    "  return x\n",
    "\n",
    "\n",
    "class Parallelism(object):\n",
    "  \"\"\"Helper class for creating sets of parallel function calls.\n",
    "\n",
    "  The purpose of this class is to replace this code:\n",
    "\n",
    "      e = []\n",
    "      f = []\n",
    "      for i in xrange(len(devices)):\n",
    "        with tf.device(devices[i]):\n",
    "          e_, f_ = func(a[i], b[i], c)\n",
    "          e.append(e_)\n",
    "          f.append(f_)\n",
    "\n",
    "  with this code:\n",
    "\n",
    "      e, f = expert_utils.Parallelism(devices)(func, a, b, c)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               device_names_or_functions,\n",
    "               reuse=None,\n",
    "               caching_devices=None,\n",
    "               daisy_chain_variables=False):\n",
    "    \"\"\"Create a Parallelism.\n",
    "\n",
    "    Args:\n",
    "      device_names_or_functions: A list of of length n, containing device names\n",
    "        or device functions (see `tf.device`)\n",
    "      reuse: True or None.  Whether to reuse variables created in the first\n",
    "        replica in the subsequent replicas.\n",
    "      caching_devices: Either `None`, or a list of length n containing device\n",
    "        names.\n",
    "      daisy_chain_variables: a boolean - if true, then copies variables in a\n",
    "        daisy chain between devices.\n",
    "\n",
    "    Returns:\n",
    "      a Parallelism.\n",
    "    \"\"\"\n",
    "    assert device_names_or_functions\n",
    "    self._devices = device_names_or_functions\n",
    "    self._n = len(device_names_or_functions)\n",
    "    self._reuse = reuse\n",
    "    self._caching_devices = self._MaybeRepeat(caching_devices)\n",
    "    self._daisy_chain_variables = daisy_chain_variables\n",
    "\n",
    "  def __call__(self, fn, *args, **kwargs):\n",
    "    \"\"\"A parallel set of function calls (using the specified devices).\n",
    "\n",
    "    Args:\n",
    "      fn: a function or a list of n functions.\n",
    "      *args: additional args.  Each arg should either be not a list, or a list\n",
    "         of length n.\n",
    "      **kwargs: additional keyword args.  Each arg should either be not a\n",
    "         list, or a list of length n.\n",
    "\n",
    "    Returns:\n",
    "      either a single list of length n (if fn does not return a tuple), or a\n",
    "      tuple of lists of length n (if fn returns a tuple).\n",
    "    \"\"\"\n",
    "    # Construct lists or args and kwargs for each function.\n",
    "    if args:\n",
    "      my_args = TransposeListOfLists([self._MaybeRepeat(arg) for arg in args])\n",
    "    else:\n",
    "      my_args = [[] for _ in xrange(self.n)]\n",
    "    my_kwargs = [{} for _ in xrange(self.n)]\n",
    "    for k, v in six.iteritems(kwargs):\n",
    "      vals = self._MaybeRepeat(v)\n",
    "      for i in xrange(self.n):\n",
    "        my_kwargs[i][k] = vals[i]\n",
    "\n",
    "    # Construct lists of functions.\n",
    "    fns = self._MaybeRepeat(fn)\n",
    "\n",
    "    # Now make the parallel call.\n",
    "    outputs = []\n",
    "    cache = {}\n",
    "    for i in xrange(self.n):\n",
    "\n",
    "      def DaisyChainGetter(getter, name, *args, **kwargs):\n",
    "        \"\"\"Get a variable and cache in a daisy chain.\"\"\"\n",
    "        device_var_key = (self._devices[i], name)\n",
    "        if device_var_key in cache:\n",
    "          # if we have the variable on the correct device, return it.\n",
    "          return cache[device_var_key]\n",
    "        if name in cache:\n",
    "          # if we have it on a different device, copy it from the last device\n",
    "          v = tf.identity(cache[name])\n",
    "        else:\n",
    "          var = getter(name, *args, **kwargs)\n",
    "          v = tf.identity(var._ref())  # pylint: disable=protected-access\n",
    "        # update the cache\n",
    "        cache[name] = v\n",
    "        cache[device_var_key] = v\n",
    "        return v\n",
    "\n",
    "      # Variable scope will not reset caching_device on reused variables,\n",
    "      # so we make a custom getter that uses identity to cache the variable.\n",
    "      # pylint: disable=cell-var-from-loop\n",
    "      def CachingGetter(getter, name, *args, **kwargs):\n",
    "        v = getter(name, *args, **kwargs)\n",
    "        key = (self._caching_devices[i], name)\n",
    "        if key in cache:\n",
    "          return cache[key]\n",
    "        with tf.device(self._caching_devices[i]):\n",
    "          ret = tf.identity(v._ref())  # pylint: disable=protected-access\n",
    "        cache[key] = ret\n",
    "        return ret\n",
    "\n",
    "      if self._daisy_chain_variables:\n",
    "        custom_getter = DaisyChainGetter\n",
    "      elif self._caching_devices:\n",
    "        custom_getter = CachingGetter\n",
    "      else:\n",
    "        custom_getter = None\n",
    "      # pylint: enable=cell-var-from-loop\n",
    "      with tf.name_scope('parallel_%d' % i):\n",
    "        with tf.variable_scope(\n",
    "            tf.get_variable_scope(),\n",
    "            reuse=True if i > 0 and self._reuse else None,\n",
    "            caching_device=self._caching_devices[i],\n",
    "            custom_getter=custom_getter):\n",
    "          with tf.device(self._devices[i]):\n",
    "            outputs.append(fns[i](*my_args[i], **my_kwargs[i]))\n",
    "    if isinstance(outputs[0], tuple):\n",
    "      outputs = list(zip(*outputs))\n",
    "      outputs = tuple([list(o) for o in outputs])\n",
    "    return outputs\n",
    "\n",
    "  @property\n",
    "  def n(self):\n",
    "    return self._n\n",
    "\n",
    "  @property\n",
    "  def devices(self):\n",
    "    return self._devices\n",
    "\n",
    "  def _MaybeRepeat(self, x):\n",
    "    \"\"\"Utility function for processing arguments that are singletons or lists.\n",
    "\n",
    "    Args:\n",
    "      x: either a list of self.n elements, or not a list.\n",
    "\n",
    "    Returns:\n",
    "      a list of self.n elements.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "      assert len(x) == self.n\n",
    "      return x\n",
    "    else:\n",
    "      return [x] * self.n\n",
    "\n",
    "\n",
    "def Parallel(device_names_or_functions, fn, *args):\n",
    "  \"\"\"Deprecated interface.\n",
    "\n",
    "  Use `Parallelism(device_names_or_functions)(fn, *args)` instead.\n",
    "\n",
    "  Args:\n",
    "    device_names_or_functions: A list of length n.\n",
    "    fn: a function or a list of n functions.\n",
    "    *args: additional args.  Each arg should either be not a list, or a list\n",
    "       of length n.\n",
    "\n",
    "  Returns:\n",
    "    either a single list of length n (if fn does not return a tuple), or a\n",
    "    tuple of lists of length n (if fn returns a tuple).\n",
    "  \"\"\"\n",
    "  return Parallelism(device_names_or_functions)(fn, *args)\n",
    "\n",
    "\n",
    "def TransposeListOfLists(lol):\n",
    "  \"\"\"Transpose a list of equally-sized python lists.\n",
    "\n",
    "  Args:\n",
    "    lol: a list of lists\n",
    "  Returns:\n",
    "    a list of lists\n",
    "  \"\"\"\n",
    "  assert lol, 'cannot pass the empty list'\n",
    "  return [list(x) for x in zip(*lol)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DddNT1IQeAK2"
   },
   "outputs": [],
   "source": [
    "\"\"\"Encoders for text data.\n",
    "\n",
    "* TextEncoder: base class\n",
    "* ByteTextEncoder: for ascii text\n",
    "* TokenTextEncoder: with user-supplied vocabulary file\n",
    "* SubwordTextEncoder: invertible\n",
    "\"\"\"\n",
    "\n",
    "class TextEncoder(object):\n",
    "    \"\"\"Base class for converting from ints to/from human readable strings.\"\"\"\n",
    "\n",
    "    def __init__(self, num_reserved_ids=4):\n",
    "        self._num_reserved_ids = len(RESERVED_TOKENS)\n",
    "\n",
    "    def encode(self, s):\n",
    "\n",
    "        \"\"\"Transform a human-readable string into a sequence of int ids.\"\"\"\n",
    "        return [int(w) + self._num_reserved_ids for w in s.split()]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Transform a sequence of int ids into a human-readable string.\"\"\"\n",
    "\n",
    "        decoded_ids = []\n",
    "        for id_ in ids:\n",
    "            if 0 <= id_ < self._num_reserved_ids:\n",
    "                decoded_ids.append(RESERVED_TOKENS[int(id_)])\n",
    "            else:\n",
    "                decoded_ids.append(id_ - self._num_reserved_ids)\n",
    "        return \" \".join([str(d) for d in decoded_ids])\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class TokenTextEncoder(TextEncoder):\n",
    "    \"\"\"Encoder based on a user-supplied vocabulary.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_filename, reverse=False, num_reserved_ids=4):\n",
    "        \"\"\"Initialize from a file, one token per line.\"\"\"\n",
    "        super(TokenTextEncoder, self).__init__(num_reserved_ids=num_reserved_ids)\n",
    "        self._reverse = reverse\n",
    "        self._load_vocab_from_file(vocab_filename)\n",
    "\n",
    "    def encode(self, sentence, replace_oov=None):\n",
    "        \"\"\"Converts a space-separated string of tokens to a list of ids.\"\"\"\n",
    "        tokens = sentence.strip().split()\n",
    "        if replace_oov is not None:\n",
    "            tokens = [t if t in self._token_to_id else replace_oov for t in tokens]\n",
    "        ret = [self._token_to_id[tok] for tok in tokens]\n",
    "        return ret[::-1] if self._reverse else ret\n",
    "\n",
    "    def decode(self, ids):\n",
    "        seq = reversed(ids) if self._reverse else ids\n",
    "        return \" \".join([self._safe_id_to_token(i) for i in seq])\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self._id_to_token)\n",
    "\n",
    "    def _safe_id_to_token(self, idx):\n",
    "        return self._id_to_token.get(idx, \"ID_%d\" % idx)\n",
    "\n",
    "    def _load_vocab_from_file(self, filename):\n",
    "        \"\"\"Load vocab from a file.\"\"\"\n",
    "        self._token_to_id = {}\n",
    "        self._id_to_token = {}\n",
    "\n",
    "        for idx, tok in enumerate(RESERVED_TOKENS):\n",
    "            self._token_to_id[tok] = idx\n",
    "            self._id_to_token[idx] = tok\n",
    "\n",
    "        token_start_idx = self._num_reserved_ids\n",
    "        # token_start_idx = 0\n",
    "        with tf.gfile.Open(filename) as f:\n",
    "            \n",
    "            vocab = [line.strip().split() for line in f][0]\n",
    "            # TODO I need to modify vocabulary size automatically\n",
    "            vocab = vocab[:30000] if 30000 else vocab\n",
    "            \n",
    "            new_idx = token_start_idx -1\n",
    "            for _, tok in enumerate(vocab):\n",
    "\n",
    "                if tok not in self._token_to_id:\n",
    "                    new_idx += 1\n",
    "                    self._token_to_id[tok] = new_idx\n",
    "                    self._id_to_token[new_idx] = tok\n",
    "            \n",
    "#             for i, line in enumerate(f):\n",
    "#                 idx = token_start_idx + i\n",
    "#                 tok = line.strip().split()[0]\n",
    "#                 ipdb.set_trace()\n",
    "#                 self._token_to_id[tok] = idx\n",
    "#                 self._id_to_token[idx] = tok\n",
    "                \n",
    "            \n",
    "            assert len(self._token_to_id) == len(self._id_to_token)\n",
    "            print('vocab size is %d' % len(self._token_to_id))\n",
    "\n",
    "\n",
    "#     def _load_vocab_from_file(self, vocab_path, vocab_size=None):\n",
    "# #         vocab = [line.strip().split()[0] for line in open(vocab_path, 'r')]\n",
    "#         vocab = [line.strip().split() for line in open(vocab_path, 'r')][0]\n",
    "#         vocab = vocab[:vocab_size] if vocab_size else vocab\n",
    "#         token2idx = defaultdict()\n",
    "#         idx2token = defaultdict()\n",
    "\n",
    "#         for idx, tok in enumerate(RESERVED_TOKENS):\n",
    "#             token2idx[tok] = idx\n",
    "#             idx2token[idx] = tok\n",
    "#         token_start_idx = self._num_reserved_ids\n",
    "#         new_idx = token_start_idx -1\n",
    "#         for _, tok in enumerate(vocab):\n",
    "        \n",
    "#             if tok not in token2idx:\n",
    "#                 new_idx += 1\n",
    "#                 token2idx[tok] = new_idx\n",
    "#                 idx2token[new_idx] = tok\n",
    "                \n",
    "# #         for idx, tok in enumerate(vocab):\n",
    "# #             new_idx = token_start_idx + idx\n",
    "# #             token2idx[tok] = new_idx\n",
    "# #             idx2token[new_idx] = tok\n",
    "\n",
    "# #         ipdb.set_trace()\n",
    "#         assert len(token2idx) == len(idx2token)\n",
    "#         print('vocab size is %d' % len(token2idx))\n",
    "#         return token2idx, idx2token\n",
    "###########################################################\n",
    "def examples_queue(data_sources,\n",
    "                   data_fields_to_features,\n",
    "                   training,\n",
    "                   capacity=32,\n",
    "                   data_items_to_decoders=None,\n",
    "                   data_items_to_decode=None):\n",
    "    \"\"\"Contruct a queue of training or evaluation examples.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"examples_queue\"):\n",
    "        # Read serialized examples using slim parallel_reader.\n",
    "        num_epochs = None if training else 1\n",
    "        data_files = tf.contrib.slim.parallel_reader.get_data_files(data_sources)\n",
    "        num_readers = min(4 if training else 1, len(data_files))\n",
    "        _, example_serialized = tf.contrib.slim.parallel_reader.parallel_read(\n",
    "            data_sources,\n",
    "            tf.TFRecordReader,\n",
    "            num_epochs=num_epochs,\n",
    "            shuffle=training,\n",
    "            capacity=2 * capacity,\n",
    "            min_after_dequeue=capacity,\n",
    "            num_readers=num_readers)\n",
    "\n",
    "        if data_items_to_decoders is None:\n",
    "            data_items_to_decoders = {\n",
    "            field: tf.contrib.slim.tfexample_decoder.Tensor(field)\n",
    "            for field in data_fields_to_features\n",
    "        }\n",
    "\n",
    "        decoder = tf.contrib.slim.tfexample_decoder.TFExampleDecoder(\n",
    "            data_fields_to_features, data_items_to_decoders)\n",
    "\n",
    "        if data_items_to_decode is None:\n",
    "            data_items_to_decode = list(data_items_to_decoders)\n",
    "\n",
    "        decoded = decoder.decode(example_serialized, items=data_items_to_decode)\n",
    "        return {\n",
    "            field: tensor\n",
    "            for (field, tensor) in zip(data_items_to_decode, decoded)\n",
    "        }\n",
    "\n",
    "\n",
    "def input_pipeline(data_file_pattern, capacity, mode):\n",
    "    \"\"\"Input pipeline, returns a dictionary of tensors from queues.\"\"\"\n",
    "\n",
    "    data_fields = {\n",
    "        \"inputs\": tf.VarLenFeature(tf.int64),\n",
    "        #\"targets_l2r\": tf.VarLenFeature(tf.int64)}\n",
    "        \"targets_l2r\": tf.VarLenFeature(tf.int64),\n",
    "        \"targets_r2l\": tf.VarLenFeature(tf.int64)}\n",
    "    data_items_to_decoders = None\n",
    "\n",
    "    examples = examples_queue(\n",
    "        [data_file_pattern],\n",
    "        data_fields,\n",
    "        training=(mode == tf.contrib.learn.ModeKeys.TRAIN),\n",
    "        capacity=capacity,\n",
    "        data_items_to_decoders=data_items_to_decoders)\n",
    "\n",
    "    # We do not want int64s as they do are not supported on GPUs.\n",
    "    return {k: tf.to_int32(v) for (k, v) in six.iteritems(examples)}\n",
    "\n",
    "\n",
    "def batch_examples(examples, batching_scheme):\n",
    "    \"\"\"Given a queue of examples, create batches of examples with similar lengths.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"batch_examples\"):\n",
    "        # The queue to bucket on will be chosen based on maximum length.\n",
    "        max_length = 0\n",
    "        for v in examples.values():\n",
    "        # For images the sequence length is the size of the spatial dimensions.\n",
    "            sequence_length = (tf.shape(v)[0] if len(v.get_shape()) < 3 else\n",
    "                    tf.shape(v)[0] * tf.shape(v)[1])\n",
    "            max_length = tf.maximum(max_length, sequence_length)\n",
    "        (_, outputs) = tf.contrib.training.bucket_by_sequence_length(\n",
    "            max_length,\n",
    "            examples,\n",
    "            batching_scheme[\"batch_sizes\"],\n",
    "            [b + 1 for b in batching_scheme[\"boundaries\"]],\n",
    "            capacity=2,  # Number of full batches to store, we don't need many.\n",
    "            bucket_capacities=[2 * b for b in batching_scheme[\"batch_sizes\"]],\n",
    "            dynamic_pad=True,\n",
    "            keep_input=(max_length <= batching_scheme[\"max_length\"]))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def bucket_boundaries(max_length, min_length=8, mantissa_bits=2):\n",
    "    \"\"\"A default set of length-bucket boundaries.\"\"\"\n",
    "    x = min_length\n",
    "    boundaries = []\n",
    "    while x < max_length:\n",
    "        boundaries.append(x)\n",
    "        x += 2**max(0, int(math.log(x, 2)) - mantissa_bits)\n",
    "    return boundaries\n",
    "\n",
    "\n",
    "def hparams_to_batching_scheme(hparams,\n",
    "                               drop_long_sequences=False,\n",
    "                               shard_multiplier=1,\n",
    "                               length_multiplier=1):\n",
    "    \"\"\"A batching scheme based on model hyperparameters.\n",
    "    \"\"\"\n",
    "    max_length = hparams.max_length or hparams.batch_size\n",
    "    boundaries = bucket_boundaries(\n",
    "        max_length, mantissa_bits=hparams.batching_mantissa_bits)\n",
    "    batch_sizes = [\n",
    "        max(1, hparams.batch_size // length)\n",
    "        for length in boundaries + [max_length]\n",
    "    ]\n",
    "    batch_sizes = [b * shard_multiplier for b in batch_sizes]\n",
    "    max_length *= length_multiplier\n",
    "    boundaries = [boundary * length_multiplier for boundary in boundaries]\n",
    "    return {\n",
    "        \"boundaries\": boundaries,\n",
    "        \"batch_sizes\": batch_sizes,\n",
    "        \"max_length\": (max_length if drop_long_sequences else 10**9)\n",
    "    }\n",
    "\n",
    "\n",
    "def get_datasets(data_dir, mode):\n",
    "    \"\"\"Return the location of a dataset for a given mode.\"\"\"\n",
    "    datasets = []\n",
    "    for problem in [\"translation\", ]:\n",
    "    # for problem in [\"wmt_ende_bpe32k\", ]:\n",
    "        # problem, _, _ = common_hparams.parse_problem_name(problem)\n",
    "        path = os.path.join(data_dir, problem)\n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            datasets.append(\"%s-train*\" % path)\n",
    "        else:\n",
    "            datasets.append(\"%s-dev*\" % path)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "s-YGo-78eQcZ"
   },
   "outputs": [],
   "source": [
    "# tf fea opr\n",
    "def tf_kaldi_fea_delt1(features):\n",
    "    feats_padded = tf.pad(features, [[1, 1], [0, 0]], \"SYMMETRIC\")\n",
    "    feats_padded = tf.pad(feats_padded, [[1, 1], [0, 0]], \"SYMMETRIC\")\n",
    "\n",
    "    shape = tf.shape(features)\n",
    "    l2 = tf.slice(feats_padded, [0, 0], shape)\n",
    "    l1 = tf.slice(feats_padded, [1, 0], shape)\n",
    "    r1 = tf.slice(feats_padded, [3, 0], shape)\n",
    "    r2 = tf.slice(feats_padded, [4, 0], shape)\n",
    "\n",
    "    delt1 = (r1 - l1) * 0.1 + (r2 - l2) * 0.2\n",
    "    return delt1\n",
    "\n",
    "\n",
    "def tf_kaldi_fea_delt2(features):\n",
    "    feats_padded = tf.pad(features, [[1, 1], [0, 0]], \"SYMMETRIC\")\n",
    "    feats_padded = tf.pad(feats_padded, [[1, 1], [0, 0]], \"SYMMETRIC\")\n",
    "    feats_padded = tf.pad(feats_padded, [[1, 1], [0, 0]], \"SYMMETRIC\")\n",
    "    feats_padded = tf.pad(feats_padded, [[1, 1], [0, 0]], \"SYMMETRIC\")\n",
    "\n",
    "    shape = tf.shape(features)\n",
    "    l4 = tf.slice(feats_padded, [0, 0], shape)\n",
    "    l3 = tf.slice(feats_padded, [1, 0], shape)\n",
    "    l2 = tf.slice(feats_padded, [2, 0], shape)\n",
    "    l1 = tf.slice(feats_padded, [3, 0], shape)\n",
    "    c = tf.slice(feats_padded, [4, 0], shape)\n",
    "    r1 = tf.slice(feats_padded, [5, 0], shape)\n",
    "    r2 = tf.slice(feats_padded, [6, 0], shape)\n",
    "    r3 = tf.slice(feats_padded, [7, 0], shape)\n",
    "    r4 = tf.slice(feats_padded, [8, 0], shape)\n",
    "\n",
    "    delt2 = - 0.1 * c - 0.04 * (l1 + r1) + 0.01 * (l2 + r2) + 0.04 * (l3 + l4 + r4 + r3)\n",
    "    return delt2\n",
    "\n",
    "\n",
    "def add_delt(feature):\n",
    "    fb = []\n",
    "    fb.append(feature)\n",
    "    delt1 = tf_kaldi_fea_delt1(feature)\n",
    "    fb.append(delt1)\n",
    "    delt2 = tf_kaldi_fea_delt2(feature)\n",
    "    fb.append(delt2)\n",
    "    return tf.concat(axis=1, values=fb)\n",
    "\n",
    "\n",
    "def cmvn_global(feature, mean, var):\n",
    "    fea = (feature - mean) / var\n",
    "    return fea\n",
    "\n",
    "\n",
    "def cmvn_utt(feature):\n",
    "    fea_mean = tf.reduce_mean(feature, 0)\n",
    "    fea_var = tf.reduce_mean(tf.square(feature), 0)\n",
    "    fea_var = fea_var - fea_mean * fea_mean\n",
    "    fea_ivar = tf.rsqrt(fea_var + 1E-12)\n",
    "    fea = (feature - fea_mean) * fea_ivar\n",
    "    return fea\n",
    "\n",
    "\n",
    "def splice(features, left_num, right_num):\n",
    "    \"\"\"\n",
    "    [[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5], [6,6,6], [7,7,7]]\n",
    "    left_num=0, right_num=2:\n",
    "        [[1 1 1 2 2 2 3 3 3]\n",
    "         [2 2 2 3 3 3 4 4 4]\n",
    "         [3 3 3 4 4 4 5 5 5]\n",
    "         [4 4 4 5 5 5 6 6 6]\n",
    "         [5 5 5 6 6 6 7 7 7]\n",
    "         [6 6 6 7 7 7 0 0 0]\n",
    "         [7 7 7 0 0 0 0 0 0]]\n",
    "    \"\"\"\n",
    "    shape = tf.shape(features)\n",
    "    splices = []\n",
    "    pp = tf.pad(features, [[left_num, right_num], [0, 0]])\n",
    "    for i in range(left_num + right_num + 1):\n",
    "        splices.append(tf.slice(pp, [i, 0], shape))\n",
    "    splices = tf.concat(axis=1, values=splices)\n",
    "\n",
    "    return splices\n",
    "\n",
    "\n",
    "def down_sample(features, rate, axis=1):\n",
    "    \"\"\"\n",
    "    features: batch x time x deep\n",
    "    Notation: you need to set the shape of the output! tensor.set_shape(None, dim_input)\n",
    "    \"\"\"\n",
    "    len_seq = tf.shape(features)[axis]\n",
    "\n",
    "    return tf.gather(features, tf.range(len_seq, delta=rate), axis=axis)\n",
    "\n",
    "\n",
    "def target_delay(features, num_target_delay):\n",
    "    seq_len = tf.shape(features)[0]\n",
    "    feats_part1 = tf.slice(features, [num_target_delay, 0], [seq_len-num_target_delay, -1])\n",
    "    frame_last = tf.slice(features, [seq_len-1, 0], [1, -1])\n",
    "    feats_part2 = tf.concat([frame_last for _ in range(num_target_delay)], axis=0)\n",
    "    features = tf.concat([feats_part1, feats_part2], axis=0)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cfHKsm5FeQZX"
   },
   "outputs": [],
   "source": [
    "# def get_argument():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--tmp_dir\", default='./', help=\"Temporary storage directory.\")\n",
    "#     parser.add_argument(\"--data_dir\", default='./', help=\"Directory with training data.\")\n",
    "    \n",
    "#     parser.add_argument(\"--train_csv_name\", default='./', help=\"Filename of training data.\")\n",
    "#     parser.add_argument(\"--dev_csv_name\", default='./', help=\"Filename of dev data.\")\n",
    "#     parser.add_argument(\"--test_csv_name\", default='./', help=\"Filename of test data.\")\n",
    "    \n",
    "#     parser.add_argument(\"--wav_dir_train\", default='./', help=\"Wavefile path of training data.\")\n",
    "#     parser.add_argument(\"--wav_dir_dev\", default='./', help=\"Wavefile path of dev data.\")\n",
    "#     parser.add_argument(\"--wav_dir_test\", default='./', help=\"Wavefile path of test data.\")\n",
    "    \n",
    "#     parser.add_argument(\"--vocabA_name\", default='./', help=\"Vocab language A file name.\")\n",
    "#     parser.add_argument(\"--vocabB_name\", default='./', help=\"Vocab language B file name.\")\n",
    "    \n",
    "#     parser.add_argument(\"--vocab_size\", type=int, default=30000, help=\"Vocabulary size.\")\n",
    "    \n",
    "#     parser.add_argument(\"-d\", \"--dim_raw_input\", type=int, default=80, help=\"The dimension of input feature.\")\n",
    "#     args = parser.parse_args()\n",
    "#     return args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save2tfrecord(dataset, mode, dir_save, size_file=5000000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset = ASRdataSet(dataset, mode, dir_save, size_file)\n",
    "        mode: Train or Dev\n",
    "        dir_save: the dir to save the tfdata files\n",
    "        size_file: average size of each record file\n",
    "    Return:\n",
    "        a folder consist of `tfdata.info`, `*.record`\n",
    "    \"\"\"\n",
    "\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    num_token = 0\n",
    "    idx_file = -1\n",
    "    num_damaged_sample = 0\n",
    "    dim_feature = 0\n",
    "\n",
    "    for i, sample in enumerate(tqdm(dataset)):\n",
    "        if sample['inputs'] is None or not sample:\n",
    "            num_damaged_sample += 1\n",
    "            continue\n",
    "        assert len(sample) == 4\n",
    "\n",
    "        dim_feature = sample['inputs'].shape[-1]\n",
    "        if (num_token // size_file) > idx_file:\n",
    "            idx_file = num_token // size_file\n",
    "            print('saving to file {}/{}.{}.record'.format(dir_save, mode, idx_file))\n",
    "            writer = tf.python_io.TFRecordWriter('{}/{}.{}.record'.format(dir_save, mode, idx_file))\n",
    "\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={'inputs': _bytes_feature(sample['inputs'].tostring()),\n",
    "                         'target_l1': _bytes_feature(sample['target_l1'].tostring()),\n",
    "                         'target_l2': _bytes_feature(sample['target_l2'].tostring())}\n",
    "            )\n",
    "        )\n",
    "        writer.write(example.SerializeToString())\n",
    "        num_token += len(sample['inputs'])\n",
    "\n",
    "    with open(os.path.join(dir_save, '%s.info' % mode), 'w') as fw:\n",
    "        fw.write('data_file {}\\n'.format(dataset.list_files))\n",
    "        fw.write('dim_feature {}\\n'.format(dim_feature))\n",
    "        fw.write('num_tokens {}\\n'.format(num_token))\n",
    "        fw.write('size_dataset {}\\n'.format(i-num_damaged_sample+1))\n",
    "        fw.write('damaged samples: {}\\n'.format(num_damaged_sample))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "class DataSet(object):\n",
    "    def __init__(self, filepath, vocabA_path, vocabB_path, wav_path, dim_raw_input, vocab_size, _shuffle=False):\n",
    "        self.list_utterances = self.gen_utter_list(filepath)\n",
    "        self.list_files = filepath\n",
    "        \n",
    "        if _shuffle:\n",
    "            self.shuffle_utts()\n",
    "            \n",
    "        self.wav_path = wav_path\n",
    "        self.dim_raw_input = dim_raw_input\n",
    "        self._num_reserved_ids = len(RESERVED_TOKENS)\n",
    "        \n",
    "        self.token2idxA, self.idx2tokenA = self._load_vocab_from_file(vocabA_path, vocab_size)\n",
    "        self.token2idxB, self.idx2tokenB = self._load_vocab_from_file(vocabB_path, vocab_size)\n",
    "        \n",
    "        self.end_id = self.token2idxA['<EOS>']\n",
    "        self.id_l1 = self.token2idxA['<2L1>']\n",
    "        self.id_l2 = self.token2idxB['<2L2>']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utterance = self.list_utterances[idx]\n",
    "        # if len(utterance.strip().split('\\t'))!=3:\n",
    "        #     ipdb.set_trace()\n",
    "        wavname, target_l1, target_l2 = utterance.strip().split('\\t')\n",
    "#         wavname, target_l1, target_l2 = utterance.strip().split('\\t\\t')\n",
    "        try:\n",
    "            wavid = wavname.split(\"_\")[0]\n",
    "#             print(utterance.strip().split('\\t'))\n",
    "#             ipdb.set_trace()\n",
    "            feature = audio2vector(os.path.join(self.wav_path, wavname), self.dim_raw_input)\n",
    "#             feature = audio2vector(os.path.join(self.wav_path, wavid, wavname), self.dim_raw_input)\n",
    "        except:\n",
    "            print(\"wavefile {} is empty or damaged, we pass it away.\".format(wavname))\n",
    "            feature = None\n",
    "        target_l1 = np.array([self.id_l1] +\n",
    "                             [self.token2idxA.get(word, self.token2idxA['<UNK>']) for word in target_l1.split(' ')] +\n",
    "                             [self.end_id],\n",
    "                             dtype=np.int32)\n",
    "\n",
    "        target_l2 = np.array([self.id_l2] +\n",
    "                             [self.token2idxB.get(word, self.token2idxB['<UNK>']) for word in target_l2.split(' ')] +\n",
    "                             [self.end_id],\n",
    "                             dtype=np.int32)\n",
    "                             \n",
    "        sample = {'id': wavname, 'inputs': feature, 'target_l1': target_l1, 'target_l2': target_l2}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_utter_list(list_files):\n",
    "        with open(list_files, 'r') as f:\n",
    "            list_utter = f.readlines()\n",
    "        return list_utter\n",
    "\n",
    "    def shuffle_utts(self):\n",
    "        shuffle(self.list_utterances)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_utterances)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        utility the __getitem__ to impliment the __iter__\n",
    "        \"\"\"\n",
    "        for idx in range(len(self)):\n",
    "            yield self[idx]\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        return self.__getitem__(idx)\n",
    "\n",
    "    def _load_vocab_from_file(self, vocab_path, vocab_size=None):\n",
    "#         vocab = [line.strip().split()[0] for line in open(vocab_path, 'r')]\n",
    "        vocab = [line.strip().split() for line in open(vocab_path, 'r')][0]\n",
    "        vocab = vocab[:vocab_size] if vocab_size else vocab\n",
    "        token2idx = defaultdict()\n",
    "        idx2token = defaultdict()\n",
    "\n",
    "        for idx, tok in enumerate(RESERVED_TOKENS):\n",
    "            token2idx[tok] = idx\n",
    "            idx2token[idx] = tok\n",
    "        token_start_idx = self._num_reserved_ids\n",
    "        new_idx = token_start_idx -1\n",
    "        for _, tok in enumerate(vocab):\n",
    "        \n",
    "            if tok not in token2idx:\n",
    "                new_idx += 1\n",
    "                token2idx[tok] = new_idx\n",
    "                idx2token[new_idx] = tok\n",
    "                \n",
    "#         for idx, tok in enumerate(vocab):\n",
    "#             new_idx = token_start_idx + idx\n",
    "#             token2idx[tok] = new_idx\n",
    "#             idx2token[new_idx] = tok\n",
    "\n",
    "#         ipdb.set_trace()\n",
    "        assert len(token2idx) == len(idx2token)\n",
    "        print('vocab size is %d' % len(token2idx))\n",
    "        return token2idx, idx2token\n",
    "\n",
    "\n",
    "def audio2vector(audio_filename, dim_feature):\n",
    "    '''\n",
    "    Turn an audio file into feature representation.\n",
    "    16k wav, size 283K -> len 903\n",
    "    '''\n",
    "    if audio_filename.endswith('.wav'):\n",
    "        rate, sig = wav.read(audio_filename)\n",
    "    else:\n",
    "        raise IOError('NOT support file type or not a filename: {}'.format(audio_filename))\n",
    "    # Get fbank coefficients. numcep is the feature size\n",
    "#     orig_inputs = logfbank(sig, samplerate=rate, nfilt=dim_feature).astype(np.float32)\n",
    "    orig_inputs = logfbank(sig, samplerate=rate, nfilt=dim_feature, nfft=600).astype(np.float32)\n",
    "    orig_inputs = (orig_inputs - np.mean(orig_inputs)) / np.std(orig_inputs)\n",
    "\n",
    "    return orig_inputs\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = get_argument()\n",
    "#     wav_path_train = args.wav_dir_train\n",
    "#     wav_path_dev = args.wav_dir_dev\n",
    "#     wav_path_test = args.wav_dir_test\n",
    "\n",
    "#     vocabA_path = os.path.join(args.tmp_dir, args.vocabA_name)\n",
    "#     vocabB_path = os.path.join(args.tmp_dir, args.vocabB_name)\n",
    "\n",
    "#     train_csv_path = os.path.join(args.tmp_dir, args.train_csv_name)\n",
    "#     dev_csv_path = os.path.join(args.tmp_dir, args.dev_csv_name)\n",
    "#     test_csv_path = os.path.join(args.tmp_dir, args.test_csv_name)\n",
    "#     dim_raw_input = args.dim_raw_input\n",
    "#     vocab_size = args.vocab_size\n",
    "\n",
    "#     dataset_dev = DataSet(dev_csv_path, vocabA_path, vocabB_path, wav_path_dev,\n",
    "#                           dim_raw_input, vocab_size, _shuffle=False)\n",
    "#     save2tfrecord(dataset_dev, 'dev', args.data_dir)\n",
    "    \n",
    "#     dataset_test = DataSet(test_csv_path, vocabA_path, vocabB_path, wav_path_test,\n",
    "#                            dim_raw_input, vocab_size, _shuffle=False)\n",
    "#     save2tfrecord(dataset_test, 'test', args.data_dir)\n",
    "    \n",
    "#     dataset_train = DataSet(train_csv_path, vocabA_path, vocabB_path, wav_path_train,\n",
    "#                             dim_raw_input, vocab_size, _shuffle=True)\n",
    "#     save2tfrecord(dataset_train, 'train', args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XFTJuBxseQV7"
   },
   "outputs": [],
   "source": [
    "\"\"\"Utilities for trainer binary.\"\"\"\n",
    "\n",
    "\n",
    "def create_hparams():\n",
    "    \"\"\"Returns hyperparameters, including any flag value overrides.\n",
    "    \"\"\"\n",
    "    if FLAGS.hparams_set == \"transformer_params_base\":\n",
    "        hparams = transformer_params_base(FLAGS.data_dir, FLAGS.vocab_src_name, FLAGS.vocab_tgt_name) ## !!\n",
    "    elif FLAGS.hparams_set == \"transformer_params_big\":\n",
    "        hparams = transformer_params_big(FLAGS.data_dir, FLAGS.vocab_src_name, FLAGS.vocab_tgt_name) ## !!\n",
    "    elif FLAGS.hparams_set == \"transformer_params_small\":\n",
    "        hparams = transformer_params_small(FLAGS.data_dir, FLAGS.vocab_src_name, FLAGS.vocab_tgt_name)  ## !!\n",
    "#     elif FLAGS.hparams_set == \"transformer_params_listra\":\n",
    "#         hparams = transformer_params_listra(FLAGS.data_dir, FLAGS.vocab_src_name, FLAGS.vocab_tgt_name)  ## !!\n",
    "    else:\n",
    "        raise ValueError(\"Do not have right model params\")\n",
    "\n",
    "    hparams.vocab_src_size = FLAGS.vocab_src_size\n",
    "    hparams.vocab_tgt_size = FLAGS.vocab_tgt_size\n",
    "\n",
    "    if FLAGS.hparams:\n",
    "        hparams = hparams.parse(FLAGS.hparams)\n",
    "\n",
    "    hparams.add_hparam(\"data_dir\", FLAGS.data_dir)\n",
    "    hparams.add_hparam(\"dim_feature\", FLAGS.dim_feature)\n",
    "    hparams.add_hparam(\"num_context\", FLAGS.num_context)\n",
    "    hparams.add_hparam(\"downsample\", FLAGS.downsample)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def run(model, output_dir):\n",
    "    \"\"\"Runs an Estimator locally or distributed.\n",
    "    \"\"\"\n",
    "    # Build Params\n",
    "    tf.logging.info(\"Build Params...\")\n",
    "    hparams = create_hparams()\n",
    "\n",
    "    if FLAGS.train_steps == 0:\n",
    "        tf.logging.info(\"Prepare for Inference...\")\n",
    "        inference_run(model, hparams, output_dir)\n",
    "        return\n",
    "\n",
    "    tf.logging.info(\"Prepare for Training...\")\n",
    "    train_run(model, hparams, output_dir)\n",
    "    return\n",
    "\n",
    "\n",
    "def train_run(model, hparams, output_dir):\n",
    "    # Build Data\n",
    "    tf.logging.info(\"Build Data...\")\n",
    "    train_input_fn = get_input_fn(\n",
    "        mode=tf.contrib.learn.ModeKeys.TRAIN,\n",
    "        hparams=hparams)\n",
    "    \n",
    "    # Build Model\n",
    "    tf.logging.info(\"Build Model...\")\n",
    "    model_fn = model_builder(model, hparams=hparams)\n",
    "    \n",
    "    # Build Graph\n",
    "    tf.logging.info(\"Build Graph...\")\n",
    "    all_hooks = []\n",
    "    with ops.Graph().as_default() as g:\n",
    "        global_step = tf.train.create_global_step(g)\n",
    "        features, labels = train_input_fn()\n",
    "        model_fn_ops = model_fn(features, labels) # total_loss, train_op\n",
    "        ops.add_to_collection(ops.GraphKeys.LOSSES, model_fn_ops[0])\n",
    "\n",
    "        pre_saver = tf.train.Saver([var for var in tf.global_variables() if \"encoder\" in var.name])\n",
    "        print(FLAGS.pretrain_output_dir)\n",
    "        print(tf.train.latest_checkpoint(FLAGS.pretrain_output_dir))\n",
    "        # ipdb.set_trace()\n",
    "\n",
    "        saver = tf.train.Saver(sharded=True,\n",
    "                               max_to_keep=FLAGS.keep_checkpoint_max,\n",
    "                               defer_build=True,\n",
    "                               save_relative_paths=True)\n",
    "        tf.add_to_collection(tf.GraphKeys.SAVERS, saver)\n",
    "  \n",
    "        all_hooks.extend([\n",
    "            tf.train.StopAtStepHook(last_step=FLAGS.train_steps),\n",
    "            tf.train.NanTensorHook(model_fn_ops[0]),\n",
    "            tf.train.LoggingTensorHook(\n",
    "            {\n",
    "                'loss': model_fn_ops[0],\n",
    "                'step': global_step\n",
    "            },\n",
    "            every_n_iter=100),\n",
    "            tf.train.CheckpointSaverHook(\n",
    "                checkpoint_dir=output_dir,\n",
    "                save_secs=FLAGS.save_checkpoint_secs or None,\n",
    "                save_steps=FLAGS.save_checkpoint_steps or None,\n",
    "                saver=saver) \n",
    "        ])\n",
    "\n",
    "        with tf.train.MonitoredTrainingSession(\n",
    "                checkpoint_dir=output_dir,\n",
    "                hooks=all_hooks,\n",
    "                save_checkpoint_secs=0,  # Saving is handled by a hook.\n",
    "                config=session_config(gpu_mem_fraction=FLAGS.gpu_mem_fraction)) as mon_sess:\n",
    "#             ipdb.set_trace()\n",
    "            pre_saver.restore(mon_sess, tf.train.latest_checkpoint(FLAGS.pretrain_output_dir))\n",
    "            loss = None\n",
    "            while not mon_sess.should_stop():\n",
    "                _, loss = mon_sess.run([model_fn_ops[1], model_fn_ops[0]])\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _save_until_eos(hyp):\n",
    "        ret = []\n",
    "        index = 0\n",
    "        # until you reach <EOS> id\n",
    "        while index < len(hyp) and hyp[index] != 1:\n",
    "            ret.append(hyp[index])\n",
    "            index += 1\n",
    "        return np.array(ret)\n",
    "\n",
    "def inference_run(model, hparams, output_dir):\n",
    "\n",
    "    # Build Model\n",
    "    tf.logging.info(\"Build Model...\")\n",
    "\n",
    "    # Build Graph\n",
    "    tf.logging.info(\"Build Graph...\")\n",
    "#     ipdb.set_trace()\n",
    "    checkpoint_path = saver.latest_checkpoint(output_dir)\n",
    "    if not checkpoint_path:\n",
    "        raise LookupError(\"Couldn't find trained model at %s.\" % output_dir)\n",
    "\n",
    "    dev_input_fn = get_input_fn(\n",
    "        mode=tf.contrib.learn.ModeKeys.INFER,\n",
    "        hparams=hparams)\n",
    "    features, labels = dev_input_fn()\n",
    "\n",
    "    model_fn_inference = model_builder_inference(model, hparams=hparams)\n",
    "    infer_ops = model_fn_inference(features, labels) # predictions, None, None\n",
    "    predictions, targets = infer_ops[0], infer_ops[1]\n",
    "    mon_sess = tf.train.MonitoredSession(\n",
    "        session_creator=tf.train.ChiefSessionCreator(\n",
    "            checkpoint_filename_with_path=checkpoint_path,\n",
    "            config=session_config(gpu_mem_fraction=FLAGS.gpu_mem_fraction)))\n",
    "\n",
    "    targets_vocab = hparams.vocabulary[\"targets\"]\n",
    "    inputs_vocab = hparams.vocabulary[\"inputs\"]\n",
    "\n",
    "    inputs_l1 = []\n",
    "    decodes_l1 = []\n",
    "    scores_l1 = []\n",
    "    inputs_l2 = []\n",
    "    decodes_l2 = []\n",
    "    scores_l2 = []\n",
    "\n",
    "    start = time.clock()\n",
    "    decode_num = 0\n",
    "    with mon_sess as sess:\n",
    "        while True:\n",
    "            if mon_sess.should_stop():\n",
    "                break\n",
    "            preds, target = sess.run([predictions, targets])\n",
    "\n",
    "            first_tensor = list(preds.values())[0]\n",
    "            batch_length = first_tensor.shape[0]\n",
    "\n",
    "            def log_fn(inputs_l1, inputs_l2, outputs_l1, outputs_l2):\n",
    "                decoded_inputs_l1 = inputs_vocab.decode(_save_until_eos(inputs_l1.flatten())).replace(\"@@ \", \"\")\n",
    "                tf.logging.info(\"INPUT en: %s\" % decoded_inputs_l1)\n",
    "                decoded_outputs_l1 = inputs_vocab.decode(\n",
    "                    _save_until_eos(outputs_l1.flatten())).replace(\"@@ \", \"\")\n",
    "                tf.logging.info(\"OUPUT en: %s\" % decoded_outputs_l1)\n",
    "\n",
    "                decoded_inputs_l2 = targets_vocab.decode(_save_until_eos(inputs_l2.flatten())).replace(\"@@ \", \"\")\n",
    "                tf.logging.info(\"INPUT ln: %s\" % decoded_inputs_l2)\n",
    "                decoded_outputs_l2 = targets_vocab.decode(_save_until_eos(outputs_l2.flatten())).replace(\"@@ \", \"\")\n",
    "                tf.logging.info(\"OUPUT ln: %s\" % decoded_outputs_l2)\n",
    "                return decoded_inputs_l1, decoded_inputs_l2, decoded_outputs_l1, decoded_outputs_l2\n",
    "\n",
    "            for i in range(batch_length):\n",
    "                decode_num += 1\n",
    "                tf.logging.info(\"#########sentence {}#######\".format(decode_num))\n",
    "                result = {key: value[i] for key, value in six.iteritems(preds)}\n",
    "\n",
    "                if FLAGS.decode_return_beams:\n",
    "                    beam_decodes = []\n",
    "                    output_l1_beams = np.split(\n",
    "                        result[\"outputs_l1\"], FLAGS.decode_beam_size, axis=0)\n",
    "                    output_l2_beams = np.split(\n",
    "                        result[\"outputs_l2\"], FLAGS.decode_beam_size, axis=0)\n",
    "                    index = 0\n",
    "                    for output_l1, output_l2 in zip(output_l1_beams, output_l2_beams):\n",
    "                        index += 1\n",
    "                        tf.logging.info(\"##########beam {}########\".format(index))\n",
    "                        beam_decodes.append(log_fn(result[\"targets_l1\"], result[\"targets_l2\"], output_l1, output_l1))\n",
    "                else:\n",
    "                    input_text_1, input_text_2, output_text_1, output_text_2 = log_fn(result[\"targets_l1\"],\n",
    "                                                                                      result[\"targets_l2\"],\n",
    "                                                                                      result[\"outputs_l1\"],\n",
    "                                                                                      result[\"outputs_l2\"])\n",
    "                    inputs_l1.append(input_text_1)\n",
    "                    inputs_l2.append(input_text_2)\n",
    "                    decodes_l1.append(output_text_1)\n",
    "                    decodes_l2.append(output_text_2)\n",
    "                    scores_l1.append(result[\"scores_l1\"])\n",
    "                    scores_l2.append(result[\"scores_l2\"])\n",
    "\n",
    "    input_filename_l1 = os.path.join(FLAGS.output_dir, FLAGS.decode_to_file_l1) + \".ref\"\n",
    "    input_filename_l2 = os.path.join(FLAGS.output_dir, FLAGS.decode_to_file_l2) + \".ref\"\n",
    "    decode_filename_l1 = os.path.join(FLAGS.output_dir, FLAGS.decode_to_file_l1)\n",
    "    decode_filename_l2 = os.path.join(FLAGS.output_dir, FLAGS.decode_to_file_l2)\n",
    "    \n",
    "    tf.logging.info(\"Writing decodes into %s\" % decode_filename_l1)\n",
    "    tf.logging.info(\"Writing decodes into %s\" % decode_filename_l2)\n",
    "    inputfile_l1 = tf.gfile.Open(input_filename_l1, \"w\")\n",
    "    outfile_l1 = tf.gfile.Open(decode_filename_l1, \"w\")\n",
    "    inputfile_l2 = tf.gfile.Open(input_filename_l2, \"w\")\n",
    "    outfile_l2 = tf.gfile.Open(decode_filename_l2, \"w\")\n",
    "    for index in range(len(decodes_l1)):\n",
    "        inputfile_l1.write(\"%s\\n\" % inputs_l1[index])\n",
    "        outfile_l1.write(\"%s\\n\" % (decodes_l1[index]))\n",
    "        inputfile_l2.write(\"%s\\n\" % inputs_l2[index])\n",
    "        outfile_l2.write(\"%s\\n\" % (decodes_l2[index]))\n",
    "\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(\"Time used:\", elapsed)\n",
    "\n",
    "\n",
    "def model_builder_inference(model, hparams):\n",
    "\n",
    "    def model_fn(features, targets):\n",
    "\n",
    "        mode = tf.contrib.learn.ModeKeys.INFER\n",
    "        dp = data_parallelism()\n",
    "\n",
    "        model_class = Transformer(hparams, mode, dp)\n",
    "\n",
    "        result_list = model_class.infer(\n",
    "            features,\n",
    "            beam_size=FLAGS.decode_beam_size,\n",
    "            top_beams=(FLAGS.decode_beam_size if FLAGS.decode_return_beams else 1),\n",
    "            alpha=FLAGS.decode_alpha,\n",
    "            decode_length=FLAGS.decode_extra_length)\n",
    "        ret = {\n",
    "            \"outputs_l1\": result_list[\"outputs_l1\"],\n",
    "            \"outputs_l2\": result_list[\"outputs_l2\"],\n",
    "            \"scores_l1\": result_list[\"scores_l1\"],\n",
    "            \"scores_l2\": result_list[\"scores_l2\"]\n",
    "        }, targets, None\n",
    "        if \"inputs\" in features:\n",
    "            ret[0][\"inputs\"] = features[\"inputs\"]\n",
    "            ret[0][\"targets_l2\"] = features[\"targets_l2\"]\n",
    "            ret[0][\"targets_l1\"] = targets\n",
    "        if \"infer_targets\" in features:\n",
    "            ret[0][\"targets\"] = features[\"infer_targets\"]\n",
    "\n",
    "        return ret\n",
    "\n",
    "    return model_fn\n",
    "\n",
    "\n",
    "def validate_flags():\n",
    "    if not FLAGS.model:\n",
    "        raise ValueError(\"Must specify a model with --model.\")\n",
    "    if not (FLAGS.hparams_set or FLAGS.hparams_range):\n",
    "        raise ValueError(\"Must specify either --hparams_set or --hparams_range.\")\n",
    "    if not FLAGS.schedule:\n",
    "        raise ValueError(\"Must specify --schedule.\")\n",
    "    if not FLAGS.output_dir:\n",
    "        FLAGS.output_dir = \"/tmp/tensor2tensor\"\n",
    "        tf.logging.warning(\"It is strongly recommended to specify --output_dir. \"\n",
    "                       \"Using default output_dir=%s.\", FLAGS.output_dir)\n",
    "\n",
    "\n",
    "def session_config(gpu_mem_fraction=0.95):\n",
    "    \"\"\"The TensorFlow Session config to use.\"\"\"\n",
    "    graph_options = tf.GraphOptions(optimizer_options=tf.OptimizerOptions(\n",
    "        opt_level=tf.OptimizerOptions.L1, do_function_inlining=False))\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_mem_fraction)\n",
    "    config = tf.ConfigProto(\n",
    "        allow_soft_placement=True, graph_options=graph_options, gpu_options=gpu_options)\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def model_builder(model, hparams):\n",
    "\n",
    "    def model_fn(features, targets): \n",
    "        mode = tf.contrib.learn.ModeKeys.TRAIN\n",
    "        features[\"targets_l1\"] = targets\n",
    "        dp = data_parallelism()\n",
    "        tf.get_variable_scope().set_initializer(initializer(hparams))\n",
    "\n",
    "        # We choose which problem to process.\n",
    "        loss_moving_avgs = []  # Need loss moving averages for that.\n",
    "        # for n in xrange(1):\n",
    "        with tf.variable_scope(\"losses_avg\"):\n",
    "            loss_moving_avgs.append(\n",
    "                tf.get_variable(\"total_loss\", initializer=100.0, trainable=False))\n",
    "            tf.get_variable(\"training_loss\", initializer=100.0, trainable=False)\n",
    "            tf.get_variable(\"extra_loss\", initializer=100.0, trainable=False)\n",
    "\n",
    "        def get_model():\n",
    "            \"\"\"Build the model for the n-th problem, plus some added variables.\"\"\"\n",
    "            model_class = Transformer(hparams, mode, dp) ##!!!!\n",
    "            sharded_logits, training_loss, extra_loss = model_class.model_fn(features)\n",
    "\n",
    "            with tf.variable_scope(\"losses_avg\", reuse=True):\n",
    "                loss_moving_avg = tf.get_variable(\"training_loss\")\n",
    "                o1 = loss_moving_avg.assign(loss_moving_avg * 0.9 + training_loss * 0.1)\n",
    "                loss_moving_avg = tf.get_variable(\"extra_loss\")\n",
    "                o2 = loss_moving_avg.assign(loss_moving_avg * 0.9 + extra_loss * 0.1)\n",
    "                loss_moving_avg = tf.get_variable(\"total_loss\")\n",
    "                total_loss = training_loss + extra_loss\n",
    "                o3 = loss_moving_avg.assign(loss_moving_avg * 0.9 + total_loss * 0.1)\n",
    "            with tf.variable_scope(\"train_stats\"):  # Count steps for this problem.\n",
    "                problem_steps = tf.get_variable(\n",
    "                    \"steps\", initializer=0, trainable=False)\n",
    "                o4 = problem_steps.assign_add(1)\n",
    "            with tf.control_dependencies([o1, o2, o3, o4]):  # Make sure the ops run.\n",
    "                # Ensure the loss is a scalar here.\n",
    "                total_loss = tf.reshape(total_loss, [], name=\"total_loss_control_id\")\n",
    "            return [total_loss] + sharded_logits    # Need to flatten for cond later.\n",
    "\n",
    "        result_list = get_model()\n",
    "        sharded_logits, total_loss = result_list[1:], result_list[0]\n",
    "\n",
    "        # Some training statistics.\n",
    "        with tf.name_scope(\"training_stats\"):\n",
    "            learning_rate = hparams.learning_rate * learning_rate_decay(hparams)\n",
    "            learning_rate /= math.sqrt(float(FLAGS.worker_replicas))\n",
    "            tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "            global_step = tf.to_float(tf.train.get_global_step())\n",
    "\n",
    "        # Log trainable weights and add decay.\n",
    "        total_size, weight_decay_loss = 0, 0.0\n",
    "        all_weights = {v.name: v for v in tf.trainable_variables()}\n",
    "        for v_name in sorted(list(all_weights)):\n",
    "            v = all_weights[v_name]\n",
    "            v_size = int(np.prod(np.array(v.shape.as_list())))\n",
    "            # tf.logging.info(\"Weight  %s\\tshape    %s\\tsize    %d\",\n",
    "            #        v.name[:-2].ljust(80), str(v.shape).ljust(20), v_size)\n",
    "            total_size += v_size\n",
    "            if hparams.weight_decay > 0.0 and len(v.shape.as_list()) > 1:\n",
    "                # Add weight regularization if set and the weight is not a bias (dim>1).\n",
    "                with tf.device(v._ref().device):  # pylint: disable=protected-access\n",
    "                    v_loss = tf.nn.l2_loss(v) / v_size\n",
    "                weight_decay_loss += v_loss\n",
    "            is_body = len(v_name) > 5 and v_name[:5] == \"body/\"\n",
    "            if hparams.weight_noise > 0.0 and is_body:\n",
    "                # Add weight noise if set in hparams.\n",
    "                with tf.device(v._ref().device):  # pylint: disable=protected-access\n",
    "                    scale = learning_rate * 0.001\n",
    "                    noise = tf.truncated_normal(v.shape) * hparams.weight_noise * scale\n",
    "                    noise_op = v.assign_add(noise)\n",
    "                with tf.control_dependencies([noise_op]):\n",
    "                    total_loss = tf.identity(total_loss)\n",
    "        tf.logging.info(\"Total trainable variables size: %d\", total_size)\n",
    "        if hparams.weight_decay > 0.0:\n",
    "            total_loss += weight_decay_loss * hparams.weight_decay\n",
    "        total_loss = tf.identity(total_loss, name=\"total_loss\")\n",
    "\n",
    "        # Define the train_op for the TRAIN mode.\n",
    "        opt = _ConditionalOptimizer(hparams.optimizer, learning_rate, hparams)\n",
    "        tf.logging.info(\"Computing gradients for global model_fn.\")\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            name=\"training\",\n",
    "            loss=total_loss,\n",
    "            global_step=tf.train.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            clip_gradients=hparams.clip_grad_norm or None,\n",
    "            optimizer=opt,\n",
    "            colocate_gradients_with_ops=True)\n",
    "        tf.logging.info(\"Global model_fn finished.\")\n",
    "        return total_loss, train_op\n",
    "\n",
    "    return model_fn\n",
    "\n",
    "\n",
    "def initializer(hparams):\n",
    "    if hparams.initializer == \"orthogonal\":\n",
    "        return tf.orthogonal_initializer(gain=hparams.initializer_gain)\n",
    "    elif hparams.initializer == \"uniform\":\n",
    "        max_val = 0.1 * hparams.initializer_gain\n",
    "        return tf.random_uniform_initializer(-max_val, max_val)\n",
    "    elif hparams.initializer == \"normal_unit_scaling\":\n",
    "        return init_ops.variance_scaling_initializer(\n",
    "            hparams.initializer_gain, mode=\"fan_avg\", distribution=\"normal\")\n",
    "    elif hparams.initializer == \"uniform_unit_scaling\":\n",
    "        return init_ops.variance_scaling_initializer(\n",
    "            hparams.initializer_gain, mode=\"fan_avg\", distribution=\"uniform\")\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized initializer: %s\" % hparams.initializer)\n",
    "\n",
    "\n",
    "def learning_rate_decay(hparams):\n",
    "    \"\"\"Inverse-decay learning rate until warmup_steps, then decay.\"\"\"\n",
    "    warmup_steps = tf.to_float(\n",
    "        hparams.learning_rate_warmup_steps * FLAGS.worker_replicas)\n",
    "    step = tf.to_float(tf.train.get_global_step())\n",
    "    if hparams.learning_rate_decay_scheme == \"noam\":\n",
    "        return 5000.0 * hparams.hidden_size**-0.5 * tf.minimum(\n",
    "            (step + 1) * warmup_steps**-1.5, (step + 1)**-0.5)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized learning rate decay scheme: %s\" %\n",
    "                hparams.learning_rate_decay_scheme)\n",
    "\n",
    "\n",
    "class _ConditionalOptimizer(tf.train.Optimizer):\n",
    "    \"\"\"Conditional optimizer.\"\"\"\n",
    "\n",
    "    def __init__(self, optimizer_name, lr, hparams):\n",
    "\n",
    "        if optimizer_name == \"Adam\":\n",
    "            # We change the default epsilon for Adam and re-scale lr.\n",
    "            # Using LazyAdam as it's much faster for large vocabulary embeddings.\n",
    "            self._opt = tf.contrib.opt.LazyAdamOptimizer(\n",
    "                lr / 500.0,\n",
    "                beta1=hparams.optimizer_adam_beta1,\n",
    "                beta2=hparams.optimizer_adam_beta2,\n",
    "                epsilon=hparams.optimizer_adam_epsilon)\n",
    "        elif optimizer_name == \"Momentum\":\n",
    "            self._opt = tf.train.MomentumOptimizer(\n",
    "                lr, momentum=hparams.optimizer_momentum_momentum)\n",
    "        else:\n",
    "            self._opt = tf.contrib.layers.OPTIMIZER_CLS_NAMES[optimizer_name](lr)\n",
    "\n",
    "    def compute_gradients(self, loss, var_list, colocate_gradients_with_ops):\n",
    "        return self._opt.compute_gradients(\n",
    "                loss, var_list, colocate_gradients_with_ops=colocate_gradients_with_ops)\n",
    "\n",
    "    def apply_gradients(self, gradients, global_step=None, name=None):\n",
    "        return self._opt.apply_gradients(\n",
    "                gradients, global_step=global_step, name=name)\n",
    "\n",
    "\n",
    "def _gpu_order(num_gpus):\n",
    "    if FLAGS.gpu_order:\n",
    "        ret = [int(s) for s in FLAGS.gpu_order.split(\" \")]\n",
    "        if len(ret) == num_gpus:\n",
    "            return ret\n",
    "    return list(range(num_gpus))\n",
    "\n",
    "\n",
    "def data_parallelism(all_workers=False):\n",
    "    \"\"\"Over which devices do we split each training batch.\n",
    "    \"\"\"\n",
    "\n",
    "    if FLAGS.schedule == \"local_run\":\n",
    "        #assert not FLAGS.sync\n",
    "        datashard_devices = [\"gpu:%d\" % d for d in _gpu_order(FLAGS.worker_gpu)]\n",
    "        if FLAGS.locally_shard_to_cpu:\n",
    "            datashard_devices += [\"cpu:0\"]\n",
    "        caching_devices = None\n",
    "    \n",
    "    tf.logging.info(\"datashard_devices: %s\", datashard_devices)\n",
    "    tf.logging.info(\"caching_devices: %s\", caching_devices)\n",
    "    return Parallelism(\n",
    "            datashard_devices,\n",
    "            reuse=True,\n",
    "            caching_devices=caching_devices,\n",
    "            daisy_chain_variables=FLAGS.daisy_chain_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImBkgj9T3xbX"
   },
   "source": [
    "# Speech features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MKB9DWwB3zXP"
   },
   "outputs": [],
   "source": [
    "# calculate filterbank features. Provides e.g. fbank and mfcc features for use in ASR applications\n",
    "# Author: James Lyons 2012\n",
    "from __future__ import division\n",
    "import numpy\n",
    "from python_speech_features import sigproc\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "\n",
    "def mfcc(signal,samplerate=16000,winlen=0.025,winstep=0.01,numcep=13,\n",
    "         nfilt=26,nfft=512,lowfreq=0,highfreq=None,preemph=0.97,ceplifter=22,appendEnergy=True,\n",
    "         winfunc=lambda x:numpy.ones((x,))):\n",
    "    \"\"\"Compute MFCC features from an audio signal.\n",
    "\n",
    "    :param signal: the audio signal from which to compute features. Should be an N*1 array\n",
    "    :param samplerate: the samplerate of the signal we are working with.\n",
    "    :param winlen: the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n",
    "    :param winstep: the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n",
    "    :param numcep: the number of cepstrum to return, default 13\n",
    "    :param nfilt: the number of filters in the filterbank, default 26.\n",
    "    :param nfft: the FFT size. Default is 512.\n",
    "    :param lowfreq: lowest band edge of mel filters. In Hz, default is 0.\n",
    "    :param highfreq: highest band edge of mel filters. In Hz, default is samplerate/2\n",
    "    :param preemph: apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n",
    "    :param ceplifter: apply a lifter to final cepstral coefficients. 0 is no lifter. Default is 22.\n",
    "    :param appendEnergy: if this is true, the zeroth cepstral coefficient is replaced with the log of the total frame energy.\n",
    "    :param winfunc: the analysis window to apply to each frame. By default no window is applied. You can use numpy window functions here e.g. winfunc=numpy.hamming\n",
    "    :returns: A numpy array of size (NUMFRAMES by numcep) containing features. Each row holds 1 feature vector.\n",
    "    \"\"\"\n",
    "    feat, energy = fbank(signal,samplerate,winlen,winstep,nfilt,nfft,lowfreq,highfreq,preemph,winfunc)\n",
    "    feat = numpy.log(feat)\n",
    "    feat = dct(feat, type=2, axis=1, norm='ortho')[:,:numcep]\n",
    "    feat = lifter(feat,ceplifter)\n",
    "    if appendEnergy: feat[:,0] = numpy.log(energy) # replace first cepstral coefficient with log of frame energy\n",
    "    return feat\n",
    "\n",
    "\n",
    "def fbank(signal,samplerate=16000,winlen=0.025,winstep=0.01,\n",
    "          nfilt=26,nfft=512,lowfreq=0,highfreq=None,preemph=0.97,\n",
    "          winfunc=lambda x:numpy.ones((x,))):\n",
    "    \"\"\"Compute Mel-filterbank energy features from an audio signal.\n",
    "\n",
    "    :param signal: the audio signal from which to compute features. Should be an N*1 array\n",
    "    :param samplerate: the samplerate of the signal we are working with.\n",
    "    :param winlen: the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n",
    "    :param winstep: the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n",
    "    :param nfilt: the number of filters in the filterbank, default 26.\n",
    "    :param nfft: the FFT size. Default is 512.\n",
    "    :param lowfreq: lowest band edge of mel filters. In Hz, default is 0.\n",
    "    :param highfreq: highest band edge of mel filters. In Hz, default is samplerate/2\n",
    "    :param preemph: apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n",
    "    :param winfunc: the analysis window to apply to each frame. By default no window is applied. You can use numpy window functions here e.g. winfunc=numpy.hamming\n",
    "    :returns: 2 values. The first is a numpy array of size (NUMFRAMES by nfilt) containing features. Each row holds 1 feature vector. The\n",
    "        second return value is the energy in each frame (total energy, unwindowed)\n",
    "    \"\"\"\n",
    "    highfreq= highfreq or samplerate/2\n",
    "    signal = sigproc.preemphasis(signal,preemph)\n",
    "    frames = sigproc.framesig(signal, winlen*samplerate, winstep*samplerate, winfunc)\n",
    "    pspec = sigproc.powspec(frames,nfft)\n",
    "    energy = numpy.sum(pspec,1) # this stores the total energy in each frame\n",
    "    energy = numpy.where(energy == 0,numpy.finfo(float).eps,energy) # if energy is zero, we get problems with log\n",
    "\n",
    "    fb = get_filterbanks(nfilt,nfft,samplerate,lowfreq,highfreq)\n",
    "    feat = numpy.dot(pspec,fb.T) # compute the filterbank energies\n",
    "    feat = numpy.where(feat == 0,numpy.finfo(float).eps,feat) # if feat is zero, we get problems with log\n",
    "\n",
    "    return feat,energy\n",
    "\n",
    "def logfbank(signal,samplerate=16000,winlen=0.025,winstep=0.01,\n",
    "          nfilt=26,nfft=512,lowfreq=0,highfreq=None,preemph=0.97):\n",
    "    \"\"\"Compute log Mel-filterbank energy features from an audio signal.\n",
    "\n",
    "    :param signal: the audio signal from which to compute features. Should be an N*1 array\n",
    "    :param samplerate: the samplerate of the signal we are working with.\n",
    "    :param winlen: the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n",
    "    :param winstep: the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n",
    "    :param nfilt: the number of filters in the filterbank, default 26.\n",
    "    :param nfft: the FFT size. Default is 512.\n",
    "    :param lowfreq: lowest band edge of mel filters. In Hz, default is 0.\n",
    "    :param highfreq: highest band edge of mel filters. In Hz, default is samplerate/2\n",
    "    :param preemph: apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n",
    "    :returns: A numpy array of size (NUMFRAMES by nfilt) containing features. Each row holds 1 feature vector.\n",
    "    \"\"\"\n",
    "    feat,energy = fbank(signal,samplerate,winlen,winstep,nfilt,nfft,lowfreq,highfreq,preemph)\n",
    "    return numpy.log(feat)\n",
    "\n",
    "def ssc(signal,samplerate=16000,winlen=0.025,winstep=0.01,\n",
    "        nfilt=26,nfft=512,lowfreq=0,highfreq=None,preemph=0.97,\n",
    "        winfunc=lambda x:numpy.ones((x,))):\n",
    "    \"\"\"Compute Spectral Subband Centroid features from an audio signal.\n",
    "\n",
    "    :param signal: the audio signal from which to compute features. Should be an N*1 array\n",
    "    :param samplerate: the samplerate of the signal we are working with.\n",
    "    :param winlen: the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n",
    "    :param winstep: the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n",
    "    :param nfilt: the number of filters in the filterbank, default 26.\n",
    "    :param nfft: the FFT size. Default is 512.\n",
    "    :param lowfreq: lowest band edge of mel filters. In Hz, default is 0.\n",
    "    :param highfreq: highest band edge of mel filters. In Hz, default is samplerate/2\n",
    "    :param preemph: apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n",
    "    :param winfunc: the analysis window to apply to each frame. By default no window is applied. You can use numpy window functions here e.g. winfunc=numpy.hamming\n",
    "    :returns: A numpy array of size (NUMFRAMES by nfilt) containing features. Each row holds 1 feature vector.\n",
    "    \"\"\"\n",
    "    highfreq= highfreq or samplerate/2\n",
    "    signal = sigproc.preemphasis(signal,preemph)\n",
    "    frames = sigproc.framesig(signal, winlen*samplerate, winstep*samplerate, winfunc)\n",
    "    pspec = sigproc.powspec(frames,nfft)\n",
    "    pspec = numpy.where(pspec == 0,numpy.finfo(float).eps,pspec) # if things are all zeros we get problems\n",
    "\n",
    "    fb = get_filterbanks(nfilt,nfft,samplerate,lowfreq,highfreq)\n",
    "    feat = numpy.dot(pspec,fb.T) # compute the filterbank energies\n",
    "    R = numpy.tile(numpy.linspace(1,samplerate/2,numpy.size(pspec,1)),(numpy.size(pspec,0),1))\n",
    "\n",
    "    return numpy.dot(pspec*R,fb.T) / feat\n",
    "\n",
    "def hz2mel(hz):\n",
    "    \"\"\"Convert a value in Hertz to Mels\n",
    "\n",
    "    :param hz: a value in Hz. This can also be a numpy array, conversion proceeds element-wise.\n",
    "    :returns: a value in Mels. If an array was passed in, an identical sized array is returned.\n",
    "    \"\"\"\n",
    "    return 2595 * numpy.log10(1+hz/700.)\n",
    "\n",
    "def mel2hz(mel):\n",
    "    \"\"\"Convert a value in Mels to Hertz\n",
    "\n",
    "    :param mel: a value in Mels. This can also be a numpy array, conversion proceeds element-wise.\n",
    "    :returns: a value in Hertz. If an array was passed in, an identical sized array is returned.\n",
    "    \"\"\"\n",
    "    return 700*(10**(mel/2595.0)-1)\n",
    "\n",
    "def get_filterbanks(nfilt=20,nfft=512,samplerate=16000,lowfreq=0,highfreq=None):\n",
    "    \"\"\"Compute a Mel-filterbank. The filters are stored in the rows, the columns correspond\n",
    "    to fft bins. The filters are returned as an array of size nfilt * (nfft/2 + 1)\n",
    "\n",
    "    :param nfilt: the number of filters in the filterbank, default 20.\n",
    "    :param nfft: the FFT size. Default is 512.\n",
    "    :param samplerate: the samplerate of the signal we are working with. Affects mel spacing.\n",
    "    :param lowfreq: lowest band edge of mel filters, default 0 Hz\n",
    "    :param highfreq: highest band edge of mel filters, default samplerate/2\n",
    "    :returns: A numpy array of size nfilt * (nfft/2 + 1) containing filterbank. Each row holds 1 filter.\n",
    "    \"\"\"\n",
    "    highfreq= highfreq or samplerate/2\n",
    "    assert highfreq <= samplerate/2, \"highfreq is greater than samplerate/2\"\n",
    "\n",
    "    # compute points evenly spaced in mels\n",
    "    lowmel = hz2mel(lowfreq)\n",
    "    highmel = hz2mel(highfreq)\n",
    "    melpoints = numpy.linspace(lowmel,highmel,nfilt+2)\n",
    "    # our points are in Hz, but we use fft bins, so we have to convert\n",
    "    #  from Hz to fft bin number\n",
    "    bin = numpy.floor((nfft+1)*mel2hz(melpoints)/samplerate)\n",
    "\n",
    "    fbank = numpy.zeros([nfilt,nfft//2+1])\n",
    "    for j in range(0,nfilt):\n",
    "        for i in range(int(bin[j]), int(bin[j+1])):\n",
    "            fbank[j,i] = (i - bin[j]) / (bin[j+1]-bin[j])\n",
    "        for i in range(int(bin[j+1]), int(bin[j+2])):\n",
    "            fbank[j,i] = (bin[j+2]-i) / (bin[j+2]-bin[j+1])\n",
    "    return fbank\n",
    "\n",
    "def lifter(cepstra, L=22):\n",
    "    \"\"\"Apply a cepstral lifter the the matrix of cepstra. This has the effect of increasing the\n",
    "    magnitude of the high frequency DCT coeffs.\n",
    "\n",
    "    :param cepstra: the matrix of mel-cepstra, will be numframes * numcep in size.\n",
    "    :param L: the liftering coefficient to use. Default is 22. L <= 0 disables lifter.\n",
    "    \"\"\"\n",
    "    if L > 0:\n",
    "        nframes,ncoeff = numpy.shape(cepstra)\n",
    "        n = numpy.arange(ncoeff)\n",
    "        lift = 1 + (L/2.)*numpy.sin(numpy.pi*n/L)\n",
    "        return lift*cepstra\n",
    "    else:\n",
    "        # values of L <= 0, do nothing\n",
    "        return cepstra\n",
    "\n",
    "def delta(feat, N):\n",
    "    \"\"\"Compute delta features from a feature vector sequence.\n",
    "\n",
    "    :param feat: A numpy array of size (NUMFRAMES by number of features) containing features. Each row holds 1 feature vector.\n",
    "    :param N: For each frame, calculate delta features based on preceding and following N frames\n",
    "    :returns: A numpy array of size (NUMFRAMES by number of features) containing delta features. Each row holds 1 delta feature vector.\n",
    "    \"\"\"\n",
    "    if N < 1:\n",
    "        raise ValueError('N must be an integer >= 1')\n",
    "    NUMFRAMES = len(feat)\n",
    "    denominator = 2 * sum([i**2 for i in range(1, N+1)])\n",
    "    delta_feat = numpy.empty_like(feat)\n",
    "    padded = numpy.pad(feat, ((N, N), (0, 0)), mode='edge')   # padded version of feat\n",
    "    for t in range(NUMFRAMES):\n",
    "        delta_feat[t] = numpy.dot(numpy.arange(-N, N+1), padded[t : t+2*N+1]) / denominator   # [t : t+2*N+1] == [(N+t)-N : (N+t)+N+1]\n",
    "    return delta_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jEmSsPFS3zT9"
   },
   "outputs": [],
   "source": [
    "# This file includes routines for basic signal processing including framing and computing power spectra.\n",
    "# Author: James Lyons 2012\n",
    "import decimal\n",
    "\n",
    "import numpy\n",
    "import math\n",
    "import logging\n",
    "\n",
    "\n",
    "def round_half_up(number):\n",
    "    return int(decimal.Decimal(number).quantize(decimal.Decimal('1'), rounding=decimal.ROUND_HALF_UP))\n",
    "\n",
    "\n",
    "def rolling_window(a, window, step=1):\n",
    "    # http://ellisvalentiner.com/post/2017-03-21-np-strides-trick\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return numpy.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::step]\n",
    "\n",
    "\n",
    "def framesig(sig, frame_len, frame_step, winfunc=lambda x: numpy.ones((x,)), stride_trick=True):\n",
    "    \"\"\"Frame a signal into overlapping frames.\n",
    "\n",
    "    :param sig: the audio signal to frame.\n",
    "    :param frame_len: length of each frame measured in samples.\n",
    "    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.\n",
    "    :param winfunc: the analysis window to apply to each frame. By default no window is applied.\n",
    "    :param stride_trick: use stride trick to compute the rolling window and window multiplication faster\n",
    "    :returns: an array of frames. Size is NUMFRAMES by frame_len.\n",
    "    \"\"\"\n",
    "    slen = len(sig)\n",
    "    frame_len = int(round_half_up(frame_len))\n",
    "    frame_step = int(round_half_up(frame_step))\n",
    "    if slen <= frame_len:\n",
    "        numframes = 1\n",
    "    else:\n",
    "        numframes = 1 + int(math.ceil((1.0 * slen - frame_len) / frame_step))\n",
    "\n",
    "    padlen = int((numframes - 1) * frame_step + frame_len)\n",
    "\n",
    "    zeros = numpy.zeros((padlen - slen,))\n",
    "    padsignal = numpy.concatenate((sig, zeros))\n",
    "    if stride_trick:\n",
    "        win = winfunc(frame_len)\n",
    "        frames = rolling_window(padsignal, window=frame_len, step=frame_step)\n",
    "    else:\n",
    "        indices = numpy.tile(numpy.arange(0, frame_len), (numframes, 1)) + numpy.tile(\n",
    "            numpy.arange(0, numframes * frame_step, frame_step), (frame_len, 1)).T\n",
    "        indices = numpy.array(indices, dtype=numpy.int32)\n",
    "        frames = padsignal[indices]\n",
    "        win = numpy.tile(winfunc(frame_len), (numframes, 1))\n",
    "\n",
    "    return frames * win\n",
    "\n",
    "\n",
    "def deframesig(frames, siglen, frame_len, frame_step, winfunc=lambda x: numpy.ones((x,))):\n",
    "    \"\"\"Does overlap-add procedure to undo the action of framesig.\n",
    "\n",
    "    :param frames: the array of frames.\n",
    "    :param siglen: the length of the desired signal, use 0 if unknown. Output will be truncated to siglen samples.\n",
    "    :param frame_len: length of each frame measured in samples.\n",
    "    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.\n",
    "    :param winfunc: the analysis window to apply to each frame. By default no window is applied.\n",
    "    :returns: a 1-D signal.\n",
    "    \"\"\"\n",
    "    frame_len = round_half_up(frame_len)\n",
    "    frame_step = round_half_up(frame_step)\n",
    "    numframes = numpy.shape(frames)[0]\n",
    "    assert numpy.shape(frames)[1] == frame_len, '\"frames\" matrix is wrong size, 2nd dim is not equal to frame_len'\n",
    "\n",
    "    indices = numpy.tile(numpy.arange(0, frame_len), (numframes, 1)) + numpy.tile(\n",
    "        numpy.arange(0, numframes * frame_step, frame_step), (frame_len, 1)).T\n",
    "    indices = numpy.array(indices, dtype=numpy.int32)\n",
    "    padlen = (numframes - 1) * frame_step + frame_len\n",
    "\n",
    "    if siglen <= 0: siglen = padlen\n",
    "\n",
    "    rec_signal = numpy.zeros((padlen,))\n",
    "    window_correction = numpy.zeros((padlen,))\n",
    "    win = winfunc(frame_len)\n",
    "\n",
    "    for i in range(0, numframes):\n",
    "        window_correction[indices[i, :]] = window_correction[\n",
    "                                               indices[i, :]] + win + 1e-15  # add a little bit so it is never zero\n",
    "        rec_signal[indices[i, :]] = rec_signal[indices[i, :]] + frames[i, :]\n",
    "\n",
    "    rec_signal = rec_signal / window_correction\n",
    "    return rec_signal[0:siglen]\n",
    "\n",
    "\n",
    "def magspec(frames, NFFT):\n",
    "    \"\"\"Compute the magnitude spectrum of each frame in frames. If frames is an NxD matrix, output will be Nx(NFFT/2+1).\n",
    "\n",
    "    :param frames: the array of frames. Each row is a frame.\n",
    "    :param NFFT: the FFT length to use. If NFFT > frame_len, the frames are zero-padded.\n",
    "    :returns: If frames is an NxD matrix, output will be Nx(NFFT/2+1). Each row will be the magnitude spectrum of the corresponding frame.\n",
    "    \"\"\"\n",
    "    if numpy.shape(frames)[1] > NFFT:\n",
    "        logging.warn(\n",
    "            'frame length (%d) is greater than FFT size (%d), frame will be truncated. Increase NFFT to avoid.',\n",
    "            numpy.shape(frames)[1], NFFT)\n",
    "    complex_spec = numpy.fft.rfft(frames, NFFT)\n",
    "    return numpy.absolute(complex_spec)\n",
    "\n",
    "\n",
    "def powspec(frames, NFFT):\n",
    "    \"\"\"Compute the power spectrum of each frame in frames. If frames is an NxD matrix, output will be Nx(NFFT/2+1).\n",
    "\n",
    "    :param frames: the array of frames. Each row is a frame.\n",
    "    :param NFFT: the FFT length to use. If NFFT > frame_len, the frames are zero-padded.\n",
    "    :returns: If frames is an NxD matrix, output will be Nx(NFFT/2+1). Each row will be the power spectrum of the corresponding frame.\n",
    "    \"\"\"\n",
    "    return 1.0 / NFFT * numpy.square(magspec(frames, NFFT))\n",
    "\n",
    "\n",
    "def logpowspec(frames, NFFT, norm=1):\n",
    "    \"\"\"Compute the log power spectrum of each frame in frames. If frames is an NxD matrix, output will be Nx(NFFT/2+1).\n",
    "\n",
    "    :param frames: the array of frames. Each row is a frame.\n",
    "    :param NFFT: the FFT length to use. If NFFT > frame_len, the frames are zero-padded.\n",
    "    :param norm: If norm=1, the log power spectrum is normalised so that the max value (across all frames) is 0.\n",
    "    :returns: If frames is an NxD matrix, output will be Nx(NFFT/2+1). Each row will be the log power spectrum of the corresponding frame.\n",
    "    \"\"\"\n",
    "    ps = powspec(frames, NFFT);\n",
    "    ps[ps <= 1e-30] = 1e-30\n",
    "    lps = 10 * numpy.log10(ps)\n",
    "    if norm:\n",
    "        return lps - numpy.max(lps)\n",
    "    else:\n",
    "        return lps\n",
    "\n",
    "\n",
    "def preemphasis(signal, coeff=0.95):\n",
    "    \"\"\"perform preemphasis on the input signal.\n",
    "\n",
    "    :param signal: The signal to filter.\n",
    "    :param coeff: The preemphasis coefficient. 0 is no filter, default is 0.95.\n",
    "    :returns: the filtered signal.\n",
    "    \"\"\"\n",
    "    return numpy.append(signal[0], signal[1:] - coeff * signal[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e9wRGfrfpUI"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zuAc_nAKfo1h"
   },
   "outputs": [],
   "source": [
    "\"\"\"Utilities for attention.\"\"\"\n",
    "\n",
    "def get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    position = tf.to_float(tf.range(length))\n",
    "    num_timescales = channels // 2\n",
    "    log_timescale_increment = (\n",
    "            math.log(float(max_timescale) / float(min_timescale)) /\n",
    "            (tf.to_float(num_timescales) - 1))\n",
    "    inv_timescales = min_timescale * tf.exp(\n",
    "        tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
    "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n",
    "    signal = tf.reshape(signal, [1, length, channels])\n",
    "    return signal\n",
    "\n",
    "\n",
    "def add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    \"\"\"Adds a bunch of sinusoids of different frequencies to a Tensor.\n",
    "    \"\"\"\n",
    "    length = tf.shape(x)[1]\n",
    "    channels = tf.shape(x)[2]\n",
    "    position = tf.to_float(tf.range(length))\n",
    "    num_timescales = channels // 2\n",
    "    log_timescale_increment = (\n",
    "            math.log(float(max_timescale) / float(min_timescale)) /\n",
    "            (tf.to_float(num_timescales) - 1))\n",
    "    inv_timescales = min_timescale * tf.exp(\n",
    "            tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
    "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n",
    "    signal = tf.reshape(signal, [1, length, channels])\n",
    "    return x + signal\n",
    "\n",
    "\n",
    "def embedding_to_padding(emb):\n",
    "    \"\"\"Input embeddings -> is_padding.\n",
    "    \"\"\"\n",
    "    emb_sum = tf.reduce_sum(tf.abs(emb), axis=-1)\n",
    "    return tf.equal(emb_sum, 0.0)\n",
    "\n",
    "\n",
    "def attention_bias_lower_triangle(length):\n",
    "    \"\"\"Create an bias tensor to be added to attention logits.\n",
    "    \"\"\"\n",
    "    lower_triangle = tf.matrix_band_part(tf.ones([length, length]), -1, 0)\n",
    "    ret = -1e9 * (1.0 - lower_triangle)\n",
    "    return tf.reshape(ret, [1, 1, length, length])\n",
    "\n",
    "\n",
    "def attention_bias_ignore_padding(memory_padding):\n",
    "    \"\"\"Create an bias tensor to be added to attention logits.\n",
    "        input: [batch, memory_length], return: [batch, 1, 1, memory_length].\n",
    "    \"\"\"\n",
    "    ret = tf.to_float(memory_padding) * -1e9\n",
    "    return tf.expand_dims(tf.expand_dims(ret, 1), 1)\n",
    "\n",
    "\n",
    "def split_last_dimension(x, n):\n",
    "    \"\"\"Reshape x so that the last dimension becomes two dimensions.\n",
    "    note: [..., m] --> [..., n, m/n]\n",
    "    \"\"\"\n",
    "    old_shape = x.get_shape().dims\n",
    "    last = old_shape[-1]\n",
    "    new_shape = old_shape[:-1] + [n] + [last // n if last else None]\n",
    "    ret = tf.reshape(x, tf.concat([tf.shape(x)[:-1], [n, -1]], 0))\n",
    "    ret.set_shape(new_shape)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def combine_last_two_dimensions(x):\n",
    "    \"\"\"Reshape x so that the last two dimension become one.\n",
    "    note: [..., a, b] --> [..., ab]\n",
    "    \"\"\"\n",
    "    old_shape = x.get_shape().dims\n",
    "    a, b = old_shape[-2:]\n",
    "    new_shape = old_shape[:-2] + [a * b if a and b else None]\n",
    "    ret = tf.reshape(x, tf.concat([tf.shape(x)[:-2], [-1]], 0))\n",
    "    ret.set_shape(new_shape)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"Split channels (dimension 3) into multiple heads (becomes dimension 1).\n",
    "    note: [batch, length, channels] -> [batch, num_heads, length, channels / num_heads]\n",
    "    \"\"\"\n",
    "    return tf.transpose(split_last_dimension(x, num_heads), [0, 2, 1, 3])\n",
    "\n",
    "\n",
    "def sb_split_heads(x, num_heads):\n",
    "    return tf.transpose(split_last_dimension(x, num_heads), [0, 1, 3, 2, 4])\n",
    "\n",
    "\n",
    "def combine_heads(x):\n",
    "    \"\"\"Inverse of split_heads.\n",
    "    note: [batch, num_heads, length, channels / num_heads] -> [batch, length, channels]\n",
    "    \"\"\"\n",
    "    return combine_last_two_dimensions(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "\n",
    "def sb_combine_heads(x):\n",
    "    return combine_last_two_dimensions(tf.transpose(x, [0, 1, 3, 2, 4]))\n",
    "\n",
    "\n",
    "def shape_list(x):\n",
    "    if x.get_shape().dims is None:\n",
    "        return tf.shape(x)\n",
    "    static = x.get_shape().as_list()\n",
    "    shape = tf.shape(x)\n",
    "\n",
    "    ret = []\n",
    "    for i in range(len(static)):\n",
    "        dim = static[i]\n",
    "        if dim is None:\n",
    "            dim = shape[i]\n",
    "        ret.append(dim)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def residual_fn(x, y, hparams):\n",
    "    return layer_norm(x + tf.nn.dropout(\n",
    "            y, 1.0 - hparams.residual_dropout))\n",
    "\n",
    "\n",
    "def dot_product_attention(q,\n",
    "                          k,\n",
    "                          v,\n",
    "                          bias,\n",
    "                          dropout_rate=0.0,\n",
    "                          summaries=False,\n",
    "                          image_shapes=None,\n",
    "                          name=None):\n",
    "    \"\"\"dot-product attention.\n",
    "        q: a Tensor with shape [batch, heads, length_q, depth_k]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\n",
    "            name, default_name=\"dot_product_attention\", values=[q, k, v]):\n",
    "        # [batch, num_heads, query_length, memory_length]\n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        if bias is not None:\n",
    "            logits += bias\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        # dropping out the attention links for each of the heads\n",
    "        weights = tf.nn.dropout(weights, 1.0 - dropout_rate)\n",
    "        return tf.matmul(weights, v)\n",
    "\n",
    "\n",
    "def multihead_attention(query_antecedent,\n",
    "                        memory_antecedent,\n",
    "                        bias,\n",
    "                        total_key_depth,\n",
    "                        total_value_depth,\n",
    "                        output_depth,\n",
    "                        num_heads,\n",
    "                        dropout_rate,\n",
    "                        cache=None,\n",
    "                        summaries=False,\n",
    "                        image_shapes=None,\n",
    "                        name=None):\n",
    "    \"\"\"Multihead scaled-dot-product attention with input/output transformations.\n",
    "        query_antecedent: a Tensor with shape [batch, length_q, channels]\n",
    "        memory_antecedent: a Tensor with shape [batch, length_m, channels]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\n",
    "        name,\n",
    "        default_name=\"multihead_attention\",\n",
    "        values=[query_antecedent, memory_antecedent]):\n",
    "        if memory_antecedent is None:\n",
    "            # self attention\n",
    "            combined = conv1d(\n",
    "                    query_antecedent,\n",
    "                    total_key_depth * 2 + total_value_depth,\n",
    "                    1,\n",
    "                    name=\"qkv_transform\")\n",
    "            q, k, v = tf.split(\n",
    "                    combined, [total_key_depth, total_key_depth, total_value_depth],\n",
    "                    axis=2)\n",
    "        else:\n",
    "            q = conv1d(\n",
    "                    query_antecedent, total_key_depth, 1, name=\"q_transform\")\n",
    "            combined = conv1d(\n",
    "                    memory_antecedent,\n",
    "                    total_key_depth + total_value_depth,\n",
    "                    1,\n",
    "                    name=\"kv_transform\")\n",
    "            k, v = tf.split(combined, [total_key_depth, total_value_depth], axis=2)\n",
    "\n",
    "        if cache is not None:\n",
    "            if bias is None:\n",
    "                raise ValueError(\"Bias required for caching. See function docstring \"\n",
    "                                 \"for details.\")\n",
    "            k = cache[\"k\"] = tf.concat([cache[\"k\"], k], axis=1)\n",
    "            v = cache[\"v\"] = tf.concat([cache[\"v\"], v], axis=1)\n",
    "\n",
    "        q = split_heads(q, num_heads)\n",
    "        k = split_heads(k, num_heads)\n",
    "        v = split_heads(v, num_heads)\n",
    "        key_depth_per_head = total_key_depth // num_heads\n",
    "        q *= key_depth_per_head**-0.5\n",
    "        x = dot_product_attention(\n",
    "                q, k, v, bias, dropout_rate, summaries, image_shapes)\n",
    "        x = combine_heads(x)\n",
    "        x = conv1d(x, output_depth, 1, name=\"output_transform\")\n",
    "        return x\n",
    "\n",
    "\n",
    "def sb_dot_product_attention_for_decoding(q,\n",
    "                          k,\n",
    "                          v,\n",
    "                          bias,\n",
    "                          batch_size=None,\n",
    "                          beam_size=None,\n",
    "                          dropout_rate=0.0,\n",
    "                          summaries=False,\n",
    "                          image_shapes=None,\n",
    "                          name=None):\n",
    "    \"\"\"dot-product attention.\n",
    "        q: a Tensor with shape [batch, heads, length_q, depth_k]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\n",
    "            name, default_name=\"sb_dot_product_attention\", values=[q, k, v]):\n",
    "        # [batch, num_heads, query_length, memory_length]\n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        if bias is not None:\n",
    "            logits += bias\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights_l2r\")\n",
    "        # dropping out the attention links for each of the heads\n",
    "        weights = tf.nn.dropout(weights, 1.0 - dropout_rate)\n",
    "        # if summaries and not tf.get_variable_scope().reuse:\n",
    "        #    attention_image_summary(weights, image_shapes)\n",
    "        final_l2r = tf.matmul(weights, v)  # [batch*beam, num_heads, length_tmp, hidden_size/num_heads]\n",
    "\n",
    "        # calculate final_r2l\n",
    "        shape = shape_list(k)\n",
    "        new_shape = [batch_size]+[2]+[tf.cast(beam_size/2,tf.int32)]+shape[1:]\n",
    "        k_ = tf.reshape(k, new_shape)  # [batch, 2, beam/2, num_heads, length_tmp, hidden_size/num_heads]\n",
    "        k_ = tf.reverse(k_,[1])\n",
    "        v_ = tf.reshape(v, new_shape)\n",
    "        v_ = tf.reverse(v_,[1])\n",
    "\n",
    "        shape_ = shape_list(k_)\n",
    "        new_shape_ = [batch_size*beam_size]+shape_[3:]\n",
    "        k_ = tf.reshape(k_, new_shape_)  # [batch*beam, num_heads, length_tmp, hidden_size/num_heads]\n",
    "        v_ = tf.reshape(v_, new_shape_)\n",
    "        logits_ = tf.matmul(q, k_, transpose_b=True)\n",
    "        logits_ += bias\n",
    "        weights_ = tf.nn.softmax(logits_, name=\"attention_weights_r2l\")\n",
    "        weights_ = tf.nn.dropout(weights_, 1.0 - dropout_rate)\n",
    "        final_r2l = tf.matmul(weights_, v_)\n",
    "\n",
    "        final_all = final_l2r + 0.1 * final_r2l  # [batch*beam, num_heads, length_tmp, hidden_size/num_heads]\n",
    "        return final_all\n",
    "\n",
    "\n",
    "def sb_dot_product_attention(q,\n",
    "                          k,\n",
    "                          v,\n",
    "                          bias,\n",
    "                          dropout_rate=0.0,\n",
    "                          summaries=False,\n",
    "                          image_shapes=None,\n",
    "                          name=None):\n",
    "    \"\"\"dot-product attention.\n",
    "        q: a Tensor with shape [batch, heads, length_q, depth_k]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\n",
    "            name, default_name=\"sb_dot_product_attention\", values=[q, k, v]):\n",
    "        # [2, batch, num_heads, query_length, memory_length]\n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        bias = tf.expand_dims(bias, axis=0)\n",
    "        logits += bias\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights_l2r\")\n",
    "        weights = tf.nn.dropout(weights, 1.0 - dropout_rate)\n",
    "        # if summaries and not tf.get_variable_scope().reuse:\n",
    "        #    attention_image_summary(weights[0], image_shapes)\n",
    "        final_l2r = tf.matmul(weights, v)  # [2, batch, num_heads, length, hidden_size/num_heads]\n",
    "\n",
    "        # calculate final_r2l\n",
    "        k_ = tf.reverse(k, [0])\n",
    "        v_ = tf.reverse(v, [0])\n",
    "        logits_ = tf.matmul(q, k_, transpose_b=True)\n",
    "        logits_ += bias\n",
    "        weights_ = tf.nn.softmax(logits_, name=\"attention_weights_r2l\")\n",
    "        weights_ = tf.nn.dropout(weights_, 1.0 - dropout_rate)\n",
    "        final_r2l = tf.matmul(weights_, v_)\n",
    "\n",
    "        final_all = final_l2r + 0.1 * final_r2l\n",
    "        return final_all  # [2, batch, num_heads, length, hidden_size/num_heads]\n",
    "\n",
    "\n",
    "def multi_dot_product_attention(q, k, v, bias,\n",
    "                                language_num=1,\n",
    "                                dropout_rate=0.0,\n",
    "                                summaries=False,\n",
    "                                image_shapes=None,\n",
    "                                name=None):\n",
    "    \"\"\"dot-product attention.\n",
    "        q: a Tensor with shape [batch, heads, length_q, depth_k]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\n",
    "            name, default_name=\"sb_dot_product_attention\", values=[q, k, v]):\n",
    "        # [n, batch, num_heads, query_length, memory_length]\n",
    "        final_list = []\n",
    "        bias = tf.expand_dims(bias, axis=0)\n",
    "        for i in range(language_num):\n",
    "            # [1, batch, num_heads, query_length, memory_length] *\n",
    "            # [n, batch, num_heads, query_length, memory_length]\n",
    "            temp_q = tf.expand_dims(q[i], axis=0)\n",
    "            logits = tf.matmul(temp_q, k, transpose_b=True)\n",
    "            logits += bias\n",
    "            # weights?\n",
    "            weights = tf.nn.softmax(logits, name=\"attention_weights_%d\" % i)\n",
    "            weights = tf.nn.dropout(weights, 1.0 - dropout_rate)\n",
    "            # final? [n, batch, num_heads, query_length, memory_length]\n",
    "            final = tf.matmul(weights, v)\n",
    "            # W = [n, 1]\n",
    "            # [n, batch, num_heads, query_length, memory_length] ==>\n",
    "            # [1, batch, num_heads, query_length, memory_length]\n",
    "            final = tf.matmul(W, final)\n",
    "            final_list.append(final)\n",
    "        # final_list [n, batch, num_heads, query_length, memory_length]\n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        bias = tf.expand_dims(bias, axis=0)\n",
    "        logits += bias\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights_l2r\")\n",
    "        weights = tf.nn.dropout(weights, 1.0 - dropout_rate)\n",
    "        # if summaries and not tf.get_variable_scope().reuse:\n",
    "        #    attention_image_summary(weights[0], image_shapes)\n",
    "        final_l2r = tf.matmul(weights, v)  # [2, batch, num_heads, length, hidden_size/num_heads]\n",
    "\n",
    "        # calculate final_r2l\n",
    "        k_ = tf.reverse(k, [0])\n",
    "        v_ = tf.reverse(v, [0])\n",
    "        logits_ = tf.matmul(q, k_, transpose_b=True)\n",
    "        logits_ += bias # modify err, logits --> logits_\n",
    "        weights_ = tf.nn.softmax(logits_, name=\"attention_weights_r2l\")\n",
    "        weights_ = tf.nn.dropout(weights_, 1.0 - dropout_rate)\n",
    "        final_r2l = tf.matmul(weights_, v_)\n",
    "\n",
    "        final_all = final_l2r + 0.1 * final_r2l\n",
    "        return final_all  # [2, batch, num_heads, length, hidden_size/num_heads]\n",
    "\n",
    "\n",
    "def sb_multihead_attention( query_antecedent,\n",
    "                            memory_antecedent,\n",
    "                            bias,\n",
    "                            total_key_depth,\n",
    "                            total_value_depth,\n",
    "                            output_depth,\n",
    "                            num_heads,\n",
    "                            dropout_rate,\n",
    "                            cache=None,\n",
    "                            summaries=False,\n",
    "                            image_shapes=None,\n",
    "                            name=None,\n",
    "                            is_decoding=False):\n",
    "    \"\"\"Multihead scaled-dot-product attention with input/output transformations.\n",
    "        query_antecedent: a Tensor with shape [batch, length_q, channels]\n",
    "        memory_antecedent: a Tensor with shape [batch, length_m, channels]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\n",
    "                name,\n",
    "                default_name=\"sb_multihead_attention\",\n",
    "                values=[query_antecedent, memory_antecedent]):\n",
    "        if memory_antecedent is None:\n",
    "        # self attention\n",
    "            combined = sb_conv1d(\n",
    "                query_antecedent,\n",
    "                total_key_depth * 2 + total_value_depth,\n",
    "                1,\n",
    "                name=\"qkv_transform\")\n",
    "            q, k, v = tf.split(\n",
    "                combined, [total_key_depth, total_key_depth, total_value_depth], axis=3) ## 2-->3\n",
    "        else:\n",
    "            q = sb_conv1d(\n",
    "                query_antecedent, total_key_depth, 1, name=\"q_transform\")\n",
    "            combined = conv1d(\n",
    "                memory_antecedent,\n",
    "                total_key_depth + total_value_depth,\n",
    "                1,\n",
    "                name=\"kv_transform\")\n",
    "            k, v = tf.split(combined, [total_key_depth, total_value_depth], axis=2)\n",
    "\n",
    "            k = tf.concat([tf.expand_dims(k,0), tf.expand_dims(k,0)], axis=0) ## [2, batch, length, hidden_size]\n",
    "            v = tf.concat([tf.expand_dims(v,0), tf.expand_dims(v,0)], axis=0)\n",
    "\n",
    "        if cache is not None:\n",
    "            if bias is None:\n",
    "                raise ValueError(\"Bias required for caching. See function docstring \"\n",
    "                                 \"for details.\")\n",
    "            k = cache[\"k\"] = tf.concat([cache[\"k\"], k], axis=1)\n",
    "            v = cache[\"v\"] = tf.concat([cache[\"v\"], v], axis=1)\n",
    "\n",
    "        q = sb_split_heads(q, num_heads)\n",
    "        k = sb_split_heads(k, num_heads)\n",
    "        v = sb_split_heads(v, num_heads)\n",
    "        key_depth_per_head = total_key_depth // num_heads\n",
    "        q *= key_depth_per_head**-0.5\n",
    "        if memory_antecedent is None:  # decoder self attention (synchronous bidirectional att)\n",
    "            x = sb_dot_product_attention(q, k, v, bias, dropout_rate, summaries,\n",
    "                                         image_shapes)  # q: [2, num_heads, length_tmp, lenght]\n",
    "        else:  # enc-dec attention\n",
    "            x = dot_product_attention(\n",
    "                q, k, v, bias, dropout_rate, summaries, image_shapes)\n",
    "        x = sb_combine_heads(x)\n",
    "        x = sb_conv1d(x, output_depth, 1, name=\"output_transform\")\n",
    "        return x\n",
    "\n",
    "\n",
    "def sb_multihead_attention_for_decoding(query_antecedent,\n",
    "                                        memory_antecedent,\n",
    "                                        bias,\n",
    "                                        total_key_depth,\n",
    "                                        total_value_depth,\n",
    "                                        output_depth,\n",
    "                                        num_heads,\n",
    "                                        dropout_rate,\n",
    "                                        batch_size=None,\n",
    "                                        beam_size=None,\n",
    "                                        cache=None,\n",
    "                                        summaries=False,\n",
    "                                        image_shapes=None,\n",
    "                                        name=None):\n",
    "    \"\"\"Multihead scaled-dot-product attention with input/output transformations.\n",
    "        query_antecedent: a Tensor with shape [batch, length_q, channels]\n",
    "        memory_antecedent: a Tensor with shape [batch, length_m, channels]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\n",
    "                name,\n",
    "                default_name=\"sb_multihead_attention\",\n",
    "                values=[query_antecedent, memory_antecedent]):\n",
    "        if memory_antecedent is None:\n",
    "            # self attention\n",
    "            combined = conv1d(\n",
    "                query_antecedent,\n",
    "                total_key_depth * 2 + total_value_depth,\n",
    "                1,\n",
    "                name=\"qkv_transform\")\n",
    "            q, k, v = tf.split(\n",
    "                combined, [total_key_depth, total_key_depth, total_value_depth], axis=2)\n",
    "        else:\n",
    "            q = conv1d(\n",
    "                query_antecedent, total_key_depth, 1, name=\"q_transform\")\n",
    "            combined = conv1d(\n",
    "                memory_antecedent,\n",
    "                total_key_depth + total_value_depth,\n",
    "                1,\n",
    "                name=\"kv_transform\")\n",
    "            k, v = tf.split(combined, [total_key_depth, total_value_depth], axis=2)\n",
    "\n",
    "        if cache is not None:\n",
    "            if bias is None:\n",
    "                raise ValueError(\"Bias required for caching. See function docstring \"\n",
    "                             \"for details.\")\n",
    "            k = cache[\"k\"] = tf.concat([cache[\"k\"], k], axis=1)\n",
    "            v = cache[\"v\"] = tf.concat([cache[\"v\"], v], axis=1)\n",
    "\n",
    "        q = split_heads(q, num_heads)\n",
    "        k = split_heads(k, num_heads)\n",
    "        v = split_heads(v, num_heads)\n",
    "        key_depth_per_head = total_key_depth // num_heads\n",
    "        q *= key_depth_per_head**-0.5\n",
    "        if memory_antecedent is None:  # decoder self attention (synchronous bidirectional att)\n",
    "            x = sb_dot_product_attention_for_decoding(q, k, v, bias, batch_size, beam_size, dropout_rate, summaries,\n",
    "                                                      image_shapes)  # q: [batch, num_heads, length_tmp, lenght]\n",
    "        else:  # enc-dec attention\n",
    "            x = dot_product_attention(\n",
    "                q, k, v, bias, dropout_rate, summaries, image_shapes)\n",
    "        x = combine_heads(x)\n",
    "        x = conv1d(x, output_depth, 1, name=\"output_transform\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "a6Nmu0dofox0"
   },
   "outputs": [],
   "source": [
    "\"\"\"Hyperparameters and ranges common to multiple models.\"\"\"\n",
    "\n",
    "def transformer_params():\n",
    "    \"\"\"A set of basic hyperparameters.\"\"\"\n",
    "    return tf.contrib.training.HParams(\n",
    "        batching_mantissa_bits=3,\n",
    "        kernel_height=3,\n",
    "        kernel_width=1,\n",
    "        compress_steps=0,\n",
    "        dropout=0.0,\n",
    "        clip_grad_norm=0.0,\n",
    "        initializer=\"uniform_unit_scaling\",\n",
    "        initializer_gain=1.0,\n",
    "        label_smoothing=0.1,\n",
    "        optimizer=\"Adam\",\n",
    "        optimizer_adam_epsilon=1e-9,\n",
    "        optimizer_adam_beta1=0.9,\n",
    "        optimizer_adam_beta2=0.998,\n",
    "        optimizer_momentum_momentum=0.9,\n",
    "        weight_decay=0.0,\n",
    "        weight_noise=0.0,\n",
    "        learning_rate_decay_scheme=\"noam\",\n",
    "        learning_rate_warmup_steps=16000,\n",
    "        learning_rate=0.1,\n",
    "        sampling_method=\"argmax\",  # \"argmax\" or \"random\"\n",
    "        multiply_embedding_mode=\"sqrt_depth\",\n",
    "        symbol_modality_num_shards=16,\n",
    "        num_sampled_classes=0,\n",
    "        shared_source_embedding_and_softmax_weights=int(True),\n",
    "        shared_target_embedding_and_softmax_weights=int(True),\n",
    "        pos=\"timing\",\n",
    "        ffn_layer=\"conv_hidden_relu\",\n",
    "        attention_key_channels=0,\n",
    "        attention_value_channels=0,\n",
    "      \n",
    "        hidden_size=256,\n",
    "        batch_size=4096,\n",
    "        max_length=256,\n",
    "        filter_size=1024,\n",
    "        num_heads=4,\n",
    "        attention_dropout=0.0,\n",
    "        relu_dropout=0.0,\n",
    "        residual_dropout=0.1,\n",
    "        nbr_decoder_problems=1,\n",
    "        num_hidden_layers=6,\n",
    "        num_hidden_layers_src=6,\n",
    "        num_hidden_layers_tgt=6,\n",
    "      \n",
    "        # problem hparams\n",
    "        loss_multiplier=1.4,\n",
    "        batch_size_multiplier=1,\n",
    "        max_expected_batch_size_per_shard=64,\n",
    "        input_modality=None,\n",
    "        target_modality=None,\n",
    "        vocab_src_size=37002,\n",
    "        vocab_tgt_size=37002,\n",
    "        vocabulary={\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def transformer_params_big(data_dir, vocab_src_name, vocab_tgt_name):\n",
    "    \"\"\"A set of basic hyperparameters.\"\"\"\n",
    "    hparams = transformer_params()\n",
    "    hparams.vocabulary = {\n",
    "        \"inputs\": TokenTextEncoder(vocab_filename=os.path.join(data_dir, vocab_src_name)),\n",
    "        \"targets\": TokenTextEncoder(vocab_filename=os.path.join(data_dir, vocab_tgt_name))}\n",
    "    hparams.hidden_size = 1024\n",
    "    hparams.filter_size = 4096\n",
    "    hparams.num_heads = 16\n",
    "    hparams.batching_mantissa_bits = 3\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def transformer_params_base(data_dir, vocab_src_name, vocab_tgt_name):\n",
    "    \"\"\"A set of basic hyperparameters.\"\"\"\n",
    "    hparams = transformer_params()\n",
    "    hparams.vocabulary = {\n",
    "        \"inputs\": TokenTextEncoder(vocab_filename=os.path.join(data_dir, vocab_src_name)),\n",
    "        \"targets\": TokenTextEncoder(vocab_filename=os.path.join(data_dir, vocab_tgt_name))}\n",
    "    hparams.hidden_size = 256\n",
    "#     hparams.filter_size = 2048\n",
    "    hparams.filter_size = 1024\n",
    "    hparams.num_heads = 8\n",
    "    hparams.batching_mantissa_bits = 2    \n",
    "    \n",
    "#     hparams.batch_size=4096\n",
    "    hparams.batch_size=3024\n",
    "    \n",
    "    return hparams\n",
    "\n",
    "\n",
    "def transformer_params_small(data_dir, vocab_src_name, vocab_tgt_name):\n",
    "    \"\"\"A set of basic hyperparameters.\"\"\"\n",
    "    hparams = transformer_params()\n",
    "    hparams.vocabulary = {\n",
    "        \"inputs\": TokenTextEncoder(vocab_filename=os.path.join(data_dir, vocab_src_name)),\n",
    "        \"targets\": TokenTextEncoder(vocab_filename=os.path.join(data_dir, vocab_tgt_name))}\n",
    "    hparams.hidden_size = 256\n",
    "    hparams.filter_size = 1024\n",
    "    hparams.num_heads = 8\n",
    "    hparams.batching_mantissa_bits = 2\n",
    "        \n",
    "    return hparams\n",
    "\n",
    "def transformer_params_listra(data_dir, vocab_src_name, vocab_tgt_name):\n",
    "    \"\"\"A set of custom hyperparameters for LiSTra.\"\"\"\n",
    "    hparams = transformer_params()\n",
    "    hparams.vocabulary = {\n",
    "        \"inputs\": TokenTextEncoder(vocab_filename=os.path.join(data_dir, vocab_src_name)),\n",
    "        \"targets\": TokenTextEncoder(vocab_filename=os.path.join(data_dir, vocab_tgt_name))}\n",
    "    hparams.hidden_size = 256\n",
    "    hparams.filter_size = 1024\n",
    "    hparams.num_heads = 8\n",
    "    hparams.batching_mantissa_bits = 2\n",
    "    \n",
    "    learning_rate=0.1\n",
    "    max_expected_batch_size_per_shard=64\n",
    "    batch_size=512\n",
    "    \n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aipeHk-mfoq6"
   },
   "outputs": [],
   "source": [
    "\"\"\"Layers common to multiple models.\"\"\"\n",
    "\n",
    "# This is a global setting. When turned off, no @function.Defun is used.\n",
    "allow_defun = True\n",
    "\n",
    "def flatten4d3d(x):\n",
    "    \"\"\"Flatten a 4d-tensor into a 3d-tensor by joining width and height.\"\"\"\n",
    "    xshape = tf.shape(x)\n",
    "    result = tf.reshape(x, [xshape[0], xshape[1] * xshape[2], xshape[3]])\n",
    "    # Preserve static shapes when available.\n",
    "    xshape_static = x.get_shape()\n",
    "    result.set_shape([xshape_static[0], None, xshape_static[3]])\n",
    "    return result\n",
    "\n",
    "\n",
    "def embedding(x, vocab_size, dense_size, name=None, reuse=None, multiplier=1.0):\n",
    "    \"\"\"Embed x of type int64 into dense vectors, reducing to max 4 dimensions.\"\"\"\n",
    "    with tf.variable_scope(\n",
    "            name, default_name=\"embedding\", values=[x], reuse=reuse):\n",
    "        embedding_var = tf.get_variable(\"kernel\", [vocab_size, dense_size])\n",
    "        # On the backwards pass, we want to convert the gradient from\n",
    "        # an indexed-slices to a regular tensor before sending it back to the\n",
    "        # parameter server. This avoids excess computation on the parameter server.\n",
    "        embedding_var = ConvertGradientToTensor(embedding_var)\n",
    "        emb_x = tf.gather(embedding_var, x)\n",
    "        if multiplier != 1.0:\n",
    "            emb_x *= multiplier\n",
    "        shape, static_shape = tf.shape(emb_x), emb_x.shape.as_list()\n",
    "        if not static_shape or len(static_shape) < 5:\n",
    "            return emb_x\n",
    "        # If we had extra channel dimensions, assume it's 1, i.e. shape[3] == 1.\n",
    "        assert len(static_shape) == 5\n",
    "        return tf.reshape(emb_x, [shape[0], shape[1], shape[2], static_shape[4]])\n",
    "\n",
    "\n",
    "def shift_left(x, pad_value=None):\n",
    "    \"\"\"Shift the second dimension of x right by one.\"\"\"\n",
    "    if pad_value is None:\n",
    "        shifted_targets = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])[:, :-1, :, :]\n",
    "    else:\n",
    "        shifted_targets = tf.concat([pad_value, x], axis=1)[:, :-1, :, :]\n",
    "    return shifted_targets\n",
    "\n",
    "\n",
    "def shift_left_3d(x, pad_value=None):\n",
    "    \"\"\"Shift the second dimension of x right by one.\"\"\"\n",
    "    if pad_value is None:\n",
    "        shifted_targets = tf.pad(x, [[0, 0], [1, 0], [0, 0]])[:, :-1, :]\n",
    "    else:\n",
    "        shifted_targets = tf.concat([pad_value, x], axis=1)[:, :-1, :]\n",
    "    return shifted_targets\n",
    "\n",
    "\n",
    "def conv_internal(conv_fn, inputs, filters, kernel_size, **kwargs):\n",
    "    \"\"\"Conditional conv_fn making kernel 1d or 2d depending on inputs shape.\"\"\"\n",
    "    static_shape = inputs.get_shape()\n",
    "    if not static_shape or len(static_shape) != 4:\n",
    "        raise ValueError(\"Inputs to conv must have statically known rank 4.\")\n",
    "    # Add support for left padding.\n",
    "    if \"padding\" in kwargs and kwargs[\"padding\"] == \"LEFT\":\n",
    "        dilation_rate = (1, 1)\n",
    "        if \"dilation_rate\" in kwargs:\n",
    "            dilation_rate = kwargs[\"dilation_rate\"]\n",
    "        assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1\n",
    "        height_padding = 2 * (kernel_size[0] // 2) * dilation_rate[0]\n",
    "        cond_padding = tf.cond(\n",
    "                tf.equal(tf.shape(inputs)[2], 1), lambda: tf.constant(0),\n",
    "                lambda: tf.constant(2 * (kernel_size[1] // 2) * dilation_rate[1]))\n",
    "        width_padding = 0 if static_shape[2] == 1 else cond_padding\n",
    "        padding = [[0, 0], [height_padding, 0], [width_padding, 0], [0, 0]]\n",
    "        inputs = tf.pad(inputs, padding)\n",
    "        # Set middle two dimensions to None to prevent convolution from complaining\n",
    "        inputs.set_shape([static_shape[0], None, None, static_shape[3]])\n",
    "        kwargs[\"padding\"] = \"VALID\"\n",
    "\n",
    "    def conv2d_kernel(kernel_size_arg, name_suffix):\n",
    "        \"\"\"Call conv2d but add suffix to name.\"\"\"\n",
    "        if \"name\" in kwargs:\n",
    "            original_name = kwargs[\"name\"]\n",
    "            name = kwargs.pop(\"name\") + \"_\" + name_suffix\n",
    "        else:\n",
    "            original_name = None\n",
    "            name = \"conv_\" + name_suffix\n",
    "        original_force2d = None\n",
    "        if \"force2d\" in kwargs:\n",
    "            original_force2d = kwargs.pop(\"force2d\")\n",
    "        result = conv_fn(inputs, filters, kernel_size_arg, name=name, **kwargs)\n",
    "        if original_name is not None:\n",
    "            kwargs[\"name\"] = original_name  # Restore for other calls.\n",
    "        if original_force2d is not None:\n",
    "            kwargs[\"force2d\"] = original_force2d\n",
    "        return result\n",
    "\n",
    "    return conv2d_kernel(kernel_size, \"single\")\n",
    "\n",
    "\n",
    "def conv(inputs, filters, kernel_size, **kwargs):\n",
    "    return conv_internal(tf.layers.conv2d, inputs, filters, kernel_size, **kwargs)\n",
    "\n",
    "\n",
    "def conv1d(inputs, filters, kernel_size, **kwargs):\n",
    "    return tf.squeeze(\n",
    "            conv(tf.expand_dims(inputs, 2), filters, (kernel_size, 1), **kwargs), 2)\n",
    "def sb_conv1d(inputs, filters, kernel_size, **kwargs):\n",
    "  return conv(inputs, filters, (kernel_size, 1), **kwargs)\n",
    "\n",
    "\n",
    "def separable_conv(inputs, filters, kernel_size, **kwargs):\n",
    "    return conv_internal(tf.layers.separable_conv2d, inputs, filters, kernel_size, **kwargs)\n",
    "\n",
    "def layer_norm_compute_python(x, epsilon, scale, bias):\n",
    "    \"\"\"Layer norm raw computation.\"\"\"\n",
    "    mean = tf.reduce_mean(x, axis=[-1], keep_dims=True)\n",
    "    variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keep_dims=True)\n",
    "    norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n",
    "    return norm_x * scale + bias\n",
    "\n",
    "\n",
    "@function.Defun(compiled=True)\n",
    "def layer_norm_compute_grad(x, epsilon, scale, bias, dy):\n",
    "    y = layer_norm_compute_python(x, epsilon, scale, bias)\n",
    "    dx = tf.gradients(ys=[y], xs=[x, epsilon, scale, bias], grad_ys=[dy])\n",
    "    return dx\n",
    "\n",
    "\n",
    "@function.Defun(\n",
    "    compiled=True,\n",
    "    separate_compiled_gradients=True,\n",
    "    grad_func=layer_norm_compute_grad)\n",
    "def layer_norm_compute(x, epsilon, scale, bias):\n",
    "    return layer_norm_compute_python(x, epsilon, scale, bias)\n",
    "\n",
    "\n",
    "def layer_norm(x, filters=None, epsilon=1e-6, name=None, reuse=None):\n",
    "    \"\"\"Layer normalize the tensor x, averaging over the last dimension.\"\"\"\n",
    "    if filters is None:\n",
    "        filters = x.get_shape()[-1]\n",
    "    with tf.variable_scope(\n",
    "            name, default_name=\"layer_norm\", values=[x], reuse=reuse):\n",
    "        scale = tf.get_variable(\n",
    "                \"layer_norm_scale\", [filters], initializer=tf.ones_initializer())\n",
    "        bias = tf.get_variable(\n",
    "                \"layer_norm_bias\", [filters], initializer=tf.zeros_initializer())\n",
    "        if allow_defun:\n",
    "            result = layer_norm_compute(x, tf.constant(epsilon), scale, bias)\n",
    "            result.set_shape(x.get_shape())\n",
    "        else:\n",
    "            result = layer_norm_compute_python(x, epsilon, scale, bias)\n",
    "        return result\n",
    "\n",
    "def residual_function(hparams):\n",
    "    \"\"\"Returns a function for combining layer input and layer output.\n",
    "    \"\"\"\n",
    "\n",
    "    def residual_fn(x, y):\n",
    "        return hparams.norm_function(x + tf.nn.dropout(\n",
    "            y, 1.0 - hparams.residual_dropout))\n",
    "\n",
    "    return residual_fn\n",
    "\n",
    "def relu_density_logit(x, reduce_dims):\n",
    "    \"\"\"logit(density(x)).\n",
    "    \"\"\"\n",
    "    frac = tf.reduce_mean(tf.to_float(x > 0.0), reduce_dims)\n",
    "    scaled = tf.log(frac + math.exp(-10)) - tf.log((1.0 - frac) + math.exp(-10))\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def conv_hidden_relu(inputs,\n",
    "                     hidden_size,\n",
    "                     output_size,\n",
    "                     kernel_size=(1, 1),\n",
    "                     second_kernel_size=(1, 1),\n",
    "                     summaries=True,\n",
    "                     dropout=0.0,\n",
    "                     **kwargs):\n",
    "    \"\"\"Hidden layer with RELU activation followed by linear projection.\"\"\"\n",
    "    name = kwargs.pop(\"name\") if \"name\" in kwargs else None\n",
    "    with tf.variable_scope(name, \"conv_hidden_relu\", [inputs]):\n",
    "        if inputs.get_shape().ndims == 3:\n",
    "            is_3d = True\n",
    "            inputs = tf.expand_dims(inputs, 2)\n",
    "        else:\n",
    "            is_3d = False\n",
    "        conv_f1 = conv if kernel_size == (1, 1) else separable_conv\n",
    "        h = conv_f1(\n",
    "                inputs,\n",
    "                hidden_size,\n",
    "                kernel_size,\n",
    "                activation=tf.nn.relu,\n",
    "                name=\"conv1\",\n",
    "                **kwargs)\n",
    "        if dropout != 0.0:\n",
    "            h = tf.nn.dropout(h, 1.0 - dropout)\n",
    "        conv_f2 = conv if second_kernel_size == (1, 1) else separable_conv\n",
    "        ret = conv_f2(h, output_size, second_kernel_size, name=\"conv2\", **kwargs)\n",
    "        if is_3d:\n",
    "            ret = tf.squeeze(ret, 2)\n",
    "        return ret\n",
    "\n",
    "def pad_to_same_length(x, y, final_length_divisible_by=1, axis=1):\n",
    "    \"\"\"Pad tensors x and y on axis 1 so that they have the same length.\"\"\"\n",
    "    if axis not in [1, 2]:\n",
    "        raise ValueError(\"Only axis=1 and axis=2 supported for now.\")\n",
    "    with tf.name_scope(\"pad_to_same_length\", values=[x, y]):\n",
    "        x_length = tf.shape(x)[axis]\n",
    "        y_length = tf.shape(y)[axis]\n",
    "        max_length = tf.maximum(x_length, y_length)\n",
    "        if final_length_divisible_by > 1:\n",
    "            # Find the nearest larger-or-equal integer divisible by given number.\n",
    "            max_length += final_length_divisible_by - 1\n",
    "            max_length //= final_length_divisible_by\n",
    "            max_length *= final_length_divisible_by\n",
    "        length_diff1 = max_length - x_length\n",
    "        length_diff2 = max_length - y_length\n",
    "\n",
    "        def padding_list(length_diff, arg):\n",
    "            if axis == 1:\n",
    "                return [[[0, 0], [0, length_diff]],\n",
    "                        tf.zeros([tf.rank(arg) - 2, 2], dtype=tf.int32)]\n",
    "            return [[[0, 0], [0, 0], [0, length_diff]],\n",
    "                    tf.zeros([tf.rank(arg) - 3, 2], dtype=tf.int32)]\n",
    "\n",
    "        paddings1 = tf.concat(padding_list(length_diff1, x), axis=0)\n",
    "        paddings2 = tf.concat(padding_list(length_diff2, y), axis=0)\n",
    "        res_x = tf.pad(x, paddings1)\n",
    "        res_y = tf.pad(y, paddings2)\n",
    "        # Static shapes are the same except for axis=1.\n",
    "        x_shape = x.shape.as_list()\n",
    "        x_shape[axis] = None\n",
    "        res_x.set_shape(x_shape)\n",
    "        y_shape = y.shape.as_list()\n",
    "        y_shape[axis] = None\n",
    "        res_y.set_shape(y_shape)\n",
    "        return res_x, res_y\n",
    "\n",
    "\n",
    "def pad_with_zeros(logits, labels):\n",
    "    \"\"\"Pad labels on the length dimension to match logits length.\"\"\"\n",
    "    with tf.name_scope(\"pad_with_zeros\", values=[logits, labels]):\n",
    "        logits, labels = pad_to_same_length(logits, labels)\n",
    "        if len(labels.shape.as_list()) == 3:  # 2-d labels.\n",
    "            logits, labels = pad_to_same_length(logits, labels, axis=2)\n",
    "        return logits, labels\n",
    "\n",
    "\n",
    "def weights_nonzero(labels):\n",
    "    \"\"\"Assign weight 1.0 to all labels except for padding (id=0).\"\"\"\n",
    "    return tf.to_float(tf.not_equal(labels, 0))\n",
    "\n",
    "\n",
    "def padded_cross_entropy(logits,\n",
    "                         labels,\n",
    "                         label_smoothing,\n",
    "                         weights_fn=weights_nonzero,\n",
    "                         reduce_sum=True):\n",
    "    \"\"\"Compute cross-entropy assuming 0s are padding.\n",
    "\n",
    "    Computes a loss numerator (the sum of losses), and loss denominator\n",
    "    (the number of non-padding tokens).\n",
    "\n",
    "    Args:\n",
    "        logits: a `Tensor` with shape `[batch, timesteps, vocab_size]`.\n",
    "        labels: an integer `Tensor` with shape `[batch, timesteps]`.\n",
    "        label_smoothing: a floating point `Scalar`.\n",
    "        weights_fn: A function from labels to weights.\n",
    "        reduce_sum: a Boolean, whether to sum at the end or not.\n",
    "\n",
    "    Returns:\n",
    "        loss_numerator: a `Scalar`.  Sum of losses.\n",
    "        loss_denominator: a `Scalar.  The number of non-padding target tokens.\n",
    "    \"\"\"\n",
    "    confidence = 1.0 - label_smoothing\n",
    "    vocab_size = tf.shape(logits)[-1]\n",
    "    with tf.name_scope(\"padded_cross_entropy\", values=[logits, labels]):\n",
    "        pad_logits, pad_labels = pad_with_zeros(logits, labels)\n",
    "        xent = smoothing_cross_entropy(pad_logits, pad_labels, vocab_size, confidence)\n",
    "        weights = weights_fn(pad_labels)\n",
    "        if not reduce_sum:\n",
    "            return xent * weights, weights\n",
    "        return tf.reduce_sum(xent * weights), tf.reduce_sum(weights)\n",
    "\n",
    "\n",
    "def smoothing_cross_entropy(logits, labels, vocab_size, confidence):\n",
    "    \"\"\"Cross entropy with label smoothing to limit over-confidence.\"\"\"\n",
    "    with tf.name_scope(\"smoothing_cross_entropy\", values=[logits, labels]):\n",
    "        # Low confidence is given to all non-true labels, uniformly.\n",
    "        low_confidence = (1.0 - confidence) / tf.to_float(vocab_size - 1)\n",
    "        # Normalizing constant is the best cross-entropy value with soft targets.\n",
    "        # We subtract it just for readability, makes no difference on learning.\n",
    "        normalizing = -(confidence * tf.log(confidence) + tf.to_float(\n",
    "                vocab_size - 1) * low_confidence * tf.log(low_confidence + 1e-20))\n",
    "        # Soft targets.\n",
    "        soft_targets = tf.one_hot(\n",
    "                tf.cast(labels, tf.int32),\n",
    "                depth=vocab_size,\n",
    "                on_value=confidence,\n",
    "                off_value=low_confidence)\n",
    "        xentropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=soft_targets)\n",
    "        return xentropy - normalizing\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Return list of dims, statically where possible.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "\n",
    "    # If unknown rank, return dynamic shape\n",
    "    if x.get_shape().dims is None:\n",
    "        return tf.shape(x)\n",
    "\n",
    "    static = x.get_shape().as_list()\n",
    "    shape = tf.shape(x)\n",
    "\n",
    "    ret = []\n",
    "    for i in range(len(static)):\n",
    "        dim = static[i]\n",
    "        if dim is None:\n",
    "            dim = shape[i]\n",
    "        ret.append(dim)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZZdCoFXJeQS_"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text modality\n",
    "bottom (embedding): source input, target input\n",
    "top (cross_entropy_loss): target output\n",
    "\"\"\"\n",
    "\n",
    "class SymbolModality(object):\n",
    "    \"\"\"Modality for sets of discrete symbols.\n",
    "    Input: Embedding.\n",
    "    Output: Linear transformation + softmax.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_hparams, vocab_size=None):\n",
    "        self._model_hparams = model_hparams\n",
    "        self._vocab_size = vocab_size\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"symbol_modality_%d_%d\" % (self._vocab_size, self._body_input_depth)\n",
    "\n",
    "    @property\n",
    "    def top_dimensionality(self):\n",
    "        return self._vocab_size\n",
    "\n",
    "    @property\n",
    "    def _body_input_depth(self):\n",
    "        return self._model_hparams.hidden_size\n",
    "\n",
    "    def _get_weights(self):\n",
    "        \"\"\"Create or get concatenated embedding or softmax variable.\n",
    "        Returns: a list of self._num_shards Tensors.\n",
    "        \"\"\"\n",
    "        num_shards = self._model_hparams.symbol_modality_num_shards\n",
    "        shards = []\n",
    "        for i in range(num_shards):\n",
    "            shard_size = (self._vocab_size // num_shards) + (\n",
    "                1 if i < self._vocab_size % num_shards else 0)\n",
    "            var_name = \"weights_%d\" % i\n",
    "            shards.append(\n",
    "                tf.get_variable(\n",
    "                    var_name, [shard_size, self._body_input_depth],\n",
    "                    initializer=tf.random_normal_initializer(\n",
    "                        0.0, self._body_input_depth ** -0.5)))\n",
    "        if num_shards == 1:\n",
    "            ret = shards[0]\n",
    "        else:\n",
    "            ret = tf.concat(shards, 0)\n",
    "        ret = ConvertGradientToTensor(ret)\n",
    "        return ret\n",
    "\n",
    "    def bottom_simple(self, x, name, reuse):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            # Squeeze out the channels dimension.\n",
    "            x = tf.squeeze(x, axis=3)\n",
    "            var = self._get_weights()\n",
    "            ret = tf.gather(var, x)\n",
    "            if self._model_hparams.multiply_embedding_mode == \"sqrt_depth\":\n",
    "                ret *= self._body_input_depth ** 0.5\n",
    "            ret *= tf.expand_dims(tf.to_float(tf.not_equal(x, 0)), -1)\n",
    "            return ret\n",
    "\n",
    "    def bottom(self, x):\n",
    "        if self._model_hparams.shared_source_embedding_and_softmax_weights:\n",
    "            return self.bottom_simple(x, \"shared\", reuse=None)\n",
    "        else:\n",
    "            return self.bottom_simple(x, \"input_emb\", reuse=None)\n",
    "\n",
    "    def targets_bottom(self, x):\n",
    "        if self._model_hparams.shared_target_embedding_and_softmax_weights:\n",
    "            return self.bottom_simple(x, \"shared\", reuse=tf.AUTO_REUSE)\n",
    "        else:\n",
    "            return self.bottom_simple(x, \"target_emb\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    def top(self, body_output, targets):\n",
    "        \"\"\"Generate logits.\n",
    "        Args:\n",
    "            body_output: A Tensor with shape [batch, p0, p1, body_input_depth]\n",
    "            targets: A Tensor with shape [batch, p0, p1, 1]\n",
    "        Returns:\n",
    "            logits: A Tensor with shape  [batch, p0, p1, ?, vocab_size].\n",
    "        \"\"\"\n",
    "        if self._model_hparams.shared_target_embedding_and_softmax_weights:\n",
    "            scope_name = \"shared\"\n",
    "            reuse = True\n",
    "        else:\n",
    "            scope_name = \"softmax\"\n",
    "            # reuse = False\n",
    "            reuse = tf.AUTO_REUSE\n",
    "        with tf.variable_scope(scope_name, reuse=reuse):\n",
    "            var = self._get_weights()\n",
    "            shape = tf.shape(body_output)[:-1]\n",
    "            body_output = tf.reshape(body_output, [-1, self._body_input_depth])\n",
    "            logits = tf.matmul(body_output, var, transpose_b=True)\n",
    "            logits = tf.reshape(logits, tf.concat([shape, [self._vocab_size]], 0))\n",
    "            # insert a channels dimension\n",
    "            return tf.expand_dims(logits, 3)\n",
    "\n",
    "    def bottom_sharded(self, xs, data_parallelism):\n",
    "        \"\"\"Transform the inputs.\n",
    "            [batch, p0, p1, depth --> [batch, p0, p1, body_input_depth].\n",
    "        \"\"\"\n",
    "        return data_parallelism(self.bottom, xs)\n",
    "\n",
    "    def targets_bottom_sharded(self, xs, data_parallelism):\n",
    "        \"\"\"Transform the targets.\n",
    "            [batch, p0, p1, target_channels] --> [batch, p0, p1, body_input_depth].\n",
    "        \"\"\"\n",
    "        return data_parallelism(self.targets_bottom, xs)\n",
    "\n",
    "    def top_sharded(self,\n",
    "                    sharded_body_output,\n",
    "                    sharded_targets,\n",
    "                    data_parallelism,\n",
    "                    weights_fn=weights_nonzero):\n",
    "        \"\"\"Transform all shards of targets.\n",
    "        Classes with cross-shard interaction will override this function.\n",
    "        \"\"\"\n",
    "        sharded_logits = data_parallelism(self.top, sharded_body_output,\n",
    "                                          sharded_targets)\n",
    "        if sharded_targets is None:\n",
    "            return sharded_logits, 0\n",
    "\n",
    "        loss_num, loss_den = data_parallelism(\n",
    "            padded_cross_entropy,\n",
    "            sharded_logits,\n",
    "            sharded_targets,\n",
    "            self._model_hparams.label_smoothing,\n",
    "            weights_fn=weights_fn)\n",
    "        loss = tf.add_n(loss_num) / tf.maximum(1.0, tf.add_n(loss_den))\n",
    "        return sharded_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YIlWu9wNgJWK"
   },
   "outputs": [],
   "source": [
    "\"\"\"transformer (attention).\n",
    "\n",
    "encoder: [Self-Attention, Feed-forward] x n\n",
    "decoder: [Self-Attention, Source-Target-Attention, Feed-forward] x n\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(object):\n",
    "    \"\"\"Attention net.  See file docstring.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 hparams,\n",
    "                 mode,\n",
    "                 data_parallelism=None):\n",
    "    \n",
    "        hparams = copy.copy(hparams)\n",
    "        hparams.add_hparam(\"mode\", mode)\n",
    "        if mode != tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            for key in hparams.values():\n",
    "                if key[-len(\"dropout\"):] == \"dropout\":\n",
    "                    setattr(hparams, key, 0.0)\n",
    "        self._hparams = hparams\n",
    "        self._data_parallelism = data_parallelism\n",
    "        self._num_datashards = data_parallelism.n   \n",
    "        # source side\n",
    "        self._hparams.input_modality = SymbolModality(hparams, hparams.vocab_src_size)\n",
    "        # target side\n",
    "        self._hparams.target_modality = SymbolModality(hparams, hparams.vocab_tgt_size)\n",
    "        \n",
    "    def infer(self,\n",
    "              features=None,\n",
    "              decode_length=50,\n",
    "              beam_size=1,\n",
    "              top_beams=1,\n",
    "              alpha=0.0):\n",
    "        \"\"\"\n",
    "        A inference method.\n",
    "        \"\"\"\n",
    "        local_features = dict()\n",
    "        local_features[\"_num_datashards\"] = self._num_datashards\n",
    "        local_features[\"_data_parallelism\"] = self._data_parallelism\n",
    "        local_features[\"_hparams\"] = self._hparams\n",
    "        local_features[\"_shard_features\"] = self._shard_features\n",
    "        local_features[\"encode\"] = self.encode\n",
    "        local_features[\"decode\"] = self.decode\n",
    "\n",
    "        tf.logging.info(\"Beam Decoding with beam size %d\" % beam_size)\n",
    "        return _beam_decode(features, decode_length, beam_size, top_beams, alpha, local_features)\n",
    "\n",
    "    def _shard_features(self, features):  # pylint: disable=missing-docstring\n",
    "        sharded_features = dict()\n",
    "        for k, v in six.iteritems(features):\n",
    "            v = tf.convert_to_tensor(v)\n",
    "            if not v.shape.as_list():\n",
    "                v = tf.expand_dims(v, axis=-1)\n",
    "                v = tf.tile(v, [self._num_datashards])\n",
    "            sharded_features[k] = self._data_parallelism(tf.identity, tf.split(v, self._num_datashards, 0))\n",
    "\n",
    "        return sharded_features\n",
    "\n",
    "    def model_fn(self, features):\n",
    "        \"\"\"Computes the entire model and produces sharded logits and training loss.\n",
    "        \"\"\"\n",
    "    \n",
    "        start_time = time.time()\n",
    "        dp = self._data_parallelism\n",
    "        sharded_features = self._shard_features(features)    \n",
    "        transformed_features = {}\n",
    "    \n",
    "        # source embedding\n",
    "        with tf.variable_scope(self._hparams.input_modality.name, reuse=False):\n",
    "            transformed_features[\"inputs\"] = sharded_features[\"inputs\"]\n",
    "    \n",
    "        # target embedding\n",
    "        with tf.variable_scope(self._hparams.target_modality.name, reuse=False):\n",
    "            transformed_features[\"targets_l1\"] = self._hparams.target_modality.targets_bottom_sharded(\n",
    "                    sharded_features[\"targets_l1\"], dp)\n",
    "            transformed_features[\"targets_l2\"] = self._hparams.target_modality.targets_bottom_sharded(\n",
    "                    sharded_features[\"targets_l2\"], dp)\n",
    "\n",
    "        # Construct the model body.\n",
    "        with tf.variable_scope(\"body\", reuse=False):\n",
    "            with tf.name_scope(\"model\"):\n",
    "                datashard_to_features = [{\n",
    "                    k: v[d] for k, v in six.iteritems(transformed_features)\n",
    "                    } for d in range(self._num_datashards)]\n",
    "                body_outputs = self._data_parallelism(self.model_fn_body, datashard_to_features)\n",
    "                extra_loss = 0.\n",
    "    \n",
    "        body_outputs_l1 = []\n",
    "        body_outputs_l2 = []\n",
    "        # for multi-gpus\n",
    "        for output in body_outputs:\n",
    "            body_outputs_l1.append(output[0])\n",
    "            body_outputs_l2.append(output[1])\n",
    "    \n",
    "        # target linear transformation and compute loss\n",
    "        with tf.variable_scope(self._hparams.target_modality.name, reuse=False):  ## = target_reuse\n",
    "            sharded_logits, training_loss_l1 = (self._hparams.target_modality.top_sharded(\n",
    "                body_outputs_l1, sharded_features[\"targets_l1\"], self._data_parallelism))\n",
    "            sharded_logits_l2, training_loss_l2 = (self._hparams.target_modality.top_sharded(\n",
    "                body_outputs_l2, sharded_features[\"targets_l2\"], self._data_parallelism))\n",
    "            # TODO: this heps to choise if we need to combine the lossesh\n",
    "            training_loss = training_loss_l1 + training_loss_l2\n",
    "            training_loss *= self._hparams.loss_multiplier\n",
    "\n",
    "#             training_loss = training_loss_l1\n",
    "#             training_loss *= 1.0\n",
    "\n",
    "        tf.logging.info(\"This model_fn took %.3f sec.\" % (time.time() - start_time))\n",
    "        return sharded_logits, training_loss, extra_loss\n",
    "\n",
    "    def model_fn_body(self, features):\n",
    "        hparams = copy.copy(self._hparams)\n",
    "        inputs = features.get(\"inputs\")\n",
    "\n",
    "        encoder_output, encoder_decoder_attention_bias = self.encode(\n",
    "            inputs, hparams)\n",
    "\n",
    "        targets_l1 = features[\"targets_l1\"]\n",
    "        targets_l2 = features[\"targets_l2\"]\n",
    "        targets_l1 = flatten4d3d(targets_l1)\n",
    "        targets_l2 = flatten4d3d(targets_l2)\n",
    "        (decoder_input, decoder_self_attention_bias) = transformer_prepare_decoder(\n",
    "                targets_l1, targets_l2, hparams)\n",
    "\n",
    "        decode_output = self.decode(decoder_input, encoder_output, encoder_decoder_attention_bias,\n",
    "                decoder_self_attention_bias, hparams)\n",
    "\n",
    "        return decode_output\n",
    "\n",
    "    def encode(self, inputs, hparams):\n",
    "        # inputs is audio feature in 3d [batch, length, raw_features]\n",
    "        (encoder_input, self_attention_bias, encoder_decoder_attention_bias) = \\\n",
    "            transformer_prepare_encoder(inputs, hparams)\n",
    "\n",
    "        encoder_input = tf.nn.dropout(encoder_input, 1.0 - hparams.residual_dropout)\n",
    "        encoder_output = transformer_encoder(encoder_input, self_attention_bias, hparams)\n",
    "\n",
    "        return encoder_output, encoder_decoder_attention_bias\n",
    "\n",
    "    def decode(self, decoder_input, encoder_output, encoder_decoder_attention_bias,\n",
    "               decoder_self_attention_bias, hparams, batch_size=None, beam_size=None, cache=None):\n",
    "        decoder_input = tf.nn.dropout(decoder_input, 1.0 - hparams.residual_dropout)\n",
    "      \n",
    "        if cache is None:  # training\n",
    "            decoder_output = transformer_decoder(\n",
    "                    decoder_input, encoder_output, decoder_self_attention_bias,\n",
    "                    encoder_decoder_attention_bias, hparams, cache=cache)\n",
    "            return tf.expand_dims(decoder_output, axis=3)\n",
    "        else:  # inference\n",
    "            decoder_output = transformer_decoder_for_decoding(\n",
    "                    decoder_input, encoder_output, decoder_self_attention_bias,\n",
    "                    encoder_decoder_attention_bias, hparams, batch_size, beam_size, cache=cache)\n",
    "            return tf.expand_dims(decoder_output, axis=2)\n",
    "\n",
    "\n",
    "def transformer_prepare_encoder(inputs, hparams):\n",
    "    \"\"\"Prepare one shard of the model for the encoder.\n",
    "    \"\"\"\n",
    "    # Flatten inputs.\n",
    "    ishape_static = inputs.shape.as_list()\n",
    "    encoder_input = inputs\n",
    "    encoder_padding = embedding_to_padding(encoder_input)\n",
    "    ignore_padding = attention_bias_ignore_padding(\n",
    "        encoder_padding)\n",
    "    encoder_self_attention_bias = ignore_padding\n",
    "    encoder_decoder_attention_bias = ignore_padding\n",
    "\n",
    "    if hparams.pos == \"timing\":\n",
    "        encoder_input = add_timing_signal_1d(encoder_input)\n",
    "    return encoder_input, encoder_self_attention_bias, encoder_decoder_attention_bias\n",
    "\n",
    "\n",
    "def transformer_prepare_decoder(targets_l1, targets_l2, hparams):\n",
    "    \"\"\"Prepare one shard of the model for the decoder.\n",
    "    \"\"\"\n",
    "    decoder_self_attention_bias = (\n",
    "        attention_bias_lower_triangle(tf.shape(targets_l1)[1])) ## [1, 1, length, length]\n",
    "    decoder_input_l1 = shift_left_3d(targets_l1)\n",
    "    decoder_input_l2 = shift_left_3d(targets_l2)\n",
    "    if hparams.pos == \"timing\":\n",
    "        decoder_input_l1 = add_timing_signal_1d(decoder_input_l1)\n",
    "        decoder_input_l2 = add_timing_signal_1d(decoder_input_l2)\n",
    "    decoder_input = tf.concat([tf.expand_dims(decoder_input_l1, 0), tf.expand_dims(decoder_input_l2, 0)], axis=0)\n",
    "    # [2, batch, length, hidden_size]\n",
    "    return decoder_input, decoder_self_attention_bias\n",
    "\n",
    "\n",
    "def transformer_encoder(encoder_input,\n",
    "                        encoder_self_attention_bias,\n",
    "                        hparams,\n",
    "                        name=\"encoder\",\n",
    "                        use_fc=True):\n",
    "    \"\"\"A stack of transformer layers.\n",
    "    \"\"\"\n",
    "    x = encoder_input\n",
    "    # Summaries don't work in multi-problem setting yet.\n",
    "    summaries = \"problems\" not in hparams.values() or len(hparams.problems) == 1\n",
    "    with tf.variable_scope(name):\n",
    "        if use_fc:\n",
    "            x = tf.layers.dense(inputs=x,\n",
    "                                units=hparams.hidden_size,\n",
    "                                activation=None,\n",
    "                                use_bias=False,\n",
    "                                name='full_connect')\n",
    "        for layer in range(hparams.num_hidden_layers_src):\n",
    "            with tf.variable_scope(\"layer_%d\" % layer):\n",
    "                y = multihead_attention(\n",
    "                    x,\n",
    "                    None,\n",
    "                    encoder_self_attention_bias,\n",
    "                    hparams.attention_key_channels or hparams.hidden_size,\n",
    "                    hparams.attention_value_channels or hparams.hidden_size,\n",
    "                    hparams.hidden_size,\n",
    "                    hparams.num_heads,\n",
    "                    hparams.attention_dropout,\n",
    "                    summaries=summaries,\n",
    "                    name=\"encoder_self_attention\")\n",
    "                x = residual_fn(x, y, hparams) ###\n",
    "                y = transformer_ffn_layer(x, hparams)\n",
    "                x = residual_fn(x, y, hparams)\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_decoder(decoder_input,\n",
    "                        encoder_output,\n",
    "                        decoder_self_attention_bias,\n",
    "                        encoder_decoder_attention_bias,\n",
    "                        hparams,\n",
    "                        cache=None,\n",
    "                        name=\"decoder\"):\n",
    "    \"\"\"A stack of transformer layers.\n",
    "    \"\"\"\n",
    "    x = decoder_input\n",
    "    # Summaries don't work in multi-problem setting yet.\n",
    "    summaries = \"problems\" not in hparams.values() or len(hparams.problems) == 1\n",
    "    with tf.variable_scope(name):\n",
    "        for layer in range(hparams.num_hidden_layers_tgt):\n",
    "            layer_name = \"layer_%d\" % layer\n",
    "            layer_cache = cache[layer_name] if cache is not None else None\n",
    "            with tf.variable_scope(layer_name):\n",
    "                y = sb_multihead_attention(\n",
    "                    x,\n",
    "                    None,\n",
    "                    decoder_self_attention_bias,\n",
    "                    hparams.attention_key_channels or hparams.hidden_size,\n",
    "                    hparams.attention_value_channels or hparams.hidden_size,\n",
    "                    hparams.hidden_size,\n",
    "                    hparams.num_heads,\n",
    "                    hparams.attention_dropout,\n",
    "                    cache=layer_cache,\n",
    "                    summaries=summaries,\n",
    "                    name=\"decoder_self_attention\")\n",
    "                x = residual_fn(x, y, hparams)\n",
    "                y = sb_multihead_attention(\n",
    "                    x,\n",
    "                    encoder_output,\n",
    "                    encoder_decoder_attention_bias,\n",
    "                    hparams.attention_key_channels or hparams.hidden_size,\n",
    "                    hparams.attention_value_channels or hparams.hidden_size,\n",
    "                    hparams.hidden_size,\n",
    "                    hparams.num_heads,\n",
    "                    hparams.attention_dropout,\n",
    "                    summaries=summaries,\n",
    "                    name=\"encdec_attention\")\n",
    "                x = residual_fn(x, y, hparams)\n",
    "                y = transformer_ffn_layer(x, hparams)\n",
    "                x = residual_fn(x, y, hparams)\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_decoder_for_decoding(decoder_input,\n",
    "                                     encoder_output,\n",
    "                                     decoder_self_attention_bias,\n",
    "                                     encoder_decoder_attention_bias,\n",
    "                                     hparams,\n",
    "                                     batch_size=None,\n",
    "                                     beam_size=None,\n",
    "                                     cache=None,\n",
    "                                     name=\"decoder\"):\n",
    "    \"\"\"A stack of transformer layers.\n",
    "    \"\"\"\n",
    "    x = decoder_input\n",
    "    # Summaries don't work in multi-problem setting yet.\n",
    "    summaries = \"problems\" not in hparams.values() or len(hparams.problems) == 1\n",
    "    with tf.variable_scope(name):\n",
    "        for layer in range(hparams.num_hidden_layers_tgt):\n",
    "            layer_name = \"layer_%d\" % layer\n",
    "            layer_cache = cache[layer_name] if cache is not None else None\n",
    "            with tf.variable_scope(layer_name):\n",
    "                y = sb_multihead_attention_for_decoding(\n",
    "                        x,\n",
    "                        None,\n",
    "                        decoder_self_attention_bias,\n",
    "                        hparams.attention_key_channels or hparams.hidden_size,\n",
    "                        hparams.attention_value_channels or hparams.hidden_size,\n",
    "                        hparams.hidden_size,\n",
    "                        hparams.num_heads,\n",
    "                        hparams.attention_dropout,\n",
    "                        batch_size,\n",
    "                        beam_size,\n",
    "                        cache=layer_cache,\n",
    "                        summaries=summaries,\n",
    "                        name=\"decoder_self_attention\")\n",
    "                x = residual_fn(x, y, hparams)\n",
    "                y = sb_multihead_attention_for_decoding(\n",
    "                        x,\n",
    "                        encoder_output,\n",
    "                        encoder_decoder_attention_bias,\n",
    "                        hparams.attention_key_channels or hparams.hidden_size,\n",
    "                        hparams.attention_value_channels or hparams.hidden_size,\n",
    "                        hparams.hidden_size,\n",
    "                        hparams.num_heads,\n",
    "                        hparams.attention_dropout,\n",
    "                        summaries=summaries,\n",
    "                        name=\"encdec_attention\")\n",
    "                x = residual_fn(x, y, hparams)\n",
    "                y = transformer_ffn_layer(x, hparams)\n",
    "                x = residual_fn(x, y, hparams)\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_ffn_layer(x, hparams):\n",
    "    \"\"\"Feed-forward layer in the transformer.\n",
    "    [batch_size, length, hparams.hidden_size] -->  [batch_size, length, hparams.hidden_size]\n",
    "    \"\"\"\n",
    "    if hparams.ffn_layer == \"conv_hidden_relu\":\n",
    "        return conv_hidden_relu(\n",
    "            x,\n",
    "            hparams.filter_size,\n",
    "            hparams.hidden_size,\n",
    "            dropout=hparams.relu_dropout)\n",
    "    else:\n",
    "        assert hparams.ffn_layer == \"none\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLG3qjidgTbJ"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pV4A43_f6JZX",
    "outputId": "3228a1e6-d047-452d-a112-860a5b350e3a"
   },
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "# TODO : I need to make this sync with the run hparam\n",
    "\n",
    "# Run this if you're running for the first time \n",
    "# flags.DEFINE_string(\"pretrain_output_dir\", \"../data/processed/pretrain_model\", \"Base output directory for run.\")\n",
    "\n",
    "flags.DEFINE_string(\"pretrain_output_dir\", \"../data/processed/wait2\", \"Base output directory for run.\")\n",
    "flags.DEFINE_string(\"output_dir\", \"../data/processed/wait2\", \"Base output directory for run.\")\n",
    "flags.DEFINE_string(\"data_dir\", \"../data/processed/tf_data\", \"Directory with training data.\")\n",
    "flags.DEFINE_string(\"train_src_name\", \"2m.bpe.unk.zh\", \"src name of training data.\")\n",
    "flags.DEFINE_string(\"train_tgt_name\", \"2m.bpe.unk.en\", \"tgt name of training data.\")\n",
    "flags.DEFINE_string(\"vocab_src_name\", \"en.vocab\", \"src name of vocab.\")\n",
    "flags.DEFINE_string(\"vocab_tgt_name\", \"ln.vocab\", \"tgt name of vocab.\")\n",
    "flags.DEFINE_integer(\"vocab_src_size\", 30000, \"source vocab size.\")\n",
    "flags.DEFINE_integer(\"vocab_tgt_size\", 30000, \"target vocab size.\")\n",
    "\n",
    "# Model\n",
    "flags.DEFINE_string(\"model\", \"Transformer\", \"Which model to use.\")\n",
    "flags.DEFINE_string(\"hparams_set\", \"transformer_params_base\", \"Which parameters to use.\")\n",
    "flags.DEFINE_string(\"hparams_range\", \"\", \"Parameters range.\")\n",
    "flags.DEFINE_string(\"hparams\", \"\", \"\"\"A comma-separated list of `name=value` hyperparameter values.\"\"\")\n",
    "flags.DEFINE_integer(\"train_steps\", 80000, \"The number of steps to run training for.\")\n",
    "flags.DEFINE_integer(\"eval_steps\", 10, \"Number of steps in evaluation.\")\n",
    "flags.DEFINE_bool(\"eval_print\", False, \"Print eval logits and predictions.\")\n",
    "flags.DEFINE_integer(\"keep_checkpoint_max\", 20, \"How many recent checkpoints to keep.\")\n",
    "flags.DEFINE_integer(\"save_checkpoint_secs\", 0, \"How seconds to save checkpoints.\")\n",
    "flags.DEFINE_integer(\"save_checkpoint_steps\", 1000, \"How steps tp save checkpoints.\")\n",
    "flags.DEFINE_float(\"gpu_mem_fraction\", 0.95, \"How GPU memory to use.\")\n",
    "flags.DEFINE_bool(\"experimental_optimize_placement\", False,\n",
    "                  \"Optimize ops placement with experimental session options.\")\n",
    "\n",
    "# Distributed training flags\n",
    "flags.DEFINE_string(\"master\", \"\", \"Address of TensorFlow master.\")\n",
    "flags.DEFINE_string(\"schedule\", \"local_run\",\n",
    "                    \"Method of tf.contrib.learn.Experiment to run.\")\n",
    "flags.DEFINE_bool(\"locally_shard_to_cpu\", False,\n",
    "                  \"Use CPU as a sharding device runnning locally. This allows \"\n",
    "                  \"to test sharded model construction on a machine with 1 GPU.\")\n",
    "flags.DEFINE_bool(\"daisy_chain_variables\", True,\n",
    "                  \"copy variables around in a daisy chain\")\n",
    "flags.DEFINE_integer(\"worker_gpu\", 1, \"How many GPUs to use.\")\n",
    "flags.DEFINE_integer(\"worker_replicas\", 1, \"How many workers to use.\")\n",
    "flags.DEFINE_integer(\"worker_id\", 0, \"Which worker task are we.\")\n",
    "flags.DEFINE_string(\"gpu_order\", \"\", \"Optional order for daisy-chaining gpus.\"\n",
    "                    \" e.g. \\\"1 3 2 4\\\"\")\n",
    "\n",
    "# Decode flags\n",
    "flags.DEFINE_string(\"decode_from_file\", None, \"Path to decode file\")\n",
    "flags.DEFINE_string(\"decode_to_file_l1\", None, \"Path to inference output file\")\n",
    "flags.DEFINE_string(\"decode_to_file_l2\", None, \"Path to inference output file\")\n",
    "flags.DEFINE_integer(\"decode_extra_length\", 100, \"Added decode length.\")\n",
    "flags.DEFINE_integer(\"decode_batch_size\", 32, \"Batch size for decoding. \")\n",
    "flags.DEFINE_integer(\"decode_beam_size\", 4, \"The beam size for beam decoding\")\n",
    "flags.DEFINE_float(\"decode_alpha\", 0.6, \"Alpha for length penalty\")\n",
    "flags.DEFINE_bool(\"decode_return_beams\", False,\"whether return all beams or one\")\n",
    "\n",
    "# Audio configuration\n",
    "flags.DEFINE_integer(\"dim_feature\", 80, \"Batch size for decoding. \")\n",
    "flags.DEFINE_integer(\"num_context\", 2, \"Batch size for decoding. \")\n",
    "flags.DEFINE_integer(\"downsample\", 3, \"Batch size for decoding. \")\n",
    "\n",
    "flags.DEFINE_bool(\"generate_data\", False, \"Generate data before training?\")\n",
    "flags.DEFINE_string(\"tmp_dir\", \"/tmp/t2t_datagen\", \"Temporary storage directory.\")\n",
    "flags.DEFINE_integer(\"num_shards\", 10, \"How many shards to use.\")\n",
    "flags.DEFINE_integer(\"max_cases\", 0, \"Maximum number of cases to generate (unbounded if 0).\")\n",
    "flags.DEFINE_integer(\"random_seed\", 429459, \"Random seed to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3C0MVYgY6rOG"
   },
   "outputs": [],
   "source": [
    "# tf.flags.FLAGS.__flags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "w0m7avGxeACS"
   },
   "outputs": [],
   "source": [
    "# # from utils import generator_utils\n",
    "# # from utils import trainer_utils as trainer_utils\n",
    "\n",
    "# flags = tf.flags\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "UNSHUFFLED_SUFFIX = \"-unshuffled\"\n",
    "\n",
    "_SUPPORTED_PROBLEM_GENERATORS = {\n",
    "    \"translation\": (\n",
    "        lambda: generator_utils.translation_token_generator(FLAGS.data_dir, FLAGS.tmp_dir, \n",
    "            FLAGS.train_src_name, FLAGS.train_tgt_name, FLAGS.vocab_src_name, FLAGS.vocab_tgt_name) )\n",
    "}\n",
    "\n",
    "\n",
    "def set_random_seed():\n",
    "    \"\"\"Set the random seed from flag everywhere.\"\"\"\n",
    "    tf.set_random_seed(FLAGS.random_seed)\n",
    "    random.seed(FLAGS.random_seed)\n",
    "    np.random.seed(FLAGS.random_seed)\n",
    "\n",
    "\n",
    "def generate_data():\n",
    "    data_dir = os.path.expanduser(FLAGS.data_dir)\n",
    "    tmp_dir = os.path.expanduser(FLAGS.tmp_dir)\n",
    "    tf.gfile.MakeDirs(data_dir)\n",
    "    tf.gfile.MakeDirs(tmp_dir)\n",
    "  \n",
    "    problem = list(sorted(_SUPPORTED_PROBLEM_GENERATORS))[0]\n",
    "    set_random_seed()\n",
    "\n",
    "    training_gen = _SUPPORTED_PROBLEM_GENERATORS[problem]\n",
    "\n",
    "    tf.logging.info(\"Generating training data for %s.\", problem)\n",
    "    train_output_files = generator_utils.generate_files(\n",
    "            training_gen(), problem + UNSHUFFLED_SUFFIX + \"-train\",\n",
    "            FLAGS.data_dir, FLAGS.num_shards, FLAGS.max_cases)\n",
    "\n",
    "    train_output_files = []\n",
    "    output_dir = FLAGS.data_dir\n",
    "    for shard in range(FLAGS.num_shards):\n",
    "        output_filename = \"%s-%.5d-of-%.5d\" % ('translation-unshuffled-train', shard, FLAGS.num_shards)\n",
    "        output_file = os.path.join(output_dir, output_filename)\n",
    "        train_output_files.append(output_file)\n",
    "\n",
    "    tf.logging.info(\"Shuffling data...\")\n",
    "    for fname in train_output_files:\n",
    "        records = generator_utils.read_records(fname)\n",
    "        random.shuffle(records)\n",
    "        out_fname = fname.replace(UNSHUFFLED_SUFFIX, \"\")\n",
    "        generator_utils.write_records(records, out_fname)\n",
    "        tf.gfile.Remove(fname)\n",
    "    tf.logging.info(\"Data Process Over\")\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    if FLAGS.generate_data:\n",
    "        generate_data()\n",
    "        if FLAGS.model != \"transformer\":\n",
    "            return\n",
    "  \n",
    "    run(model=FLAGS.model,\n",
    "        output_dir=FLAGS.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "fx0DQCoNCd_i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']; del sys \n",
    "\n",
    "def get_argument():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--tmp_dir\", default='./', help=\"Temporary storage directory.\")\n",
    "    parser.add_argument(\"--data_dir\", default='./', help=\"Directory with training data.\")\n",
    "    \n",
    "    parser.add_argument(\"--train_csv_name\", default='./', help=\"Filename of training data.\")\n",
    "    parser.add_argument(\"--dev_csv_name\", default='./', help=\"Filename of dev data.\")\n",
    "    parser.add_argument(\"--test_csv_name\", default='./', help=\"Filename of test data.\")\n",
    "    \n",
    "    parser.add_argument(\"--wav_dir_train\", default='./', help=\"Wavefile path of training data.\")\n",
    "    parser.add_argument(\"--wav_dir_dev\", default='./', help=\"Wavefile path of dev data.\")\n",
    "    parser.add_argument(\"--wav_dir_test\", default='./', help=\"Wavefile path of test data.\")\n",
    "    \n",
    "    parser.add_argument(\"--vocabA_name\", default='./', help=\"Vocab language A file name.\")\n",
    "    parser.add_argument(\"--vocabB_name\", default='./', help=\"Vocab language B file name.\")\n",
    "    \n",
    "    parser.add_argument(\"--vocab_size\", type=int, default=30000, help=\"Vocabulary size.\")\n",
    "    \n",
    "    parser.add_argument(\"-d\", \"--dim_raw_input\", type=int, default=80, help=\"The dimension of input feature.\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "args = get_argument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bcZfuUzCg8E",
    "outputId": "3b2113dc-0d31-42bb-8bde-ed67458fb3d9"
   },
   "outputs": [],
   "source": [
    "tf.app.run(main=main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Author:</h2> \n",
    "\n",
    "<a href=\"https://skabongo.github.io/\">Salomon Kabongo KABENAMUALU</a>, Master degree student at <a href=\"https://aimsammi.org/\">the African Master in Machine Intelligence (AMMI Ghana)</a> his research focused on the use machine learning technique in the field of Natural Language Processing.\n",
    "\n",
    "References : \n",
    "\n",
    "- [Tensorflow tensor2tensor](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor)\n",
    "- [Yuchen Liu and all. paper](https://arxiv.org/pdf/1912.07240.pdf)\n",
    "\n",
    "Copyright &copy; 2020. This notebook and its source code are released under the terms of the <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">Apache License 2.0</a>."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train_Listra",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf15",
   "language": "python",
   "name": "tf15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
